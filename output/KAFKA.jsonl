{"issue_id": "KAFKA-19633", "project": "KAFKA", "title": "Kafka Connect connectors sent out zombie records during rebalance", "status": "Open", "priority": "Critical", "reporter": "Shichao An", "assignee": null, "created": "2025-08-21T21:46:12.000+0000", "updated": "2025-08-21T21:46:12.000+0000", "labels": [], "description": "Hi, we run Debezium connectors on Kafka Connect. We identified several \"zombie\" records that are delivered by the connectors during or after the rebalance. Since the downstream consumers require ordering, this issue breaks several things where previous primitives were build upon.\r \r Here are an overview of the setup:\r  * Connector type: Debezium Mongo Connector\r  * Kafka Connect version: 3.2\r  * Number of workers: 3-4\r  * Kafka producer configs: at-least once settings, ack=all, max inflight requests=1\r \r The following conclusion are based on our investigation:\r {quote}When a Kafka Connect worker (part of a connector cluster) is overloaded or degraded, the connector on it may become temporarily unhealthy. The Kafka Connect cluster will rebalance the connector by \"moving\" it to another worker. When the connector is started on the new worker, the events will resume normally without any data loss and depending on the previously committed offsets, there might be a small amount of duplicate events due to replay but eventually the total ordering is still guaranteed.\u00a0\r \r However, the producer of the old worker may not have been gracefully shut down. When the old worker recovered, some old events that were already placed in the producer's internal queue got sent out to Kafka before the producer was forcefully closed. This caused the \"out-of-band\" duplicate events, which we referred to as \"ghost duplicates\" or \"zombie records.\r {quote}\r Can you verify our conclusion and do you have any recommendation for the potential fix or prevention?", "comments": [], "tasks": {"summarization": "Kafka Connect connectors sent out zombie records during rebalance - Hi, we run Debezium connectors on Kafka Connect. We identified several \"zombie\" records that are del...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19633 about?", "answer": "Kafka Connect connectors sent out zombie records during rebalance"}}}
{"issue_id": "KAFKA-19632", "project": "KAFKA", "title": "Handle overlap batch alignment on partition reassignment", "status": "Resolved", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Apoorv Mittal", "created": "2025-08-21T21:39:47.000+0000", "updated": "2025-08-27T09:07:07.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Handle overlap batch alignment on partition reassignment - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19632 about?", "answer": "Handle overlap batch alignment on partition reassignment"}}}
{"issue_id": "KAFKA-19631", "project": "KAFKA", "title": "Admin#describeReplicaLogDirs does not handle error correctly", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Chang-Yu Huang", "created": "2025-08-21T18:50:47.000+0000", "updated": "2025-09-24T16:30:34.000+0000", "labels": [], "description": "Admin#describeReplicaLogDirs has following issues:\r \r 1. Admin#describeReplicaLogDirs does not propagate the CLUSTER_AUTHORIZATION_FAILED\r \r 2. the single directory-level error is propagated to other directories result\r \r {code:java}\r                         if (logDirInfo.error() != null)\r                             handleFailure(new IllegalStateException(\r                                 \"The error \" + logDirInfo.error().getClass().getName() + \" for log directory \" + logDir + \" in the response from broker \" + brokerId + \" is illegal\"));\r {code}", "comments": [], "tasks": {"summarization": "Admin#describeReplicaLogDirs does not handle error correctly - Admin#describeReplicaLogDirs has following issues:\r \r 1. Admin#describeReplicaLogDirs does not propa...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19631 about?", "answer": "Admin#describeReplicaLogDirs does not handle error correctly"}}}
{"issue_id": "KAFKA-19630", "project": "KAFKA", "title": "LSO movement archives only the first available batch not all the ones prior to the new LSO", "status": "Resolved", "priority": "Major", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "created": "2025-08-21T11:17:38.000+0000", "updated": "2025-09-08T10:36:04.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "LSO movement archives only the first available batch not all the ones prior to the new LSO - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19630 about?", "answer": "LSO movement archives only the first available batch not all the ones prior to the new LSO"}}}
{"issue_id": "KAFKA-19629", "project": "KAFKA", "title": "Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently", "status": "Open", "priority": "Major", "reporter": "Evgheni Popusoi", "assignee": null, "created": "2025-08-21T07:40:31.000+0000", "updated": "2025-08-21T21:56:10.000+0000", "labels": [], "description": "We are using a Kafka Streams topology that continuously writes large volumes of data into a RocksDB state store with stable throughput. In parallel, another thread executes Interactive Query (IQ) requests against the same local state store.\r \r When the number of IQ requests in the queue grows (\u224850+), the application enters a {*}deadlock state{*}.\r \r *Investigation:*\r Using a thread dump, we discovered a lock inversion between RocksDB operations:\r  * {{RocksDBStore.put}}\r \r  ** blocked on {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r \r  ** holding {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r \r  * {{RocksDBStore.range}}\r \r  ** blocked on {{org.apache.kafka.streams.state.internals.RocksDBStore@414cff0e}}\r \r  ** holding {{org.apache.kafka.streams.query.Position@4ba00b6c}}\r \r This indicates that {*}{{put}} and {{range}} acquire the same locks but in different order{*}, which leads to deadlock under concurrent load.\r \r *Expected Behavior:*\r Kafka Streams API should guarantee deadlock-free operation. Store writes ({{{}put{}}}) and IQ reads ({{{}range{}}}) should not block each other in a way that leads to lock inversion.\r \r *Steps to Reproduce:*\r  # Create a Kafka Streams topology with a RocksDB state store receiving continuous writes.\r \r  # In a parallel thread, issue a high number of Interactive Query {{range}} requests (\u224850+ queued).\r \r  # Observe that the system eventually enters deadlock.\r \r  * \u00a0\r \r *Impact:*\r  * Application stops processing data.\r \r  * Interactive Queries fail indefinitely.\r \r  * Requires manual restart to recover.\r \r *Notes:*\r  * Appears to be a lock ordering bug in {{{}RocksDBStore{}}}.\r \r  * Expected the Streams API to coordinate thread-safety and prevent such deadlocks.", "comments": ["Thanks for filing this ticket. Seems the changes of KAFKA-15770 introduced this issue. Not 100% sure yet, what the right fix is, but not allocating locks in the same order on all code path is for sure incorrect.\r \r We do lock `Position` object inside `StoreQueryUtils#handleBasicQueries(...)` \u2013 maybe we would need to lock the passed in `store`, first?"], "tasks": {"summarization": "Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently - We are using a Kafka Streams topology that continuously writes large volumes of data into a RocksDB ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19629 about?", "answer": "Deadlock in Kafka Streams when processing Interactive Queries and state store updates concurrently"}}}
{"issue_id": "KAFKA-19628", "project": "KAFKA", "title": "Support partitionSizeLimitPerResponse for topicId\u2013based describeTopics request", "status": "Open", "priority": "Major", "reporter": "Kuan Po Tseng", "assignee": "Kuan Po Tseng", "created": "2025-08-20T17:10:15.000+0000", "updated": "2025-08-20T17:10:15.000+0000", "labels": ["need-kip"], "description": "DescribeTopicsOptions.partitionSizeLimitPerResponse is a prerequisite for supporting pagination, which is useful for requests involving a large number of topics.\r \r Currently, when calling Admin#describeTopics(TopicCollection) with topic IDs, KafkaAdminClient uses a MetadataRequest under the hood. However, MetadataRequest does not support the partitionSizeLimitPerResponse option. This means that pagination is effectively unsupported for topic ID\u2013based requests.\r \r In contrast, partitionSizeLimitPerResponse is supported by DescribeTopicPartitionsRequest. However, this RPC currently only supports topic names, not topic IDs. To enable full pagination support across both topic names and topic IDs, we would need to extend DescribeTopicPartitionsRequest to accept topic IDs as an additional field.", "comments": [], "tasks": {"summarization": "Support partitionSizeLimitPerResponse for topicId\u2013based describeTopics request - DescribeTopicsOptions.partitionSizeLimitPerResponse is a prerequisite for supporting pagination, whi...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19628 about?", "answer": "Support partitionSizeLimitPerResponse for topicId\u2013based describeTopics request"}}}
{"issue_id": "KAFKA-19627", "project": "KAFKA", "title": "Remove command-line arguments deprecated to implement KIP-1147", "status": "Open", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-08-20T11:29:33.000+0000", "updated": "2025-08-20T11:29:33.000+0000", "labels": [], "description": "Several arguments were replaced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Improve+consistency+of+command-line+arguments], initially deprecating in 4.2. The deprecated arguments will be removed in 5.0.", "comments": [], "tasks": {"summarization": "Remove command-line arguments deprecated to implement KIP-1147 - Several arguments were replaced in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1147%3A+Im...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19627 about?", "answer": "Remove command-line arguments deprecated to implement KIP-1147"}}}
{"issue_id": "KAFKA-19626", "project": "KAFKA", "title": "Consistency of command-line arguments for remaining CLI tools", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Jhen-Yung Hsu", "created": "2025-08-20T09:17:20.000+0000", "updated": "2025-08-29T11:04:55.000+0000", "labels": [], "description": "This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-application-reset.sh.", "comments": ["Hi [~schofielaj] , I saw that this Jira ticket is unassigned. I'd be happy to take it on if that would be alright.", "Sure.", "Add the PR link here, since it is not shown in Jira.\r https://github.com/apache/kafka/pull/20431"], "tasks": {"summarization": "Consistency of command-line arguments for remaining CLI tools - This implements KIP-1147 for kafka-cluster.sh, kafka-leader-election.sh and kafka-streams-applicatio...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19626 about?", "answer": "Consistency of command-line arguments for remaining CLI tools"}}}
{"issue_id": "KAFKA-19625", "project": "KAFKA", "title": "Consistency of command-line arguments for verifiable producer/consumer", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Jimmy Wang", "created": "2025-08-20T09:16:14.000+0000", "updated": "2025-09-04T10:55:04.000+0000", "labels": [], "description": "This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh.", "comments": ["[~schofielaj] I've assigned this issue to myself in advance. If someone else has already taken it, please feel free to assign it to another.", "Go for it."], "tasks": {"summarization": "Consistency of command-line arguments for verifiable producer/consumer - This implements KIP-1147 for kafka-verifiable-producer.sh and kafka-verifiable-consumer.sh....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19625 about?", "answer": "Consistency of command-line arguments for verifiable producer/consumer"}}}
{"issue_id": "KAFKA-19624", "project": "KAFKA", "title": "Consistency of command-line arguments for consumer performance tests", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "ally heev", "created": "2025-08-20T09:14:47.000+0000", "updated": "2025-09-29T17:09:00.000+0000", "labels": [], "description": "This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh.", "comments": ["Hi Andrew Schofield, I saw you didin't assign anyone to this jira, does that mean I can help with this one? Thanks!", "[~aheev] got there first :)", "I made an error in Jira number while creating the PR. Updated it, but for some reason it doesn't link automatically now\r \r PR: https://github.com/apache/kafka/pull/20385"], "tasks": {"summarization": "Consistency of command-line arguments for consumer performance tests - This implements KIP-1147 for kafka-consumer-perf-test.sh and kafka-share-consumer-perf-test.sh....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19624 about?", "answer": "Consistency of command-line arguments for consumer performance tests"}}}
{"issue_id": "KAFKA-19623", "project": "KAFKA", "title": "Consistency of command-line arguments for console producer/consumer", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Shivsundar R", "created": "2025-08-20T09:13:16.000+0000", "updated": "2025-09-19T08:05:35.000+0000", "labels": [], "description": "This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-consumer.", "comments": ["Hi [~schofielaj], if you're not working on this, may I take it? Thanks.", "-Hi [~schofielaj], I saw you didin't assign anyone to this jira, does that mean I can help with this one?-", "Hi [~isding_l] , I already had a volunteer in mind for this one. Feel free to take any of the unassigned others."], "tasks": {"summarization": "Consistency of command-line arguments for console producer/consumer - This implements KIP-1147 for kafka-console-producer, kafka-console-consumer and kafka-console-share-...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19623 about?", "answer": "Consistency of command-line arguments for console producer/consumer"}}}
{"issue_id": "KAFKA-19622", "project": "KAFKA", "title": "Limitations of KRAFT Dual Write Mode for Production Support", "status": "Open", "priority": "Critical", "reporter": "Sharad Garg", "assignee": null, "created": "2025-08-20T05:16:48.000+0000", "updated": "2025-08-30T06:43:39.000+0000", "labels": [], "description": "We are currently running Kafka version 3.9.0 and are in the process of migrating to KRaft. As part of the migration, we intend to operate in dual-write mode in production for an initial period to help identify and address any issues.\r \r Are there any known limitations or risks associated with running in dual-write mode? Would you recommend maintaining this mode for production stability, and are there best practices we should follow?", "comments": ["Hello, could someone please check this?"], "tasks": {"summarization": "Limitations of KRAFT Dual Write Mode for Production Support - We are currently running Kafka version 3.9.0 and are in the process of migrating to KRaft. As part o...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19622 about?", "answer": "Limitations of KRAFT Dual Write Mode for Production Support"}}}
{"issue_id": "KAFKA-19621", "project": "KAFKA", "title": "Implement cleanup mechanism for obsolete Remote Log Metadata", "status": "Open", "priority": "Major", "reporter": "Lan Ding", "assignee": "Lan Ding", "created": "2025-08-20T04:49:40.000+0000", "updated": "2025-08-20T04:49:40.000+0000", "labels": [], "description": "see https://github.com/apache/kafka/pull/20345#issuecomment-3201413774", "comments": [], "tasks": {"summarization": "Implement cleanup mechanism for obsolete Remote Log Metadata - see https://github.com/apache/kafka/pull/20345#issuecomment-3201413774...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19621 about?", "answer": "Implement cleanup mechanism for obsolete Remote Log Metadata"}}}
{"issue_id": "KAFKA-19620", "project": "KAFKA", "title": "Move AutoTopicCreationManager to server module", "status": "Open", "priority": "Major", "reporter": "Kuan Po Tseng", "assignee": "Kuan Po Tseng", "created": "2025-08-19T17:53:17.000+0000", "updated": "2025-09-24T16:55:06.000+0000", "labels": [], "description": "Move AutoTopicCreationManager and AutoTopicCreationManagerTest to server module.", "comments": [], "tasks": {"summarization": "Move AutoTopicCreationManager to server module - Move AutoTopicCreationManager and AutoTopicCreationManagerTest to server module....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19620 about?", "answer": "Move AutoTopicCreationManager to server module"}}}
{"issue_id": "KAFKA-19619", "project": "KAFKA", "title": "Refactore DescribeTopicPartitionsRequestHandler: Improve readability and add code documentation", "status": "Open", "priority": "Minor", "reporter": "Adeshina Lasisi", "assignee": null, "created": "2025-08-19T12:03:29.000+0000", "updated": "2025-08-21T03:24:37.000+0000", "labels": [], "description": "My First Kafka code improvement.", "comments": [], "tasks": {"summarization": "Refactore DescribeTopicPartitionsRequestHandler: Improve readability and add code documentation - My First Kafka code improvement....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19619 about?", "answer": "Refactore DescribeTopicPartitionsRequestHandler: Improve readability and add code documentation"}}}
{"issue_id": "KAFKA-19618", "project": "KAFKA", "title": "the `record-size` and `throughput`arguments don't work in TestRaftServer", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Jhen-Yung Hsu", "created": "2025-08-18T23:28:23.000+0000", "updated": "2025-08-21T17:43:56.000+0000", "labels": [], "description": "see https://github.com/apache/kafka/blob/656242775c321c263a3a01411b560098351e8ec4/core/src/main/scala/kafka/tools/TestRaftServer.scala#L118\r \r we always hard code the `recordsPerSec` and `recordSize`", "comments": ["I'm working on this, thanks :)"], "tasks": {"summarization": "the `record-size` and `throughput`arguments don't work in TestRaftServer - see https://github.com/apache/kafka/blob/656242775c321c263a3a01411b560098351e8ec4/core/src/main/scal...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19618 about?", "answer": "the `record-size` and `throughput`arguments don't work in TestRaftServer"}}}
{"issue_id": "KAFKA-19617", "project": "KAFKA", "title": "ConsumerPerformance#ConsumerPerfRebListener get corrupted value when the number of partitions is increased", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Lan Ding", "created": "2025-08-18T13:29:22.000+0000", "updated": "2025-09-28T16:37:28.000+0000", "labels": [], "description": "see https://github.com/apache/kafka/pull/20221#discussion_r2228931829", "comments": [], "tasks": {"summarization": "ConsumerPerformance#ConsumerPerfRebListener get corrupted value when the number of partitions is increased  - see https://github.com/apache/kafka/pull/20221#discussion_r2228931829...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19617 about?", "answer": "ConsumerPerformance#ConsumerPerfRebListener get corrupted value when the number of partitions is increased"}}}
{"issue_id": "KAFKA-19616", "project": "KAFKA", "title": "add compression type and level to log_compaction_test.py", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Yunchi Pang", "created": "2025-08-18T11:59:18.000+0000", "updated": "2025-08-30T02:21:26.000+0000", "labels": [], "description": "as title", "comments": [], "tasks": {"summarization": "add compression type and level to log_compaction_test.py - as title...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19616 about?", "answer": "add compression type and level to log_compaction_test.py"}}}
{"issue_id": "KAFKA-19615", "project": "KAFKA", "title": "Enable to ducker-ak to \"isolate\" the ducker containers", "status": "In Progress", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "HongYi Chen", "created": "2025-08-17T19:04:45.000+0000", "updated": "2025-10-07T03:17:52.000+0000", "labels": [], "description": "We can't run the e2e tests through ducker-ak on the same machine for now. That is a bit troublesome when I want to run e2e for different PRs. Perhaps we could introduce a \"prefix\" to ducker to isolate the containers.", "comments": [], "tasks": {"summarization": "Enable to ducker-ak to \"isolate\" the ducker containers - We can't run the e2e tests through ducker-ak on the same machine for now. That is a bit troublesome ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19615 about?", "answer": "Enable to ducker-ak to \"isolate\" the ducker containers"}}}
{"issue_id": "KAFKA-19614", "project": "KAFKA", "title": "The records appended to the log are illegal because of an incorrect base offset during TestLinearWriteSpeed", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Chang Chi Hsu", "created": "2025-08-17T11:18:55.000+0000", "updated": "2025-10-06T14:13:42.000+0000", "labels": [], "description": "{code:java}\r Exception in thread \"main\" org.apache.kafka.common.InvalidRecordException: The baseOffset of the record batch in the append to kafka-test-0 should be 0, but it is 9\r {code}\r \r We could simplify reset the offset or create the new records in each write", "comments": [], "tasks": {"summarization": "The records appended to the log are illegal because of an incorrect base offset during TestLinearWriteSpeed - {code:java}\r Exception in thread \"main\" org.apache.kafka.common.InvalidRecordException: The baseOffs...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19614 about?", "answer": "The records appended to the log are illegal because of an incorrect base offset during TestLinearWriteSpeed"}}}
{"issue_id": "KAFKA-19613", "project": "KAFKA", "title": "Expose consumer CorruptRecordException", "status": "In Progress", "priority": "Minor", "reporter": "Uladzislau Blok", "assignee": "Uladzislau Blok", "created": "2025-08-16T12:41:30.000+0000", "updated": "2025-09-27T10:45:40.000+0000", "labels": ["need-kip"], "description": "As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecordException}}\u00a0in Streams because it's not exposed to the client; instead, a generic {{KafkaException}}\u00a0**\u00a0is thrown\r \r The proposed solution is to expose {{CorruptRecordException}} with information about affected TopicPartition and offset we're trying read from\r \r KIP: https://cwiki.apache.org/confluence/x/NQrxFg", "comments": ["[~mjsax]\r Good evening. Sorry for ping, but I wasn't sure if you've got a notification.\u00a0\r Path is available, it's simple", "Since the clients are supposed to skip a corrupted message, it would also be beneficial if the partition and maybe the offset is added to the `CorruptRecordException`. Parsing the exception message is uncool and currently I don't see another way to get the partition?", "Definitely parsing of String message is not a solution. The idea here was just to expose exception, but let me check if more changes for consumer are required", "Okay. We can expose TopicPartition and fetchOffset within CorruptRecordException. This will help us handle original issue in KS (using Consumer#seek for TP and offset from exception)", "[~lianetm] WDYT?\r \r Exposing partition and offset information seems useful to skip over a corrupted batch IMHO \u2013 but it's a public API change, and will require a KIP.", "Agree it sounds useful to expose a Corrupt error including TP and offset (with KIP), but just to be clear, this info is not passed to the client at the moment, so it's not only about changing the client poll API, we need to sort out if the info is available when generating the error on the broker side? Just to be sure that this is doable, and that we can determine a corrupt/helpful offset to pass back in the error", "[~lianetm]\u00a0\r Hello\u00a0\r Just today I've got an access to Kafka Confluence. I'll start working on KIP ASAP\r Re-think my idea and what I see:\r What client (FetchCollector#handleInitializeErrors) gives us is not an offset of corrupted message, but message we were trying to fetch. See an example:\r !corrupted_records.excalidraw.png!\r \r \u00a0\r \r If consumer is trying to read 512 bytes it will fail, as a response will contain 2 messages and corrupted part. Correct me, if I'm wrong here\r \r My proposal was to skip (seek last read offset +1) until we'll read successfully, but this way we can lose the messages.\u00a0\r We can also do something else... return the messages before corruption... and return the corrupted offset from the broker, here I don't know how many afford it will take. I need to analyze it. What do you think?", "Line ~140:\r {code:java}\r } catch (KafkaException e) {\r     if (fetch.isEmpty())\r         throw e;{code}\r On the first poll, the 2 records would be returned. On the next poll, we would throw the CorruptRecordException. If we seek then forward, we would skip only the broken Record(batches).", "[~FliegenKLATSCH] I checked this on unit test, fetch is empty here. I think we need to confirm how broker behaves in this case, and verify if this is a case when broker will send us response with correct and corrupted messages\r \r [~lianetm] \r I check the broker code and have a question:\r \r AbstractFetcherThread#processFetchRequest has this comment just after catch for corrupted message exception:\r //we log the error and continue. This ensures two things\r // 1. If there is a corrupt message in a topic partition, it does not bring the fetcher thread\r // \u00a0 \u00a0down and cause other topic partition to also lag\r \u00a0// 2. If the message is corrupt due to a transient state in the log (truncation, partial writes\r // \u00a0 \u00a0can cause this), we simply continue and should get fixed in the subsequent fetches\r \r Case 2 looks for me as retriable error. Are you sure, this is not retriable? FYI: this comment is from 2018, so I believe it can be outdated, just want to be sure\r \r UPD: I've analyzed broker behavior and in short:\r Comment I've mentioned above this is not about reading message from log, but about appending information for followers, not sure if this is related to current ticket, but that's the only place in code where I found connection between CorruptedMessageException\r ReplicaManager#read looks to me as it's trying to read max bytes and just fail if there is any error, but I couldn't find any uses of CorruptedMessageException in this path\r Could you please suggest any broker person to contact with to verify broker behavior?", "Correct me, if I am wrong, but not sure which error gets created on the broker side - the client validates the CRC checksum?!\r \r A follower acts like a normal client(?). So it should also validate the CRC checksum and as long as nobody seeks ahead, it just hangs with an exception - so the behaviour would not change. (And I've seen this behaviour.)\r \r And since I am talking about observations: We have a replica 4 setup, with minISR 2; we've seen cases where the leader has a broken message, but 1 replica is fine, the 2 others are hanging because of the broken record. So another beneficial approach would be to try to read from another available broker. But I guess that is out of scope here. And maybe even partially implemented already with preferred-read-replica.\r \r And regarding the retry: If we're using an example of broken memory, the corruption could be introduced on reading and on another read attempt, with different memory sections being used, could succeed. So there is probably no definite answer to \"should we retry or not\". The client has to decide, like it currently is. We should just expose the information and not change the behaviour.", "About CRC checksum validation, that's depend I guess. This issue was created based on original one for Kafka Streams (btw, this is a reason why I'm also thinking / asking about validation) and exception was thrown from FetchCollector#initialize method, so this is not about reading records and fail with CRC check error, but about getting FetchResponse with error from broker. You can check AbstractFetch#handleFetchSuccess method, if I missed something, but I don't see any validation of broker response before FetchCollector#initialize method\r \r UPD: Checked it one more time and I don't see if we set this error in any place in consumer, we only check it. This means it's already here when we get response from the broker. Correct me if i miss something", "Indeed the error would probably not be filled in the FetchCollector#initialize method, not sure if this can happen. If the broker would not be able to persist the message, I hope the producer doesn't get an ACK. And on reading the client should validate the CRC checksum and the broker probably doesn't need to care. But it would not hurt to handle it the same way.\r \r The CorruptRecordException would be thrown in FetchCollector#fetchRecords, so CompletedFetch#maybeEnsureValid has to be modified to add the information there (2 times and 2 times in ShareCompletedFetch, for the record and the batch each).\r In CompletedFetch#fetchRecords it's still encapsulated in a KafkaException. Not sure if that is good/needed, but it also doesn't hurt, we just need to unwrap on the client side. And the note about seeking makes sense.", "I'd separate those cases:\r  # CorruptMessageException when client validates records e.g. FetchCollector#fetchRecords and CompletedFetch#maybeEnsureValid under the hood. For this case your proposal makes sense, and user can decide how to recover\u00a0\r  # CorruptMessageException on broker response. This still can be the case (see stack trace [https://github.com/confluentinc/kafka-streams-examples/issues/524] ) and then we need to verify, if there is possibility of loosing messages in case of seeking. Currently it looks to me as if broker can't read the log, partition data on response will be completely empty (as I understand no partial reading, when there is two records and fail at same time)\r \r UPD: Checked what Lianet wrote about this error in KS ticket:\r {quote}I believe so, a fetch request including a partition and offset should fail with CorruptMessage if any record that needs to be included on that response (according to the fetch max bytes limits etc) is found corrupted on the broker when reading the log to generate the fetch response (but to double check on the broker-side handling of fetch in case I'm missing something)\r \r get it on the consumer path: I expect it means the broker identified the data as corrupted when reading from the log -> I would expect this is not retriable (ex. disk corrupted)\r {quote}\r May be it makes sense to even separate those cases (broker can't read and broker can read, but client rejects with CRC validation) and report first one with different exception?\r \r [~mjsax] [~lianetm] What do you think?", "[~lianetm] [~mjsax]\u00a0\r Hello All,\r KIP is already prepared. Could you please take a look?\r https://cwiki.apache.org/confluence/display/KAFKA/KIP-1218%3A+Expose+consumer+CorruptRecordException"], "tasks": {"summarization": "Expose consumer CorruptRecordException - As part of our analysis around KAFKA-19430, we realized it's not possible to handle a {{CorruptRecor...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19613 about?", "answer": "Expose consumer CorruptRecordException"}}}
{"issue_id": "KAFKA-19612", "project": "KAFKA", "title": "Remove Redundant Validation in StripedReplicaPlacer", "status": "Open", "priority": "Trivial", "reporter": "Jialun Peng", "assignee": "Jialun Peng", "created": "2025-08-16T08:54:21.000+0000", "updated": "2025-08-26T03:33:34.000+0000", "labels": [], "description": "Currently, the following validation checks are performed twice for each partition placement:\r  # {{throwInvalidReplicationFactorIfNonPositive()}}\r \r  # {{throwInvalidReplicationFactorIfTooFewBrokers()}}\r \r  # {{throwInvalidReplicationFactorIfZero()}}\r \r These checks are already performed in the public\u00a0{{place()}}\u00a0method before the partition placement loop begins. Since the cluster state and replication factor don't change during the placement operation, these checks only need to be performed once.\r \r \u00a0\r \r This redundant validation could be removed from the inner loop to improve performance, especially when placing many partitions.", "comments": [], "tasks": {"summarization": "Remove Redundant Validation in StripedReplicaPlacer - Currently, the following validation checks are performed twice for each partition placement:\r  # {{t...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19612 about?", "answer": "Remove Redundant Validation in StripedReplicaPlacer"}}}
{"issue_id": "KAFKA-19611", "project": "KAFKA", "title": "Enable ProducerPerformance to abort transaction randomly", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Lan Ding", "created": "2025-08-16T00:47:36.000+0000", "updated": "2025-08-18T16:05:21.000+0000", "labels": ["need-kip"], "description": "While testing KAFKA-18884, I noticed there are no official tools available to measure the performance of aborted transactions. `ProducerPerformance` should serve this purpose by allowing us to configure a ratio of transactions to abort", "comments": ["Very nice. Of course, this will have an impact on consumption too, but adding this to `ProducerPerformance` is great."], "tasks": {"summarization": "Enable ProducerPerformance to abort transaction randomly - While testing KAFKA-18884, I noticed there are no official tools available to measure the performanc...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19611 about?", "answer": "Enable ProducerPerformance to abort transaction randomly"}}}
{"issue_id": "KAFKA-19610", "project": "KAFKA", "title": "add integration tests for Consumer#currentLag", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Bolin Lin", "created": "2025-08-15T19:56:43.000+0000", "updated": "2025-08-18T16:05:51.000+0000", "labels": ["integration-tests"], "description": "The following scenarios should be included.\r \r 1. non-existed partitions\r 2. undetermined offset\r 3. normal case", "comments": [], "tasks": {"summarization": "add integration tests for Consumer#currentLag - The following scenarios should be included.\r \r 1. non-existed partitions\r 2. undetermined offset\r 3....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19610 about?", "answer": "add integration tests for Consumer#currentLag"}}}
{"issue_id": "KAFKA-19609", "project": "KAFKA", "title": "Move TransactionLogTest to transaction-coordinator module", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "HongYi Chen", "created": "2025-08-15T18:08:34.000+0000", "updated": "2025-09-15T03:25:58.000+0000", "labels": [], "description": "this is the follow-up of KAFKA-18884", "comments": [], "tasks": {"summarization": "Move TransactionLogTest to transaction-coordinator module - this is the follow-up of KAFKA-18884...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19609 about?", "answer": "Move TransactionLogTest to transaction-coordinator module"}}}
{"issue_id": "KAFKA-19608", "project": "KAFKA", "title": "REBOOTSTRAP_REQUIRED leave connections to group coordinator untouched", "status": "Open", "priority": "Major", "reporter": "David Dufour", "assignee": null, "created": "2025-08-15T10:27:07.000+0000", "updated": "2025-08-15T10:27:07.000+0000", "labels": [], "description": "KIP-1102 has introduced a new error code to instruct clients to rebootstrap. This error has been introduced to help proxies to instruct the clients to reboostrap when brokers have changed.\r \r However, the current implementation in group consumers leave the connections to the group coordinator intact.\r \r As a result, when doing a cluster failover:\r \u00a0- the clients receive REBOOTSTRAP_REQUIRED\r \u00a0- the broker connections are reboostrapped, connections to the primary cluster are closed and connections to the secondary clusters are opened\r \r \u00a0- the consumers are still connected to the group coordinator of the primary cluster\r \r ==> the consumers are:\r \u00a0- fetching data from the secondary cluster\r \u00a0- sending hearbeats to the primary cluster\r \r The consumers are in an inconsistent state", "comments": [], "tasks": {"summarization": "REBOOTSTRAP_REQUIRED leave connections to group coordinator untouched - KIP-1102 has introduced a new error code to instruct clients to rebootstrap. This error has been int...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19608 about?", "answer": "REBOOTSTRAP_REQUIRED leave connections to group coordinator untouched"}}}
{"issue_id": "KAFKA-19607", "project": "KAFKA", "title": "MirrorMaker2 Offset Replication Issue", "status": "Open", "priority": "Minor", "reporter": "geric", "assignee": null, "created": "2025-08-15T08:42:49.000+0000", "updated": "2025-08-18T16:12:17.000+0000", "labels": ["RedHat"], "description": "I am using *Apache Kafka 4.0*\u00a0with\u00a0*MirrorMaker 2*\u00a0to link the primary cluster ({*}clusterA{*}) to the secondary cluster ({*}clusterB{*}).\r The secondary cluster will not have any producers or consumers until a disaster recovery event occurs, at which point all producers and consumers will switch to it.\r \r *Setup:*\r  * Dedicated standalone MirrorMaker 2 node\r  * {{IdentityReplicationPolicy}}\u00a0(no topic renaming)\r  * No clients connected to secondary cluster under normal operation\r \r *MirrorMaker 2 config:*\r \u00a0{{# Cluster aliases\r clusters = clusterA, clusterB\r \r # Bootstrap servers\r clusterA.bootstrap.servers = serverA-kafka-1:9092\r clusterB.bootstrap.servers = serverB-kafka-1:9092\r \r # Replication policy\r replication.policy.class=org.apache.kafka.connect.mirror.IdentityReplicationPolicy\r \r # Offset/Checkpoint sync\r emit.checkpoints.enabled=true\r emit.checkpoints.interval.seconds=5\r sync.group.offsets.enabled=true\r sync.group.offsets.interval.seconds=5\r offset.lag.max=10\r refresh.topics.interval.seconds=5}}\r ----\r h3. Test results:\r  # *Produce 300 messages when MirrorMaker is running*\r *Expected:*\u00a0Topic offset synced within a minute\r *Result:*\u00a0\u2705 Passed\r \r  # *Consume 100 messages when MirrorMaker is running, then terminate the consumer*\r *Expected:*\u00a0Consumer offset synced\r *Result:*\u00a0\u274c Failed \u2014 offset is not synced to clusterB\r \r  # *Restart MirrorMaker after test #2*\r *Expected:*\u00a0Consumer offset synced\r *Result:*\u00a0\u2705 Passed\r \r  # *Repeat test #2 \u2014 consume 100 messages when MirrorMaker is running, then terminate the consumer*\r *Expected:*\u00a0Consumer offset synced\r *Result:*\u00a0\u274c Failed \u2014 offset is not synced to clusterB\r \r  # *Restart MirrorMaker after test #4*\r *Expected:*\u00a0Consumer offset synced\r *Result:*\u00a0\u274c Failed \u2014 offset is not synced to clusterB\r \r  # *Consume messages but keep consumer running*\r *Expected:*\u00a0Offset synced\r *Result:*\u00a0\u2705 Passed\r \r ----\r h3. Problem:\r \r Consumer offsets appear to only sync in these cases:\r  # When MirrorMaker is restarted and the consumer offset does\u00a0*not*\u00a0already exist in the secondary cluster (initial sync), or\r  # When the consumer is still connected at the time of sync,\u00a0*or*\u00a0when the consumer has reached the end of the offset (i.e., consumed all available messages).\r \r However, if the consumer exits immediately after consuming some messages (but\u00a0{*}before reaching the end of the topic{*}), the committed offset is\u00a0*never synced*\u00a0to the target cluster.\r ----\r h3. Additional Context / Related Issues\r \r This problem seems related to an open discussion in the Apache Kafka mailing list:\r \r *MirrorCheckpointConnector does not replicate final batch of offsets*\r [https://lists.apache.org/thread/dxn9jyotl00f7ov541299cd8tlcl1z00]", "comments": ["Hi [~geric] Please see the other related tickets in this area:\r  * https://issues.apache.org/jira/browse/KAFKA-16364\r  * https://issues.apache.org/jira/browse/KAFKA-16291\u00a0\r  * https://issues.apache.org/jira/browse/KAFKA-15564\u00a0\r  * [https://github.com/apache/kafka/pull/15423]\u00a0\r \r For a detailed explanation of this behavior, please see this conference talk:\u00a0\r \r [https://current.confluent.io/2024-sessions/mirrormaker-2s-offset-translation-isnt-exactly-once-and-thats-okay]\u00a0\r \r Farther from the end of the topic (lag is ~200) the translation is worse (lag can double to ~400). Because you have only 300 messages in the topic, it looks like the offset never gets translated, or translates to 0 or 1.\r \r With a larger example (1000 messages produced, 800 messages consumed) I would expect translation to lead to a downstream consumer lag of ~400. Also be aware that resetting the consumer offsets may not be sufficient to clear the MM2 state in the checkpoints topics. Make sure to use a new consumer group for further experiments with the same MM2 instance.\r \r For applications, you either need to let your consumers reach 0 lag prior to cut-over, or you need to tolerate some re-delivery on the destination side.\r \r Hope this helps!", "Hi Greg,\u00a0\r Thanks for replying.\u00a0\r \r In a planned flip to DR, is there a way for me to force sync the consumer offsets?\r \r Example: to delete all the consumers and re-sync the offset?\r \r Thanks\r \r Geric", "[~geric] I'm not sure exactly what you mean by force-sync. I mentioned in my last message \"you either need to let your consumers reach 0 lag prior to cut-over, or ...\".\r \r To be more explicit, If you're planning to flip to the target cluster and want minimum redelivery you should:\r 1. Stop the source producers\r 2. Wait for source consumers to reach 0 lag and commit offsets\r 3. Wait for MM2 to translate the offsets to get 0 lag on the target\r 4. Alter target ACLs and start target producers\r 5. Start the target consumers\r 6. Stop MM2 mirroring\r \r Step 3 is the synchronization point, because if you perform step (4) early, some of the newly produced data will be dropped (data loss), and if you perform step (5) early, MM2 won't be able to sync offsets into an active group (extra redelivery)."], "tasks": {"summarization": "MirrorMaker2 Offset Replication Issue - I am using *Apache Kafka 4.0*\u00a0with\u00a0*MirrorMaker 2*\u00a0to link the primary cluster ({*}clusterA{*}) to t...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19607 about?", "answer": "MirrorMaker2 Offset Replication Issue"}}}
{"issue_id": "KAFKA-19606", "project": "KAFKA", "title": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode", "status": "Open", "priority": "Major", "reporter": "tony tang", "assignee": "tony tang", "created": "2025-08-14T20:23:09.000+0000", "updated": "2025-10-22T16:22:58.000+0000", "labels": [], "description": "JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in combined kraft mode but it's expected to be b/w 0 and 1.\r \r This is an issue with combined mode specifically because both controller + broker are using the same Meter object in combined mode, defined in {{{}RequestThreadIdleMeter#requestThreadIdleMeter{}}}, but the controller and broker are using separate\u00a0{{KafkaRequestHandlerPool}} objects, where each object's {{{}threadPoolSize == KafkaConfig.numIoThreads{}}}. This means when calculating idle time, each pool divides by its own {{numIoThreads}}\u00a0value before reporting to the shared meter and\u00a0\u00a0{{RequestHandlerAvgIdlePercent}}\u00a0calculates the final result by accumulating all the values reported by all threads. However, since there are actually 2 \u00d7 numIoThreads total threads contributing to the metric, the denominator should be doubled to get the correct average.", "comments": [], "tasks": {"summarization": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode - JMX metrics RequestHandlerAvgIdlePercent reports a value close to 2 in combined kraft mode but it's ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19606 about?", "answer": "Anomaly of JMX metrics RequestHandlerAvgIdlePercent in kraft combined mode"}}}
{"issue_id": "KAFKA-19605", "project": "KAFKA", "title": "Fix the busy loop occurring in the broker observer", "status": "Resolved", "priority": "Blocker", "reporter": "Chia-Ping Tsai", "assignee": "Kevin Wu", "created": "2025-08-14T06:56:34.000+0000", "updated": "2025-08-15T14:44:45.000+0000", "labels": [], "description": "see this comment https://github.com/apache/kafka/pull/19589#discussion_r2274143020", "comments": [], "tasks": {"summarization": "Fix the busy loop occurring in the broker observer - see this comment https://github.com/apache/kafka/pull/19589#discussion_r2274143020...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19605 about?", "answer": "Fix the busy loop occurring in the broker observer"}}}
{"issue_id": "KAFKA-19604", "project": "KAFKA", "title": "Document controller.quorum.auto.join.enable config in upgrade.html", "status": "Resolved", "priority": "Major", "reporter": "Kevin Wu", "assignee": "Lan Ding", "created": "2025-08-13T15:14:23.000+0000", "updated": "2025-09-16T08:34:19.000+0000", "labels": [], "description": "", "comments": ["[~kevinwu2412] Please feel free to unassign it if you don\u2019t have the bandwidth. I will find another contributor to take it over. :)", "[~chia7712] Someone else can take it over and I can give an initial review. It would be good to make others aware of this new functionality for KRaft at a high level.", "[~isding_l] Please provide the best documentation you can :)"], "tasks": {"summarization": "Document controller.quorum.auto.join.enable config in upgrade.html - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19604 about?", "answer": "Document controller.quorum.auto.join.enable config in upgrade.html"}}}
{"issue_id": "KAFKA-19603", "project": "KAFKA", "title": "Change log.segment.bytes configuration type from int to long to support segments larger than 2GB", "status": "Open", "priority": "Major", "reporter": "Mikhail Fesenko", "assignee": null, "created": "2025-08-13T09:43:46.000+0000", "updated": "2025-08-13T10:00:54.000+0000", "labels": [], "description": "h2. Description\r h3. Summary\r \r Change the data type of *{{log.segment.bytes}}* configuration from *{{int}}* to *{{long}}* to allow segment sizes beyond the current 2GB limit imposed by the integer maximum value.\r h3. Current Limitation\r \r The {{*log.segment.bytes*}} configuration currently uses an *{{int}}* data type, which limits the maximum segment size to ~2GB (2,147,483,647 bytes). This constraint becomes problematic for modern high-capacity storage deployments.\r h3. Background: Kafka Log Segment Structure\r \r Each Kafka topic partition consists of multiple log segments stored as separate files on disk. For each segment, Kafka maintains three core files:\r  * {*}{{.log}} files{*}: Contain the actual message data\r  * {*}{{.index}} files{*}: Store mappings between message offsets and their physical positions within the log file, allowing Kafka to quickly locate messages by their offset without scanning the entire log file\r  * {*}{{.timeindex}} files{*}: Store mappings between message timestamps and their corresponding offsets, enabling efficient time-based retrieval of messages\r \r h3. Motivation\r  # {*}Modern Hardware Capabilities{*}: Current deployments often use high-capacity storage (e.g., EPYC servers with 4\u00d715TB drives) where 2GB segments are inefficiently small\r  # {*}File Handle Optimization{*}: Large Kafka deployments with many topics can have 50-100k open files across all segment types (.log, .index, .timeindex files). Each segment requires open file handles, and larger segments would reduce the total number of files and improve caching efficiency\r  # {*}Performance Benefits{*}: Fewer segment rotations in high-traffic scenarios would reduce I/O overhead and improve overall performance. Sequential disk operations are much faster than random access patterns\r  # {*}Storage Efficiency{*}: Reducing segment file proliferation improves filesystem metadata performance and reduces inode usage on high-volume deployments\r  # {*}Community Interest{*}: Similar requests have been raised in community forums (see [Confluent forum discussion|https://forum.confluent.io/t/what-happens-if-i-increase-log-segment-bytes/5845])\r \r h3. Proposed Solution\r \r Change *{{log.segment.bytes}}* from *{{int}}* to *{{long}}* data type, allowing segment sizes of 3-4GB or larger to better align with modern storage capabilities.\r h3. Technical Considerations (Raised by Community)\r \r Based on dev mailing list discussion:\r  # {*}Index File Format Limitation{*}: Current index files use 4 bytes to represent file positions within segments, assuming 2GB cap (Jun Rao). This means:\r  ** {{.index}} files store offset-to-position mappings using 4-byte integers for file positions\r  ** If segments exceed 2GB, position values would overflow the 4-byte limit\r  ** Index format may need to be updated to support 8-byte positions\r  # {*}RemoteLogSegmentMetadata Interface{*}: Public interface currently uses {{int}} for {{segmentSizeInBytes}} and may need updates (Jun Rao)\r  # {*}Segment File Ecosystem Impact{*}: Need to evaluate impact on all three file types (.log, .index, .timeindex) and their interdependencies\r  # {*}Impact Assessment{*}: Need to evaluate all components that assume 2GB segment limit\r \r h3. Questions for Discussion\r  # What would be a reasonable maximum segment size limit?\r  # Should this change be backward compatible or require a protocol/format version bump?\r  # Are there any other components beyond index files and RemoteLogSegmentMetadata that need updates?\r \r h3. Expected Benefits\r  * Reduced number of segment files for high-volume topics\r  * Improved file handle utilization and caching efficiency\r  * Better alignment with modern storage hardware capabilities\r  * Reduced segment rotation overhead in high-traffic scenarios\r \r h3. Acceptance Criteria\r  * {{log.segment.bytes}} accepts long values > 2GB\r  * Index file format supports larger segments (if needed)\r  * RemoteLogSegmentMetadata interface updated (if needed)\r  * Backward compatibility maintained\r  * Documentation updated\r  * Unit and integration tests added\r \r *Disclaimer*\r \r I'm relatively new to Kafka internals and the JIRA contribution process. The original idea and motivation came from my experience with large-scale deployments, but I used Claude AI to help make this ticket more detailed and technically structured. There may be technical inaccuracies or missing implementation details that I haven't considered.\r This ticket is open for community discussion and feedback before implementation. \r *Expert review and guidance would be greatly appreciated.*", "comments": ["[~proggga] you might want to create a KIP and add details there. Then, you can trigger disc on mailing list. Guidelines here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"], "tasks": {"summarization": " Change log.segment.bytes configuration type from int to long to support segments larger than 2GB - h2. Description\r h3. Summary\r \r Change the data type of *{{log.segment.bytes}}* configuration from *...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19603 about?", "answer": "Change log.segment.bytes configuration type from int to long to support segments larger than 2GB"}}}
{"issue_id": "KAFKA-19602", "project": "KAFKA", "title": "Kafka Streams join after unmaterialized transformValues on KTable with extra store fails", "status": "Open", "priority": "Minor", "reporter": "Florens Pauwels", "assignee": null, "created": "2025-08-13T07:55:26.000+0000", "updated": "2025-08-13T07:55:57.000+0000", "labels": [], "description": "I believe for this to occur you need\r  # transformValues on a KTable, followed by a KTable join or leftJoin\r  # The transformValues is not materialized (no store name given)\r  # The transformValues accesses at least one extra store\r \r Tested on 3.6.1 and 3.9.1\r \r Example code:\r {code:java}\r @Component\r class TestCase {\r     private static final StoreBuilder<TimestampedKeyValueStore<String, String>> TRANSFORMER_STORE =\r           Stores.timestampedKeyValueStoreBuilder(\r                 Stores.persistentTimestampedKeyValueStore(\"transformer-store\"),\r                 Serdes.String(),\r                 Serdes.String()\r           );\r \r     private final StreamsBuilder streamsBuilder;\r \r     TestCase(StreamsBuilder streamsBuilder) {\r        this.streamsBuilder = streamsBuilder;\r     }\r \r     @PostConstruct\r     void configure() {\r        streamsBuilder.addStateStore(TRANSFORMER_STORE);\r \r        var aggregateTable = streamsBuilder\r              .stream(\"input\", Consumed.with(Serdes.String(), Serdes.String()).withName(\"input-to-stream\"))\r              .toTable(Named.as(\"to-table\"), MaterializedAs.keyValue(\"aggregate-store\",\r                    Serdes.String(), Serdes.String()))\r              .transformValues(MyTransformer::new,\r                    Materialized.with(Serdes.String(), Serdes.String()),\r                    Named.as(\"my-transformer\"), TRANSFORMER_STORE.name());\r \r        aggregateTable\r              .join(aggregateTable,\r                    (value, _) -> value,\r                    Named.as(\"after-transformer\"),\r                    Materialized.<String, String, KeyValueStore<Bytes, byte[]>>as(\"after-transformer-store\")\r                          .withKeySerde(Serdes.String())\r                          .withValueSerde(Serdes.String()))\r              .toStream(Named.as(\"aggregate-to-stream\"))\r              .to(\"output\", Produced.with(Serdes.String(), Serdes.String()).withName(\"output-to-topic\"));\r \r        System.out.println(streamsBuilder.build().describe().toString());\r     }\r \r     private static class MyTransformer implements ValueTransformerWithKey<String, String , String> {\r        @Override\r        public void init(ProcessorContext context) {\r           context.getStateStore(TRANSFORMER_STORE.name());\r        }\r \r        @Override\r        public String transform(String readOnlyKey, String value) {\r           return value;\r        }\r \r        @Override\r        public void close() {\r        }\r     }\r }\r  {code}\r Result of the above code:\r \r \u00a0\r {noformat}\r org.apache.kafka.streams.errors.StreamsException: failed to initialize processor after-transformer-join-this\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:131) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:140) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.initializeTopology(StreamTask.java:1089) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.completeRestoration(StreamTask.java:295) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.transitRestoredTaskToRunning(TaskManager.java:980) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.handleRestoredTasksFromStateUpdater(TaskManager.java:1055) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.checkStateUpdater(TaskManager.java:920) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.checkStateUpdater(StreamThread.java:1191) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:999) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:713) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:672) ~[kafka-streams-3.9.1.jar:na]\r Caused by: org.apache.kafka.streams.errors.StreamsException: Processor after-transformer-join-this has no access to StateStore transformer-store as the store is not connected to the processor. If you add stores manually via '.addStateStore()' make sure to connect the added store to the processor by providing the processor name to '.addStateStore()' or connect them via '.connectProcessorAndStateStores()'. DSL users need to provide the store name to '.process()', '.transform()', or '.transformValues()' to connect the store to the corresponding operator, or they can provide a StoreBuilder by implementing the stores() method on the Supplier itself. If you do not add stores manually, please file a bug report at https://issues.apache.org/jira/projects/KAFKA.\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.getStateStore(ProcessorContextImpl.java:174) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.ForwardingDisabledProcessorContext.getStateStore(ForwardingDisabledProcessorContext.java:90) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at be.florens.kafkaspringtest.selfjoin.TestCase$MyTransformer.init(TestCase.java:63) ~[main/:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.kstream.internals.KTableTransformValues$KTableTransformValuesGetter.init(KTableTransformValues.java:156) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.kstream.internals.KTableKTableInnerJoin$KTableKTableJoinProcessor.init(KTableKTableInnerJoin.java:83) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.ProcessorNode.init(ProcessorNode.java:123) ~[kafka-streams-3.9.1.jar:na]\r \u00a0 \u00a0 ... 10 common frames omitted{noformat}", "comments": [], "tasks": {"summarization": "Kafka Streams join after unmaterialized transformValues on KTable with extra store fails - I believe for this to occur you need\r  # transformValues on a KTable, followed by a KTable join or l...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19602 about?", "answer": "Kafka Streams join after unmaterialized transformValues on KTable with extra store fails"}}}
{"issue_id": "KAFKA-19601", "project": "KAFKA", "title": "always disallow min.insync.replicas at the broker level", "status": "Open", "priority": "Major", "reporter": "Jun Rao", "assignee": "Jhen-Yung Hsu", "created": "2025-08-12T18:28:57.000+0000", "updated": "2025-09-02T17:02:16.000+0000", "labels": [], "description": "In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.replicas at the broker level; (2) automatically add min.insync.replicas at the cluster level, if not present; (3) disallow removing min.insync.replicas at the cluster level. \u00a0The reason for this is that if brokers disagree about which partitions are under min ISR, it breaks the KIP-966 replication invariants.\r \r However, even if ELR is not enabled, it's bad to have different min.insync.replicas on different brokers since if a leader is moved to a different broker, it will behave differently on the min.insync.replicas semantic. So, it's probably better to always enforce the above regardless whether ELR is enabled or not. Similarly, we probably want to do the same for at least unclean.leader.election.enable.\r \r Since this is a public facing change, it requires a KIP.", "comments": ["I'm working on this, thanks :)", "Thanks, [~yung] !", "Nice idea, Perhaps we could do the same for `message.max.bytes`, `log.message.timestamp.type` and `log.cleanup.policy` as well. Those configs should be set only at topic-level and cluster-level.", "Sure :) I\u2019ll handle those configs as well and try to come up with a flexible way, since there could be more later.", "I've started the discussion thread: [https://lists.apache.org/thread/xt55os99ssk1ctpwb739zzk0vcn69b3f]\r Thanks for waiting, I\u2019d love to hear your thoughts!"], "tasks": {"summarization": "always disallow min.insync.replicas at the broker level - In [https://github.com/apache/kafka/pull/17952], if ELR is enabled, we (1) disallow min.insync.repli...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19601 about?", "answer": "always disallow min.insync.replicas at the broker level"}}}
{"issue_id": "KAFKA-19600", "project": "KAFKA", "title": "kraft Add/RemoveVoterHandlers do not check request's timeout", "status": "Open", "priority": "Major", "reporter": "Kevin Wu", "assignee": "Kevin Wu", "created": "2025-08-12T16:15:31.000+0000", "updated": "2025-08-12T16:34:29.000+0000", "labels": [], "description": "", "comments": ["Hi [~kevinwu2412] , may I take this one?", "Hi [~brandboat], apologies I meant to assign it to myself, since I'm still not sure about the best way to handle this properly yet.", "Gotcha, thanks for letting me know :)"], "tasks": {"summarization": "kraft Add/RemoveVoterHandlers do not check request's timeout - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19600 about?", "answer": "kraft Add/RemoveVoterHandlers do not check request's timeout"}}}
{"issue_id": "KAFKA-19599", "project": "KAFKA", "title": "Reduce the frequency of ReplicaNotAvailableException thrown to clients when RLMM is not ready", "status": "Resolved", "priority": "Major", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "created": "2025-08-12T16:00:22.000+0000", "updated": "2025-08-25T09:50:00.000+0000", "labels": [], "description": "During broker restarts, the topic-based RemoteLogMetadataManager constructs the state by reading the internal {{__remote_log_metadata}} topic. When the partition is not ready to perform remote storage operations, then [ReplicaNotAvailableException|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L127] thrown back to the consumer. The clients retries the request immediately.\u00a0\r \r This result in lot of FetchConsumer requests on the broker and utilizes the request handler threads. Using CountdownLatch the frequency of ReplicaNotAvailableException thrown back to the clients can be reduced. This will improve the request handler thread usage on the broker.\r \r Reproducer:\u00a0\r # Standalone one node cluster with LocalTieredStorage setup.\u00a0\r # Create a topic with remote storage enabled. RF = 1 and partitionCount = 2\r # Produce few message and ensure that the segments are uploaded to remote storage.\u00a0\r # Use console-consumer to read the produced messages from the beginning of the topic.\r # Update [RemoteLogMetadataPartitionStore|https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/metadata/storage/RemotePartitionMetadataStore.java?L166] to micmic that the partition is not ready.\r # Replace the kafka-storage module jar and restart the broker.\u00a0\r # Start the console-consumer to read from the beginning of the topic.\u00a0\u00a0\r \r ~9K FetchConsumer requests per second are received on the broker for one consumer [1107088 / (60 * 2) = ~9225 requests / sec per partition]:\r {code:java}\r % sh kafka-topics.sh --bootstrap-server localhost:9092  --topic apple --replication-factor 1 --partitions 2 --create  --config segment.bytes=1048576 --config local.retention.ms=60000 --config remote.storage.enable=true\r % sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic apple --from-beginning --property print.key=false --property print.value=false\r # broker logs\r  % less nohup.out | grep \u00a0\"Error occurred while reading the remote data for 4ChgxqKOTPakBikyo0Thjw\" \u00a0| grep -c \"2025-08-12 21:18\" \r 1107088\r {code}", "comments": [], "tasks": {"summarization": "Reduce the frequency of ReplicaNotAvailableException thrown to clients when RLMM is not ready - During broker restarts, the topic-based RemoteLogMetadataManager constructs the state by reading the...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19599 about?", "answer": "Reduce the frequency of ReplicaNotAvailableException thrown to clients when RLMM is not ready"}}}
{"issue_id": "KAFKA-19598", "project": "KAFKA", "title": "Consistency of command-line arguments for kafka-producer-perf-test.sh", "status": "Resolved", "priority": "Major", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "created": "2025-08-12T11:37:40.000+0000", "updated": "2025-08-22T17:14:18.000+0000", "labels": [], "description": "This implements KIP-1147 for kafka-producer-perf-test.sh.", "comments": [], "tasks": {"summarization": "Consistency of command-line arguments for kafka-producer-perf-test.sh - This implements KIP-1147 for kafka-producer-perf-test.sh....", "classification": "task", "qna": {"question": "What is the issue KAFKA-19598 about?", "answer": "Consistency of command-line arguments for kafka-producer-perf-test.sh"}}}
{"issue_id": "KAFKA-19597", "project": "KAFKA", "title": "Stop the RSM after closing the remote-log reader threads to handle requests gracefully", "status": "Resolved", "priority": "Minor", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "created": "2025-08-12T10:15:05.000+0000", "updated": "2025-08-21T20:40:43.000+0000", "labels": [], "description": "During shutdown, when the RemoteStorageManager closes first, then the ongoing requests are thrown with error. To handle the ongoing requests gracefully, closing the RSM after closing the remote-log reader thread pools.\u00a0\r \r \u00a0\r \r https://sourcegraph.com/github.com/apache/kafka/-/blob/storage/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogManager.java?L2035", "comments": [], "tasks": {"summarization": "Stop the RSM after closing the remote-log reader threads to handle requests gracefully - During shutdown, when the RemoteStorageManager closes first, then the ongoing requests are thrown wi...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19597 about?", "answer": "Stop the RSM after closing the remote-log reader threads to handle requests gracefully"}}}
{"issue_id": "KAFKA-19596", "project": "KAFKA", "title": "Log auto topic creation failures more visibly", "status": "Resolved", "priority": "Trivial", "reporter": "Rob Young", "assignee": "Rob Young", "created": "2025-08-11T22:04:03.000+0000", "updated": "2025-08-25T12:24:28.000+0000", "labels": [], "description": "Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failure to auto create the `__share_group_state` topic due to the replication factor not being satisfiable.\r \r I start up a cluster like this (taken from the dockerhub instructions):\r {code:java}\r podman run --rm \\\r \u00a0 -p 9092:9092 \\\r \u00a0 -e KAFKA_NODE_ID=1 \\\r \u00a0 -e KAFKA_PROCESS_ROLES=broker,controller \\\r \u00a0 -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\\r \u00a0 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \\\r \u00a0 -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r \u00a0 -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\\r \u00a0 -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \\\r \u00a0 -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \\\r \u00a0 -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \\\r \u00a0 -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \\\r \u00a0 -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \\\r \u00a0 -e KAFKA_NUM_PARTITIONS=3 \\\r \u00a0 apache/kafka:4.1.0-rc2{code}\r and then run the commands:\r {code:java}\r robeyoun:kafka_2.13-4.1.0$ bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --feature share.version=1\r share.version was upgraded to 1.\r robeyoun:kafka_2.13-4.1.0$ bin/kafka-topics.sh --create --topic my_topic --bootstrap-server localhost:9092\r WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\r Created topic my_topic.\r robeyoun:kafka_2.13-4.1.0$ bin/kafka-console-share-consumer.sh --bootstrap-server localhost:9092 --topic quickstart-events\r [2025-08-12 09:50:13,130] WARN Share groups and KafkaShareConsumer are part of a preview feature introduced by KIP-932, and are not recommended for use in production. (org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator)\r [2025-08-12 09:50:13,357] WARN [ShareConsumer clientId=console-share-consumer, groupId=console-share-consumer] The metadata response from the cluster reported a recoverable issue with correlation id 20 : {quickstart-events=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient)\r {code}\r the consumer now sits there, appearing to be consuming happily, but the broker is emitting a stream of logs like:\r {code:java}\r [2025-08-11 21:51:03,648] INFO Sent auto-creation request for Set(__share_group_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\r [2025-08-11 21:51:03,648] WARN Received retriable error in find coordinator for InitializeStateHandler using key SharePartitionKey{groupId=console-share-consumer, topicIdPartition=sI9vYBBGSKW_3BfG9ZJWhg:null-1}: The coordinator is not available. (org.apache.kafka.server.share.persister.PersisterStateManager$InitializeStateHandler){code}\r So there's a clue there that it's repeatedly trying to auto-create the topic.\r \r the underlying problem is the default replication factor of 3 for the __share_group_state topic, but I have to crank up the log level to DEBUG (setting -e KAFKA_LOG4J_LOGGERS=\"kafka=DEBUG\") to see the cause:\r {code:java}\r [2025-08-11 21:53:58,343] INFO Sent auto-creation request for Set(__share_group_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)\r [2025-08-11 21:53:58,343] WARN Received retriable error in find coordinator for InitializeStateHandler using key SharePartitionKey{groupId=console-share-consumer, topicIdPartition=8kjLkvNzSCy7geIIriJVCQ:null-2}: The coordinator is not available. (org.apache.kafka.server.share.persister.PersisterStateManager$InitializeStateHandler)\r [2025-08-11 21:53:58,345] DEBUG [broker-1-to-controller-forwarding-channel-manager]: Request CreateTopicsRequestData(topics=[CreatableTopic(name='__share_group_state', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreatableTopicConfig(name='compression.type', value='producer'), CreatableTopicConfig(name='cleanup.policy', value='delete'), CreatableTopicConfig(name='min.insync.replicas', value='2'), CreatableTopicConfig(name='segment.bytes', value='104857600'), CreatableTopicConfig(name='retention.ms', value='-1')])], timeoutMs=30000, validateOnly=false) received ClientResponse(receivedTimeMs=1754949238345, latencyMs=2, disconnected=false, timedOut=false, requestHeader=RequestHeader(apiKey=CREATE_TOPICS, apiVersion=7, clientId=1, correlationId=22, headerVersion=2), responseBody=CreateTopicsResponseData(throttleTimeMs=0, topics=[CreatableTopicResult(name='__share_group_state', topicId=AAAAAAAAAAAAAAAAAAAAAA, errorCode=38, errorMessage='Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.', topicConfigErrorCode=0, numPartitions=-1, replicationFactor=-1, configs=[])])) (kafka.server.NodeToControllerRequestThread)\r [2025-08-11 21:53:58,345] DEBUG Cleared inflight topic creation state for HashMap(__share_group_state -> CreatableTopic(name='__share_group_state', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreatableTopicConfig(name='compression.type', value='producer'), CreatableTopicConfig(name='cleanup.policy', value='delete'), CreatableTopicConfig(name='min.insync.replicas', value='2'), CreatableTopicConfig(name='segment.bytes', value='104857600'), CreatableTopicConfig(name='retention.ms', value='-1')])) (kafka.server.DefaultAutoTopicCreationManager)\r [2025-08-11 21:53:58,345] DEBUG Auto topic creation completed for Set(__share_group_state) with response CreateTopicsResponseData(throttleTimeMs=0, topics=[CreatableTopicResult(name='__share_group_state', topicId=AAAAAAAAAAAAAAAAAAAAAA, errorCode=38, errorMessage='Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.', topicConfigErrorCode=0, numPartitions=-1, replicationFactor=-1, configs=[])]). (kafka.server.DefaultAutoTopicCreationManager)\r  {code}\r The point where we log the auto creation outcome has two cases where it warn logs, but then this case is being handled by a catch all debug log. [https://github.com/apache/kafka/blob/18045c6ac30921503deffbef1744bb365dc599fb/core/src/main/scala/kafka/server/AutoTopicCreationManager.scala#L141]\r \r If there's an error code set, maybe it should log at warn like the other cases? I'm happy to be assigned to this.", "comments": ["[~robyoung] Thanks for this issue. It would be good to improve this area. I think we should also improve the instructions you followed, probably when share groups are enabled by default (AK 4.2, I hope). After all, we have specific configs in the config/server.properties that ship with AK to reflect the single-broker config requirements, and that's appropriate for what you did with Docker too I think.", "Hi [~schofielaj]. Yes the docker images work smoothly when you don't set any of the configuration environment variables, as the default configuration file tunes the replication factors/isrs down to 1. The docs make it pretty clear that if you are using environment variable configuration that you are responsible for configuring {_}everything{_}.\r \r I guess the examples of environment variable configuration [https://hub.docker.com/r/apache/kafka#overriding-the-default-broker-configuration] could be updated since they already contain the equivalent settings for the transaction state topic. I imagine these examples should represent a working single-node configuration that users can then modify."], "tasks": {"summarization": "Log auto topic creation failures more visibly - Hi, I was playing with Share groups on the 4.1.0-rc2 and found it a little opaque to detect a failur...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19596 about?", "answer": "Log auto topic creation failures more visibly"}}}
{"issue_id": "KAFKA-19595", "project": "KAFKA", "title": "Add configuration validation option to Kafka server startup", "status": "Open", "priority": "Minor", "reporter": "Mikhail Fesenko", "assignee": null, "created": "2025-08-11T12:39:52.000+0000", "updated": "2025-08-29T13:30:31.000+0000", "labels": [], "description": "*Problem Statement:*\u00a0Currently, Kafka administrators have no way to validate server configuration files without actually starting the Kafka broker. This leads to:\r  * Wasted time during deployments when configuration errors are discovered only at startup\r  * Potential service disruptions in production environments\r  * Difficulty in CI/CD pipelines to validate Kafka configurations before deployment\r  * No quick way to test configuration changes without full broker startup overhead\r  * *Critical cluster stability issues during rolling restarts*\u00a0- misconfigured brokers can cause:\r  ** Partition leadership imbalances\r  ** Replication factor violations\r  ** Network connectivity issues between brokers\r  ** Data consistency problems\r  ** Cascading failures across the cluster when multiple brokers restart with incompatible configurations\r \r \r *Proposed Solution:*\u00a0Add a {*}--check-config{*}{{{}{}}}\u00a0command-line option to the Kafka server startup script that would:\r \r \u00a0\r  * Parse and validate the server configuration file\r  * Check for common configuration errors and inconsistencies\r  * Validate property values and ranges\r  * *Detect configuration incompatibilities that could affect cluster operations*\r  * Support property overrides for testing different configurations\r  * Exit with appropriate status codes (0 for valid config, non-zero for errors)\r  * Provide clear error messages for invalid configurations\r \r *Usage Example:*\r {code:java}\r # Validate default server.properties\r kafka-server-start.sh --check-config config/server.properties\r \r # Validate with property overrides\r kafka-server-start.sh --check-config config/server.properties --override broker.id=1,log.dirs=/tmp/kafka-logs {code}\r \u00a0\r *Expected Benefits:*\r  * Faster feedback loop for configuration changes\r  * Reduced deployment failures due to configuration issues\r  * Better integration with automated deployment pipelines\r  * Improved operational efficiency for Kafka administrators\r  * *Prevention of cluster-wide issues during rolling restarts and maintenance*\r  * *Early detection of configuration drift across cluster nodes*\r  * *Reduced risk of data loss or corruption from misconfigured brokers*\r  * Consistent validation logic with the actual server startup process\r \r \r *Acceptance Criteria:*\r  * \r --check-config{{}}\u00a0command-line option to trigger config validation mode\r  * Comprehensive validation of server properties\r  * Clear error reporting with specific issues identified\r  * Support for property overrides during validation\r  * Exit codes that can be used in scripts and automation\r  * Documentation and usage examples\r \r \r This feature would significantly improve the operational experience for Kafka deployments and reduce configuration-related issues in production environments, especially in multi-broker cluster scenarios where configuration consistency is critical.", "comments": ["Hi , can I work on this ?"], "tasks": {"summarization": "Add configuration validation option to Kafka server startup - *Problem Statement:*\u00a0Currently, Kafka administrators have no way to validate server configuration fi...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19595 about?", "answer": "Add configuration validation option to Kafka server startup"}}}
{"issue_id": "KAFKA-19594", "project": "KAFKA", "title": "Multiple-partition topic support for TopologyTestDriver", "status": "Open", "priority": "Minor", "reporter": "Herman Kolstad Jakobsen", "assignee": null, "created": "2025-08-11T06:09:28.000+0000", "updated": "2025-08-14T08:46:37.000+0000", "labels": [], "description": "The TopologyTestDriver is great for writing fast and reliable tests for Streams applications. However, a shortcoming of the TopologyTestDriver is that it only supports topics with one partition, therefore making it difficult to test that data has been correctly partitioned throughout the entire topology. This is especially relevant for topologies utilizing PAPI, where the user has to manage the partitioning of the data.\r \r A simple starting point could be to keep the current fully synchronous execution for a single input event, just with a TestInputTopic that supports a \u201cPartitioner\u201d.", "comments": [], "tasks": {"summarization": "Multiple-partition topic support for TopologyTestDriver - The TopologyTestDriver is great for writing fast and reliable tests for Streams applications. Howeve...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19594 about?", "answer": "Multiple-partition topic support for TopologyTestDriver"}}}
{"issue_id": "KAFKA-19593", "project": "KAFKA", "title": "Stuck __consumer_offsets partition (kafka streams app)", "status": "Open", "priority": "Major", "reporter": "Matej Pucihar", "assignee": null, "created": "2025-08-11T05:32:13.000+0000", "updated": "2025-08-11T05:32:13.000+0000", "labels": ["kafka-streams"], "description": "h3. Problem Summary\r \r My Kafka Streams application cannot move its {{state_store}} from {{STARTING}} to {{{}RUNNING{}}}.\r \r I'm using a *Strimzi Kafka cluster* with:\r  * 3 *controller nodes*\r \r  * 4 *broker nodes*\r \r h3. Observations\r h4. Partition {{__consumer_offsets-35}} is {*}stuck{*}.\r \r From AKHQ, partition details:\r  * *Broker 10* is the *leader* of {{__consumer_offsets-35}}\r \r  * There are *no interesting logs* on broker 10\r \r  * However, logs are *spamming every 10ms* from broker 11 (a {*}replica{*}):\r \r 2025-08-11 04:05:50 INFO  [TxnMarkerSenderThread-11] TransactionMarkerRequestCompletionHandler:66 \r [Transaction Marker Request Completion Handler 10]: Sending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-4's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 38\r h4. Brokers 20 and 21 \u2014 neither leaders nor replicas \u2014 also spamming the same error:\r \r *Broker 20:*\r 2025-08-11 04:39:45 INFO  [TxnMarkerSenderThread-20] TransactionMarkerRequestCompletionHandler:66 \r Sending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-3's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 54\r \u00a0\r *Broker 21:*\r 2025-08-11 04:39:58 INFO  [TxnMarkerSenderThread-21] TransactionMarkerRequestCompletionHandler:66 \r Sending irm_r_sbm2_web-backend-web-1-6cad35c7-2be9-4ed2-9849-9c059cc8c409-2's transaction marker for partition __consumer_offsets-35 has failed with error org.apache.kafka.common.errors.NotLeaderOrFollowerException, retrying with current coordinator epoch 28\r \u00a0\r ----\r h3. Kafka Streams App Behavior\r \r Logs from the Kafka Streams app (at debug level) repeat continuously. The {{state_store}} *never transitions* from {{STARTING}} to {{{}RUNNING{}}}.\r \r Key repeated logs (debug log level):\r  * Polling main consumer repeatedly\r \r  * SASL/SCRAM authentication succeeds\r \r  * 0 records fetched\r \r  * 0 records processed\r \r  * Punctuators run, but nothing gets committed\r \r  * Fails to commit due to {*}rebalance in progress{*}, retrying\u2026\r \r {{}}\r ----\r h3. Workarounds Considered\r \r The *only thing that temporarily resolves the issue* is:\r  * Physically deleting the partition files for {{__consumer_offsets-35}} from both the leader and replica brokers\r \r Other drastic options:\r  * Deleting the entire {{__consumer_offsets}} topic\r \r  * Re-creating the entire Kafka cluster\r \r ----\r h3. Additional Info\r  * I cannot reproduce this in a *clean git project*\r \r  * The issue is isolated to a {*}\"corrupt\" cluster{*}, which is still available for inspection\r \r  * This problem has occurred *4 times* in the *past month*\r \r  * It *started happening after upgrading from Strimzi 3.9 to 4.0*\r \r  * I'm using quarkus (kafka-stream version is 4.0.0) with default configuration, the only config worth mentioning is that I'm using exactly_once_v2 processing guarantee.\r \r ----\r h3. Help Needed\r \r I'm hoping someone can {*}make sense of this issue{*}.\r \r Please feel free to *reach out.*", "comments": [], "tasks": {"summarization": "Stuck __consumer_offsets partition (kafka streams app) - h3. Problem Summary\r \r My Kafka Streams application cannot move its {{state_store}} from {{STARTING}...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19593 about?", "answer": "Stuck __consumer_offsets partition (kafka streams app)"}}}
{"issue_id": "KAFKA-19592", "project": "KAFKA", "title": "testGenerateAssignmentWithBootstrapServer uses wrong JSON format", "status": "Resolved", "priority": "Major", "reporter": "PoAn Yang", "assignee": "TaiJuWu", "created": "2025-08-11T02:20:06.000+0000", "updated": "2025-08-11T12:14:53.000+0000", "labels": [], "description": "In ReassignPartitiosnsCommand#generateAssignment, it uses JSON format like:\r \r \u00a0\r {code:java}\r {\r   \"topics\": [\r     { \"topic\": \"foo1\" },\r     { \"topic\": \"foo2\" }\r   ],\r   \"version\": 1\r } {code}\r However, in ReassignPartitionsCommandTest#testGenerateAssignmentWithBootstrapServer, it uses input like:\r {code:java}\r {\r   \"version\":1,\r   \"partitions\": [\r     {\r       \"topic\": \"foo\",\r       \"partition\": 0,\r       \"replicas\": [3, 1, 2],\r       \"log_dirs\": [\"any\",\"any\",\"any\"]\r     }\r   ]\r }{code}\r The test case can pass, but it doesn't test `generateAssignment` correctly.", "comments": [], "tasks": {"summarization": "testGenerateAssignmentWithBootstrapServer uses wrong JSON format - In ReassignPartitiosnsCommand#generateAssignment, it uses JSON format like:\r \r \u00a0\r {code:java}\r {\r   ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19592 about?", "answer": "testGenerateAssignmentWithBootstrapServer uses wrong JSON format"}}}
{"issue_id": "KAFKA-19591", "project": "KAFKA", "title": "Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)", "status": "Resolved", "priority": "Minor", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Dejan Stojadinovi\u0107", "created": "2025-08-08T15:05:11.000+0000", "updated": "2025-10-07T10:24:09.000+0000", "labels": [], "description": "{panel:bgColor=#ffffce}\r *_Prologue: Kafka developers can't commit jar's into Gir repo (that includes Gradle wrapper jar_*\r Related links:\r  * https://issues.apache.org/jira/browse/KAFKA-2098?focusedCommentId=14481979&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14481979\r  * https://issues.apache.org/jira/browse/KAFKA-1490\r  * [https://stackoverflow.com/questions/39856714/gradle-wrapper-without-the-jar]\r  * [https://discuss.gradle.org/t/how-hand-compile-gradle-wrapper-jar-when-gradle-cannot-be-installed-and-cannot-check-jar-files-into-git/4813]{panel}\r \u00a0\r \r *(i) Intro:* Groovy file _*unixStartScript.txt*_ (that servers as a template for a {_}*gradlew*{_}) path/module was changed in Gradle version 8.8.0\r \r  _*(x) Problem description:*_\r  # at the moment Kafka trunk branch is using Gradle version [8.14.1|https://github.com/apache/kafka/blob/8deb6c6911616f887ebb2678f3f12ee1da09a618/gradle/wrapper/gradle-wrapper.properties] but thing is that _*gradlew*_ is referencing Gradle 8.7.0 template\u00a0file _*unixStartScript.txt*_\r  # it means that _*gradlew*_ is missing all recent changes for a template file _*unixStartScript.txt*_\r \r *_Related GitHub Gradle links for unixStartScript.txt:_*\r  * Gradle <=8.7.0 version\r  ** file path: [https://github.com/gradle/gradle/blob/v8.7.0/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r  ** Git history for versions <=8.7.0: [https://github.com/gradle/gradle/commits/v8.7.0/subprojects/plugins/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r  * Gradle >= 8.8.0 version:\r  ** file path: [https://github.com/gradle/gradle/blob/v8.8.0/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r  ** Git history for versions [8.8.0, 8.14.1]: [https://github.com/gradle/gradle/commits/v8.14.1/platforms/jvm/plugins-application/src/main/resources/org/gradle/api/internal/plugins/unixStartScript.txt]\r \r !screenshot-1.png!\r \r *Other related links:*\r  * [https://github.com/gradle/gradle/compare/v8.7.0...v8.8.0]\r  * [https://github.com/gradle/gradle/commit/c5ec05d90780454c7c20c66533b804d80d67639f] Moving code out of plugins project to more appropriate sub-projects\r  * [https://github.com/gradle/gradle/commit/9041afac3b20f4878b5b0d59c1fb6b3b0982b53f] Refactor start scripts to support executable JAR\r \r *_-Loosely- related Gradle Github issue:_* [https://github.com/gradle/gradle/issues/30101] Use permalink in the generated gradlew file", "comments": ["-Canceling patch (but code will be available for a review elsewhere).-\r \r In more details: this ticket is a prerequisite for\u00a0KAFKA-19174 (and hence same GitHub PR will be used for both tickets).", "New patch (with code for KAFKA-19174) is submitted here:https://github.com/apache/kafka/pull/19513 \r \r -Progress is kind of blocked by these issues: KAFKA-19636\u00a0-\r \r See this comment for a latest update:\r https://github.com/apache/kafka/pull/19513#issuecomment-3213821218", "(Will be) implemented via KAFKA-19174 (and hence resolving).", "Reopening (jut to mark this one with \"patch available\"", "Patch is available, see linked GitHub PR link.", "Update: from my POV this ticket should be solved as a part of KAFKA-19174 (note: it can be solved in isolation, but in that case it should be solved before KAFKA-19174).\r \r Rationale and related discussion: https://github.com/apache/kafka/pull/19513#issuecomment-3333523460", "fixed by KAFKA-19174"], "tasks": {"summarization": "Fix `gradlew` content: add missing parts (old Groovy template version is being referenced) - {panel:bgColor=#ffffce}\r *_Prologue: Kafka developers can't commit jar's into Gir repo (that include...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19591 about?", "answer": "Fix `gradlew` content: add missing parts (old Groovy template version is being referenced)"}}}
{"issue_id": "KAFKA-19590", "project": "KAFKA", "title": "Have a way to set config to admin client only in TopicBasedRemoteLogMetadataManager", "status": "Open", "priority": "Major", "reporter": "Luke Chen", "assignee": "Lan Ding", "created": "2025-08-08T09:15:00.000+0000", "updated": "2025-08-22T03:30:23.000+0000", "labels": ["need-kip"], "description": "Context:\r https://github.com/apache/kafka/pull/20306#discussion_r2262298636", "comments": [], "tasks": {"summarization": "Have a way to set config to admin client only in TopicBasedRemoteLogMetadataManager - Context:\r https://github.com/apache/kafka/pull/20306#discussion_r2262298636...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19590 about?", "answer": "Have a way to set config to admin client only in TopicBasedRemoteLogMetadataManager"}}}
{"issue_id": "KAFKA-19589", "project": "KAFKA", "title": "Reduce number of events generated in AsyncKafkaConsumer.updateFetchPositions()", "status": "Resolved", "priority": "Major", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-08-08T00:22:15.000+0000", "updated": "2025-09-17T15:44:20.000+0000", "labels": ["async-kafka-consumer-performance", "consumer-threading-refactor", "performance"], "description": "We create\u2014and wait on\u2014{{{}CheckAndUpdatePositionsEvents{}}} in {{updateFetchPositions()}} even though, in a stable system, 99.9%+ of the time there are no updates to perform.", "comments": [], "tasks": {"summarization": "Reduce number of events generated in AsyncKafkaConsumer.updateFetchPositions() - We create\u2014and wait on\u2014{{{}CheckAndUpdatePositionsEvents{}}} in {{updateFetchPositions()}} even thoug...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19589 about?", "answer": "Reduce number of events generated in AsyncKafkaConsumer.updateFetchPositions()"}}}
{"issue_id": "KAFKA-19588", "project": "KAFKA", "title": "Reduce waiting for event completion in AsyncKafkaConsumer.poll()", "status": "Resolved", "priority": "Major", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-08-08T00:21:29.000+0000", "updated": "2025-09-17T15:44:10.000+0000", "labels": ["async-kafka-consumer-performance", "consumer-threading-refactor", "performance"], "description": "We create\u2014and wait on\u2014{{{}PollEvent{}}} in {{Consumer.poll()}} to ensure we wait for reconciliation and/or auto-commit. However, reconciliation is relatively rare, and auto-commit only happens every\u00a0_N_ seconds, so the remainder of the time, we should try to avoid sending poll events.", "comments": [], "tasks": {"summarization": "Reduce waiting for event completion in AsyncKafkaConsumer.poll() - We create\u2014and wait on\u2014{{{}PollEvent{}}} in {{Consumer.poll()}} to ensure we wait for reconciliation ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19588 about?", "answer": "Reduce waiting for event completion in AsyncKafkaConsumer.poll()"}}}
{"issue_id": "KAFKA-19587", "project": "KAFKA", "title": "Unify kraft shutdown logic in poll methods", "status": "Open", "priority": "Major", "reporter": "Kevin Wu", "assignee": "Kevin Wu", "created": "2025-08-07T17:34:09.000+0000", "updated": "2025-08-23T14:26:03.000+0000", "labels": [], "description": "Currently, different KRaft replica states handle polling during graceful shutdown differently. This Jira aims to unify their logic.", "comments": [], "tasks": {"summarization": "Unify kraft shutdown logic in poll methods - Currently, different KRaft replica states handle polling during graceful shutdown differently. This ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19587 about?", "answer": "Unify kraft shutdown logic in poll methods"}}}
{"issue_id": "KAFKA-19586", "project": "KAFKA", "title": "Kafka broker freezes and gets fenced during rolling restart with KRaft mode", "status": "Open", "priority": "Blocker", "reporter": "Tal Asulin", "assignee": null, "created": "2025-08-07T12:50:46.000+0000", "updated": "2025-08-17T11:52:46.000+0000", "labels": [], "description": "After upgrading our Kafka clusters to *Kafka 3.9.0* with *KRaft mode enabled* in production, we started observing strange behavior during rolling restarts of broker nodes \u2014 behavior we had never seen before.\r \r When a broker is {*}gracefully shut down by the KRaft controller{*}, it immediately restarts. Shortly afterward, while it is busy {*}replicating missing data{*}, the broker suddenly {*}freezes for approximately 20\u201350 seconds{*}. During this time, it produces {*}no logs, no metrics{*}, and *no heartbeat messages* to the controllers (see the timestamps below).\r \r \u00a0\r {code:java}\r 2025-07-30 09:21:27,224 INFO [Broker id=8] Skipped the become-follower state change for my-topic-215 with topic id Some(yO4CQayIRbyESrHHVPdOrQ) and partition state LeaderAndIsrPartitionState(topicName='my-topic', partitionIndex=215, controllerEpoch=-1, leader=4, leaderEpoch=54, isr=[4, 8], partitionEpoch=102, replicas=[4, 8], addingReplicas=[], removingReplicas=[], isNew=false, leaderRecoveryState=0) since it is already a follower with leader epoch 54. (state.change.logger) [kafka-8-metadata-loader-event-handler]\r \r 2025-07-30 09:21:51,887 INFO [Broker id=8] Transitioning 427 partition(s) to local followers. (state.change.logger) [kafka-8-metadata-loader-event-handler] \u00a0 \u00a0  \u00a0{code}\r \u00a0\r \r After this \u201changing\u201d period, the broker *resumes normal operation* without emitting any error or warning messages \u2014 as if nothing happened. However, during this gap, because the broker fails to send heartbeats to the KRaft controllers, it gets *fenced out of the cluster* ([9s timeout|https://kafka.apache.org/documentation/#brokerconfigs_broker.session.timeout.ms]), which leads to {*}partitions going offline and, ultimately, data loss{*}.\r {code:java}\r 2025-07-30 09:21:40,325 INFO [QuorumController id=300] Fencing broker 8 because its session has timed out. (org.apache.kafka.controller.ReplicationControlManager) [quorum-controller-300-event-handler]{code}\r \u00a0\r \r We were able to re-produce this behaviour when provisioning clusters with high disk utilization.\u00a0\r \r Our primary suspicion is that the Kafka broker process might be *\u201cchoking\u201d under the load* \u2014 trying to replicate a large amount of data while also taking over leadership for many partitions. This may cause the JVM to stall, leading to the observed freeze.", "comments": ["Sharing a 5m Flame graph profiling snapshot that was taken after a broker restart that experienced this exact issue - [^flame3.html].\r \r The Kafka cluster spec during the simulation was:\r  * 12 Broker nodes\r  * Using 8 vCPUs, 32G of RAM & 3.7TB local volume (im4gn.2xlarge AWS instances)\r  * Disk utilization reached 60% during the simulation"], "tasks": {"summarization": "Kafka broker freezes and gets fenced during rolling restart with KRaft mode - After upgrading our Kafka clusters to *Kafka 3.9.0* with *KRaft mode enabled* in production, we star...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19586 about?", "answer": "Kafka broker freezes and gets fenced during rolling restart with KRaft mode"}}}
{"issue_id": "KAFKA-19585", "project": "KAFKA", "title": "Avoid noisy NPE logs when closing consumer after constructor failures", "status": "Resolved", "priority": "Minor", "reporter": "Lianet Magrans", "assignee": "Genseric Ghiro", "created": "2025-08-07T12:31:28.000+0000", "updated": "2025-09-09T14:36:59.000+0000", "labels": [], "description": "If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/lianetm/kafka/blob/2329def2ff9ca4f7b9426af159b6fa19a839dc4d/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L540\r In that case, it could be the case that some components may have not been created, so we should consider some null checks to avoid noisy logs about NPE. \r \r This noisy logs have been reported with the console share consumer in a similar scenario, so this task is to review and do a similar fix for the Async if needed.", "comments": ["Hi [~lianetm], I'm new to contributing to Kafka and would be happy to take a stab at this. Can I assign this ticket to myself?", "Hi [~francisgodinho]! Thanks for your interest! There is already someone from the team working on this :S \r But stay on the lookout for new issue that we create, and also maybe check what's already out with minor/trivial complexity (ex. [this filter|https://issues.apache.org/jira/browse/KAFKA-15642?jql=project%20%3D%20KAFKA%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened%2C%20%22Patch%20Available%22)%20AND%20priority%20in%20(Minor%2C%20Trivial)%20AND%20component%20%3D%20clients] for the clients space, but also check other components). Welcome to the community!"], "tasks": {"summarization": "Avoid noisy NPE logs when closing consumer after constructor failures - If there's a failure in the kafka consumer constructor, we attempt to close it https://github.com/li...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19585 about?", "answer": "Avoid noisy NPE logs when closing consumer after constructor failures"}}}
{"issue_id": "KAFKA-19584", "project": "KAFKA", "title": "Native docker image authentication fails with SASL PLAIN", "status": "Open", "priority": "Minor", "reporter": "Rob Young", "assignee": "Gaurav Narula", "created": "2025-08-06T23:30:57.000+0000", "updated": "2025-08-07T12:40:29.000+0000", "labels": ["native-image"], "description": "I'm trying to use the native docker image for SASL PLAIN authentication.\r \r The server starts okay but when I connect a client it emits an exception:\r \r \r \u00a0\r {code:java}\r [2025-08-06 23:20:47,302] WARN [SocketServer listenerType=BROKER, nodeId=1] Unexpected error from /192.168.178.96 (channelId=192.168.178.96:9092-192.168.178.96:42552-1-1); closing connection (org.apache.kafka.common.network.Selector) java.lang.UnsupportedOperationException: Unable to find suitable Subject#doAs or Subject#callAs implementation at org.apache.kafka.common.internals.UnsupportedStrategy.createException(UnsupportedStrategy.java:40) ~[?:?] at org.apache.kafka.common.internals.UnsupportedStrategy.callAs(UnsupportedStrategy.java:58) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.lambda$callAs$1(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.performAction(CompositeStrategy.java:78) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.callAs(CompositeStrategy.java:104) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.createSaslServer(SaslServerAuthenticator.java:208) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleKafkaRequest(SaslServerAuthenticator.java:533) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:281) ~[?:?] at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:181) ~[?:?] at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:548) [kafka.Kafka:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:486) [kafka.Kafka:?] at kafka.network.Processor.poll(SocketServer.scala:1017) [kafka.Kafka:?] at kafka.network.Processor.run(SocketServer.scala:921) [kafka.Kafka:?] at java.base/java.lang.Thread.runWith(Thread.java:1596) [kafka.Kafka:?] at java.base/java.lang.Thread.run(Thread.java:1583) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.thread.PlatformThreads.threadStartRoutine(PlatformThreads.java:833) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.ClassNotFoundException: java.security.AccessController at org.graalvm.nativeimage.builder/com.oracle.svm.core.hub.ClassForNameSupport.forName(ClassForNameSupport.java:122) ~[?:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.hub.ClassForNameSupport.forName(ClassForNameSupport.java:86) ~[?:?] at java.base/java.lang.Class.forName(DynamicHub.java:1356) ~[kafka.Kafka:?] at java.base/java.lang.Class.forName(DynamicHub.java:1345) ~[kafka.Kafka:?] at org.apache.kafka.common.internals.ReflectiveStrategy$Loader.lambda$forName$0(ReflectiveStrategy.java:66) ~[kafka.Kafka:?] at org.apache.kafka.common.internals.LegacyStrategy.<init>(LegacyStrategy.java:45) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.<init>(CompositeStrategy.java:49) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.<clinit>(CompositeStrategy.java:39) ~[?:?] at org.apache.kafka.common.internals.SecurityManagerCompatibility.get(SecurityManagerCompatibility.java:38) ~[kafka.Kafka:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.createSaslServer(SaslServerAuthenticator.java:208) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleKafkaRequest(SaslServerAuthenticator.java:533) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:281) ~[?:?] at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:181) ~[?:?] at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:548) [kafka.Kafka:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:486) [kafka.Kafka:?] at kafka.network.Processor.poll(SocketServer.scala:1017) [kafka.Kafka:?] at kafka.network.Processor.run(SocketServer.scala:921) [kafka.Kafka:?] at java.base/java.lang.Thread.runWith(Thread.java:1596) [kafka.Kafka:?] at java.base/java.lang.Thread.run(Thread.java:1583) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.thread.PlatformThreads.threadStartRoutine(PlatformThreads.java:833) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?] Suppressed: java.lang.NoSuchMethodException: javax.security.auth.Subject.current() at java.base/java.lang.Class.checkMethod(DynamicHub.java:1075) ~[kafka.Kafka:?] at java.base/java.lang.Class.getDeclaredMethod(DynamicHub.java:1165) ~[kafka.Kafka:?] at org.apache.kafka.common.internals.ModernStrategy.<init>(ModernStrategy.java:43) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.<init>(CompositeStrategy.java:60) ~[?:?] at org.apache.kafka.common.internals.CompositeStrategy.<clinit>(CompositeStrategy.java:39) ~[?:?] at org.apache.kafka.common.internals.SecurityManagerCompatibility.get(SecurityManagerCompatibility.java:38) ~[kafka.Kafka:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.createSaslServer(SaslServerAuthenticator.java:208) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.handleKafkaRequest(SaslServerAuthenticator.java:533) ~[?:?] at org.apache.kafka.common.security.authenticator.SaslServerAuthenticator.authenticate(SaslServerAuthenticator.java:281) ~[?:?] at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:181) ~[?:?] at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:548) [kafka.Kafka:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:486) [kafka.Kafka:?] at kafka.network.Processor.poll(SocketServer.scala:1017) [kafka.Kafka:?] at kafka.network.Processor.run(SocketServer.scala:921) [kafka.Kafka:?] at java.base/java.lang.Thread.runWith(Thread.java:1596) [kafka.Kafka:?] at java.base/java.lang.Thread.run(Thread.java:1583) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.thread.PlatformThreads.threadStartRoutine(PlatformThreads.java:833) [kafka.Kafka:?] at org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]{code}\r \u00a0\r \u00a0\r Reproducer bash script:\r \r \r {code:java}\r temp_dir=$(mktemp -d)\r cd ${temp_dir}\r cat << EOF > kafka_server_jaas.conf\r KafkaServer {\r \u00a0 \u00a0 org.apache.kafka.common.security.plain.PlainLoginModule required\r \u00a0 \u00a0 user_admin=\"admin-secret\";\r };\r EOF\r podman run -it --rm \\\r --name kafka-sasl-broker \\\r \u00a0 -p 9092:9092 \\\r \u00a0 -p 9093:9093 \\\r \u00a0 -v ./kafka_server_jaas.conf:/opt/kafka/config/kafka_server_jaas.conf:Z \\\r \u00a0 -e KAFKA_CLUSTER_ID=$KAFKA_CLUSTER_ID \\\r \u00a0 -e KAFKA_PROCESS_ROLES=broker,controller \\\r \u00a0 -e KAFKA_NODE_ID=1 \\\r \u00a0 -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=SASL_PLAINTEXT:SASL_PLAINTEXT,INTER_BROKER:PLAINTEXT,CONTROLLER:PLAINTEXT \\\r \u00a0 -e KAFKA_LISTENERS=SASL_PLAINTEXT://0.0.0.0:9092,INTER_BROKER://0.0.0.0:9093,CONTROLLER://0.0.0.0:9094 \\\r \u00a0 -e KAFKA_ADVERTISED_LISTENERS=SASL_PLAINTEXT://localhost:9092,INTER_BROKER://localhost:9093 \\\r \u00a0 -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r \u00a0 -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9094 \\\r \u00a0 -e KAFKA_INTER_BROKER_LISTENER_NAME=INTER_BROKER \\\r \u00a0 -e KAFKA_SASL_ENABLED_MECHANISMS=PLAIN \\\r \u00a0 -e KAFKA_OPTS=\"-Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf\" \\\r \u00a0 apache/${1}{code}\r then to connect I use the producer script to try and send messages:\r \r \r {code:java}\r kafka-console-producer.sh \\\r \u00a0 --bootstrap-server localhost:9092 \\\r \u00a0 --topic your-topic-name \\\r \u00a0 --producer-property security.protocol=SASL_PLAINTEXT \\\r \u00a0 --producer-property sasl.mechanism=PLAIN \\\r \u00a0 --producer-property 'sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"admin-secret\";'{code}\r \r For `./run-plain.sh kafka-native:4.0.0` and `./run-plain.sh kafka-native:4.1.0-rc2` the producer spins, trying repeatedly to reconnect\r \r For the main image `./run-plain.sh kafka:4.0.0` and `./run-plain.sh kafka:4.1.0-rc2` I can produce messages successfully.\r \r For context I want to use the native image for integration testing and can workaround by switching to the non-native image", "comments": [], "tasks": {"summarization": "Native docker image authentication fails with SASL PLAIN - I'm trying to use the native docker image for SASL PLAIN authentication.\r \r The server starts okay b...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19584 about?", "answer": "Native docker image authentication fails with SASL PLAIN"}}}
{"issue_id": "KAFKA-19583", "project": "KAFKA", "title": "Native docker image fails to start when using SASL OAUTHBEARER mechanism", "status": "Open", "priority": "Minor", "reporter": "Rob Young", "assignee": null, "created": "2025-08-06T23:06:15.000+0000", "updated": "2025-08-06T23:06:15.000+0000", "labels": ["native-image"], "description": "Running the native image with `KAFKA_SASL_ENABLED_MECHANISMS=OAUTHBEARER` results in an exception that prevents it starting:\r \r \r \u00a0\r {code:java}\r [2025-08-06 22:55:09,656] ERROR Exiting Kafka due to fatal exception during startup. (kafka.Kafka$) org.apache.kafka.common.KafkaException: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:184) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:188) ~[?:?] at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:105) ~[?:?] at kafka.network.Processor.<init>(SocketServer.scala:883) ~[?:?] at kafka.network.Acceptor.newProcessor(SocketServer.scala:791) ~[kafka.Kafka:?] at kafka.network.Acceptor.$anonfun$addProcessors$1(SocketServer.scala:757) ~[kafka.Kafka:?] at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:190) ~[kafka.Kafka:?] at kafka.network.Acceptor.addProcessors(SocketServer.scala:756) ~[kafka.Kafka:?] at kafka.network.DataPlaneAcceptor.configure(SocketServer.scala:472) ~[?:?] at kafka.network.SocketServer.createDataPlaneAcceptorAndProcessors(SocketServer.scala:222) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16(SocketServer.scala:149) ~[?:?] at kafka.network.SocketServer.$anonfun$new$16$adapted(SocketServer.scala:149) ~[?:?] at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619) ~[kafka.Kafka:?] at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617) ~[kafka.Kafka:?] at scala.collection.AbstractIterable.foreach(Iterable.scala:935) ~[kafka.Kafka:?] at kafka.network.SocketServer.<init>(SocketServer.scala:149) ~[?:?] at kafka.server.BrokerServer.startup(BrokerServer.scala:274) ~[?:?] at kafka.server.KafkaRaftServer.$anonfun$startup$2(KafkaRaftServer.scala:96) ~[?:?] at kafka.server.KafkaRaftServer.$anonfun$startup$2$adapted(KafkaRaftServer.scala:96) ~[?:?] at scala.Option.foreach(Option.scala:437) ~[kafka.Kafka:?] at kafka.server.KafkaRaftServer.startup(KafkaRaftServer.scala:96) ~[?:?] at kafka.Kafka$.main(Kafka.scala:97) [kafka.Kafka:?] at kafka.docker.KafkaDockerWrapper$.main(KafkaDockerWrapper.scala:68) [kafka.Kafka:?] at kafka.docker.KafkaDockerWrapper.main(KafkaDockerWrapper.scala) [kafka.Kafka:?] at java.base/java.lang.invoke.LambdaForm$DMH/sa346b79c.invokeStaticInit(LambdaForm$DMH) [kafka.Kafka:?] Caused by: org.apache.kafka.common.KafkaException: Could not find a public no-argument constructor for org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:403) ~[?:?] at org.apache.kafka.common.security.authenticator.LoginManager.<init>(LoginManager.java:65) ~[?:?] at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:123) ~[?:?] at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:170) ~[?:?] ... 24 more Caused by: java.lang.NoSuchMethodException: org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredLoginCallbackHandler.<init>() at java.base/java.lang.Class.checkMethod(DynamicHub.java:1075) ~[kafka.Kafka:?] at java.base/java.lang.Class.getConstructor0(DynamicHub.java:1238) ~[kafka.Kafka:?] at java.base/java.lang.Class.getDeclaredConstructor(DynamicHub.java:2930) ~[kafka.Kafka:?] at org.apache.kafka.common.utils.Utils.newInstance(Utils.java:401) ~[?:?] at org.apache.kafka.common.security.authenticator.LoginManager.<init>(LoginManager.java:65) ~[?:?] at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:123) ~[?:?] at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:170) ~[?:?] ... 24 more{code}\r \u00a0\r Here is a reproducer bash script:\r \r \r {code:java}\r temp_dir=$(mktemp -d)\r cd ${temp_dir}\r cat << EOF > kafka_server_jaas.conf\r KafkaServer {\r \u00a0 \u00a0 org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub=principal;\r };\r EOF\r podman run -it --rm \\\r --name kafka-sasl-broker \\\r \u00a0 -p 9092:9092 \\\r \u00a0 -p 9093:9093 \\\r \u00a0 -v ./kafka_server_jaas.conf:/opt/kafka/config/kafka_server_jaas.conf:Z \\\r \u00a0 -e KAFKA_CLUSTER_ID=$KAFKA_CLUSTER_ID \\\r \u00a0 -e KAFKA_PROCESS_ROLES=broker,controller \\\r \u00a0 -e KAFKA_NODE_ID=1 \\\r \u00a0 -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=SASL_PLAINTEXT:SASL_PLAINTEXT,INTER_BROKER:PLAINTEXT,CONTROLLER:PLAINTEXT \\\r \u00a0 -e KAFKA_LISTENERS=SASL_PLAINTEXT://0.0.0.0:9092,INTER_BROKER://0.0.0.0:9093,CONTROLLER://0.0.0.0:9094 \\\r \u00a0 -e KAFKA_ADVERTISED_LISTENERS=SASL_PLAINTEXT://localhost:9092,INTER_BROKER://localhost:9093 \\\r \u00a0 -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \\\r \u00a0 -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9094 \\\r \u00a0 -e KAFKA_INTER_BROKER_LISTENER_NAME=INTER_BROKER \\\r \u00a0 -e KAFKA_SASL_ENABLED_MECHANISMS=OAUTHBEARER \\\r \u00a0 -e KAFKA_OPTS=\"-Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf\" \\\r \u00a0 apache/${1}{code}\r \r I guess some config is needed to avoid pruning out the reflectively instantiated class.\r \r `./run-oauthbearer.sh kafka-native:4.0.0` and `./run-oauthbearer.sh kafka-native:4.1.0-rc2` fail with exception.\r \r Running the same script with the main image works:\r `./run-oauthbearer.sh kafka:4.0.0` or `./run-oauthbearer.sh kafka:4.1.0-rc2`successfully start the broker.\r \r For context we want to use the image for integration testing and can workaround by running the non-native image if we need SASL oauthbearer.", "comments": [], "tasks": {"summarization": "Native docker image fails to start when using SASL OAUTHBEARER mechanism - Running the native image with `KAFKA_SASL_ENABLED_MECHANISMS=OAUTHBEARER` results in an exception th...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19583 about?", "answer": "Native docker image fails to start when using SASL OAUTHBEARER mechanism"}}}
{"issue_id": "KAFKA-19582", "project": "KAFKA", "title": "the current assignments shown by ReassignPartitionsCommand should include the log directories", "status": "Resolved", "priority": "Major", "reporter": "Chia-Ping Tsai", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-08-06T16:10:34.000+0000", "updated": "2025-08-22T18:57:44.000+0000", "labels": [], "description": "https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L572\r https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/ReassignPartitionsCommand.java#L931\r \r Since we always pass `Map.of`, the log directories are always treated as \"any\"", "comments": [], "tasks": {"summarization": "the current assignments shown by ReassignPartitionsCommand should include the log directories - https://github.com/apache/kafka/blob/trunk/tools/src/main/java/org/apache/kafka/tools/reassign/Reass...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19582 about?", "answer": "the current assignments shown by ReassignPartitionsCommand should include the log directories"}}}
{"issue_id": "KAFKA-19581", "project": "KAFKA", "title": "Issues running Streams system tests on release candidates", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": null, "created": "2025-08-05T10:01:44.000+0000", "updated": "2025-08-05T10:01:44.000+0000", "labels": [], "description": "During release we now merge the RC tag into the release branch. This means the release branch has a real release number instead of <VERSION>-SNAPSHOT.\r \r The Streams upgrade system tests expect <VERSION>-SNAPSHOT and thus fail to run.\r \r We should either only merge RC tags once the vote pass or change the tests to use <VERSION> instead of <VERSION>-SNAPSHOT.\r \r See:\r - https://lists.apache.org/thread/v6873zlvp5bl9qf5zsr3g904nxdynr74\r - https://lists.apache.org/thread/y3rh1nnxqz6tc4brnyfbpbrx2gy655y6", "comments": [], "tasks": {"summarization": "Issues running Streams system tests on release candidates - During release we now merge the RC tag into the release branch. This means the release branch has a ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19581 about?", "answer": "Issues running Streams system tests on release candidates"}}}
{"issue_id": "KAFKA-19580", "project": "KAFKA", "title": "Upgrade spotbug to 4.9.4", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Stig Rohde D\u00f8ssing", "created": "2025-08-05T05:17:13.000+0000", "updated": "2025-08-10T15:38:23.000+0000", "labels": [], "description": "see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515", "comments": ["hello,I will fix it", "I've raised https://github.com/apache/kafka/pull/20333 to fix this.\r \r [~gongxuanzhang] Sorry, I didn't notice your comment until now, I didn't mean to steal this issue, it just kind of happened since I did the previous spotbugs-related work too. Feel free to let me know if you have a better fix than the one I linked, and I can close my PR in favor of yours."], "tasks": {"summarization": "Upgrade spotbug to 4.9.4 - see discussion https://github.com/apache/kafka/pull/20295#issuecomment-3146551515...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19580 about?", "answer": "Upgrade spotbug to 4.9.4"}}}
{"issue_id": "KAFKA-19579", "project": "KAFKA", "title": "Add missing metrics for document tiered storage", "status": "Resolved", "priority": "Minor", "reporter": "majialong", "assignee": "majialong", "created": "2025-08-04T17:46:30.000+0000", "updated": "2025-08-05T05:25:28.000+0000", "labels": [], "description": "Add missing metrics for document tiered storage\r  * \r kafka.log.remote:type=RemoteLogManager,name=RemoteLogReaderFetchRateAndTimeMs\uff1aIntroduced in [KIP-1018|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1018%3A+Introduce+max+remote+fetch+timeout+config+for+DelayedRemoteFetch+requests]\r  * \r kafka.server:type=DelayedRemoteListOffsetsMetrics,name=ExpiresPerSec,topic=([-.\\w]+),partition=([0-9]+)\uff1aIntroduced in [KIP-1075|https://cwiki.apache.org/confluence/display/KAFKA/KIP-1075%3A+Introduce+delayed+remote+list+offsets+purgatory+to+make+LIST_OFFSETS+async]", "comments": [], "tasks": {"summarization": "Add missing metrics for document tiered storage - Add missing metrics for document tiered storage\r  * \r kafka.log.remote:type=RemoteLogManager,name=Re...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19579 about?", "answer": "Add missing metrics for document tiered storage"}}}
{"issue_id": "KAFKA-19578", "project": "KAFKA", "title": "Fix flaky RemoteLogManagerTest#testCopyQuota", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Uladzislau Blok", "created": "2025-08-04T16:32:46.000+0000", "updated": "2025-09-16T19:56:48.000+0000", "labels": [], "description": "see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269", "comments": ["In case it helps, saw this one today in a PR [here|https://github.com/apache/kafka/actions/runs/17769042747/job/50500642991?pr=20542], passes locally but times out every one and then on CI it seems, on:\r {code:java}\r // Verify the copy operation completes within the timeout, since it does not need to wait for quota availability\r assertTimeoutPreemptively(Duration.ofMillis(100), () -> task.copyLogSegmentsToRemote(mockLog));{code}", "Thanks, [~lianetm]\u00a0 Didn't yet have a time to take a look on this one\r btw. if we're talking about flaky tests, could you please check this one? It was already waiting for like 3 months :)\r \r https://issues.apache.org/jira/browse/KAFKA-19299?jql=resolution%20%3D%20Unresolved%20AND%20assignee%20%3D%20currentUser()%20AND%20project%20%3D%2012311720\r \r [https://github.com/apache/kafka/pull/19927]", "Sure! Sorry that one has been sitting there, I will take a look. Thanks!"], "tasks": {"summarization": "Fix flaky RemoteLogManagerTest#testCopyQuota - see https://github.com/apache/kafka/actions/runs/16651849478/job/47166854957?pr=20269...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19578 about?", "answer": "Fix flaky RemoteLogManagerTest#testCopyQuota"}}}
{"issue_id": "KAFKA-19577", "project": "KAFKA", "title": "Allow to configure custom `ReplicaPlacer` implementation", "status": "Open", "priority": "Major", "reporter": "Chia-Ping Tsai", "assignee": "\u9ec3\u7ae3\u967d", "created": "2025-08-04T13:42:25.000+0000", "updated": "2025-08-06T12:12:30.000+0000", "labels": ["need-kip"], "description": "Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and used. KAFKA-19507 aims to enhance the default assignment policy, and in my opinion, the best approach is to make the system flexible enough to allow users to customize the policy according to their specific needs", "comments": ["it looks like https://cwiki.apache.org/confluence/display/KAFKA/KIP-660%3A+Pluggable+ReplicaPlacer, which was rejected. This was a while ago so maybe things have changed.\r \r https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories is the replacement KIP I'm currently trying to get voted.", "We could make it pluggable by exposing the internal config at the very least, if there are concerns or negative experiences with exposing such internal functionality", "That might be a bit workaround, but it is a way to allow users to customize without imposing strict compatibility constraints"], "tasks": {"summarization": "Allow to configure custom `ReplicaPlacer` implementation - Replica assignment is a complex issue, as it depends on how a kafka cluster is run, maintained, and ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19577 about?", "answer": "Allow to configure custom `ReplicaPlacer` implementation"}}}
{"issue_id": "KAFKA-19576", "project": "KAFKA", "title": "Fix typo in state-change log filename after rotate", "status": "Resolved", "priority": "Blocker", "reporter": "Chia-Ping Tsai", "assignee": "Jared Harley", "created": "2025-08-04T09:34:22.000+0000", "updated": "2025-08-05T04:53:38.000+0000", "labels": [], "description": "The log4j2 config file was incorrectly rotating the state-change.log to {{stage-change.log.[date]}} (changing the filename from state to stage). The below PR corrects the file name for rotated logs.\r \r After this change is applied, the log4j2 config will not take any actions with previously-created {{stage-change.log.[date]}} files. These may need to be manually removed by users.\r \r [https://github.com/apache/kafka/pull/20269]", "comments": ["trunk: https://github.com/apache/kafka/commit/66b3c07954c94ed2f3c7dec95128d3bc5c1b7049\r \r 4.1: https://github.com/apache/kafka/commit/fc030b411c3881e752856c443c7b14f393d4c46e\r \r 4.0: https://github.com/chia7712/kafka/commit/0f9b3127030f596c8c187e22b1badd88e4bf9662"], "tasks": {"summarization": "Fix typo in state-change log filename after rotate - The log4j2 config file was incorrectly rotating the state-change.log to {{stage-change.log.[date]}} ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19576 about?", "answer": "Fix typo in state-change log filename after rotate"}}}
{"issue_id": "KAFKA-19575", "project": "KAFKA", "title": "Revisit gradle/spotbugs-exclude.xml", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Chang Chi Hsu", "created": "2025-08-04T08:20:16.000+0000", "updated": "2025-08-04T08:25:17.000+0000", "labels": [], "description": "The PR https://github.com/apache/kafka/pull/20294  will add a number of exclusions to the SpotBugs configurations, and it would be good to revisit them later to ensure that we are not overlooking potential bugs", "comments": [], "tasks": {"summarization": "Revisit gradle/spotbugs-exclude.xml - The PR https://github.com/apache/kafka/pull/20294  will add a number of exclusions to the SpotBugs c...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19575 about?", "answer": "Revisit gradle/spotbugs-exclude.xml"}}}
{"issue_id": "KAFKA-19574", "project": "KAFKA", "title": "Improve producer and consumer config files", "status": "Resolved", "priority": "Minor", "reporter": "Federico Valeri", "assignee": "Federico Valeri", "created": "2025-08-04T08:08:58.000+0000", "updated": "2025-09-02T02:24:59.000+0000", "labels": [], "description": "Improve consumer and producer config files that are shipped with Kafka binary.\r This may include additional important config to consider or simply improved comments.", "comments": [], "tasks": {"summarization": "Improve producer and consumer config files - Improve consumer and producer config files that are shipped with Kafka binary.\r This may include add...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19574 about?", "answer": "Improve producer and consumer config files"}}}
{"issue_id": "KAFKA-19573", "project": "KAFKA", "title": "Update num.recovery.threads.per.data.dir configs", "status": "Resolved", "priority": "Minor", "reporter": "Federico Valeri", "assignee": "Federico Valeri", "created": "2025-08-04T07:20:10.000+0000", "updated": "2025-08-07T06:30:40.000+0000", "labels": [], "description": "The default value of num.recovery.threads.per.data.dir is now 2 according to KIP-1030. We should update config files which are still setting 1.", "comments": [], "tasks": {"summarization": "Update num.recovery.threads.per.data.dir configs - The default value of num.recovery.threads.per.data.dir is now 2 according to KIP-1030. We should upd...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19573 about?", "answer": "Update num.recovery.threads.per.data.dir configs"}}}
{"issue_id": "KAFKA-19572", "project": "KAFKA", "title": "Fix NPE messages in ConsoleShareConsumer.", "status": "Resolved", "priority": "Major", "reporter": "Shivsundar R", "assignee": "Shivsundar R", "created": "2025-08-01T14:24:33.000+0000", "updated": "2025-08-07T19:33:53.000+0000", "labels": [], "description": "If there is any failure in construction of KafkaShareConsumer, then it logs a few NPEs while closing.\r {code:java}\r [2025-07-31 21:45:46,484] ERROR [ShareConsumer clientId=console-share-consumer, groupId=test_1] Failed to release assignment before closing consumer (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl) java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.clients.consumer.internals.events.ApplicationEventHandler.add(org.apache.kafka.clients.consumer.internals.events.ApplicationEvent)\" because \"this.applicationEventHandler\" is null at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.sendAcknowledgementsAndLeaveGroup(ShareConsumerImpl.java:936) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.lambda$close$4(ShareConsumerImpl.java:882) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.utils.Utils.swallow(Utils.java:1042) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.close(ShareConsumerImpl.java:881) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:335) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:209) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:383) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:376) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:357) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.run(ConsoleShareConsumer.java:75) [kafka-tools-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.main(ConsoleShareConsumer.java:57) [kafka-tools-4.1.0.jar:?] [2025-07-31 21:45:46,487] ERROR [ShareConsumer clientId=console-share-consumer, groupId=test_1] Failed to stop finding coordinator (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl) java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.clients.consumer.internals.events.ApplicationEventHandler.add(org.apache.kafka.clients.consumer.internals.events.ApplicationEvent)\" because \"this.applicationEventHandler\" is null at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.stopFindCoordinatorOnClose(ShareConsumerImpl.java:915) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.utils.Utils.swallow(Utils.java:1042) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.close(ShareConsumerImpl.java:883) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:335) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:209) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:383) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:376) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:357) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.run(ConsoleShareConsumer.java:75) [kafka-tools-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.main(ConsoleShareConsumer.java:57) [kafka-tools-4.1.0.jar:?] [2025-07-31 21:45:46,488] ERROR [ShareConsumer clientId=console-share-consumer, groupId=test_1] Failed invoking acknowledgement commit callback (org.apache.kafka.clients.consumer.internals.ShareConsumerImpl) java.lang.NullPointerException: Cannot invoke \"org.apache.kafka.clients.consumer.internals.events.CompletableEventReaper.reap(long)\" because \"this.backgroundEventReaper\" is null at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.processBackgroundEvents(ShareConsumerImpl.java:1107) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.processBackgroundEvents(ShareConsumerImpl.java:1074) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.handleCompletedAcknowledgements(ShareConsumerImpl.java:1023) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.lambda$close$5(ShareConsumerImpl.java:886) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.common.utils.Utils.swallow(Utils.java:1042) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.close(ShareConsumerImpl.java:885) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:335) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:209) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:383) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:376) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:357) [kafka-clients-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.run(ConsoleShareConsumer.java:75) [kafka-tools-4.1.0.jar:?] at org.apache.kafka.tools.consumer.ConsoleShareConsumer.main(ConsoleShareConsumer.java:57) [kafka-tools-4.1.0.jar:?] [2025-07-31 21:45:46,490] ERROR Unknown error when running consumer: (org.apache.kafka.tools.consumer.ConsoleShareConsumer) org.apache.kafka.common.KafkaException: Failed to construct Kafka share consumer at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:338) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerImpl.<init>(ShareConsumerImpl.java:209) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.internals.ShareConsumerDelegateCreator.create(ShareConsumerDelegateCreator.java:49) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:383) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:376) ~[kafka-clients-4.1.0.jar:?] at org.apache.kafka.clients.consumer.KafkaShareConsumer.<init>(KafkaShareConsumer.java:357) ~[kafka-clients-4.1.0.jar:?]{code}\r This is because we are trying to close some resources which were not initialized in the constructor.\r \r We need to ensure we check if the resources are null before closing them.", "comments": [], "tasks": {"summarization": "Fix NPE messages in ConsoleShareConsumer. - If there is any failure in construction of KafkaShareConsumer, then it logs a few NPEs while closing...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19572 about?", "answer": "Fix NPE messages in ConsoleShareConsumer."}}}
{"issue_id": "KAFKA-19571", "project": "KAFKA", "title": "Race condition between log segment flush and file deletion causing log dir to go offline", "status": "Open", "priority": "Major", "reporter": "Ilyas Toumlilt", "assignee": null, "created": "2025-08-01T14:04:29.000+0000", "updated": "2025-08-15T19:12:25.000+0000", "labels": [], "description": "h1. Context\r \r We are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple disks in a JBOD setup, routine intra-broker data rebalancing is performed using Cruise Control to manage disk utilization. During these rebalance operations, a race condition between a log segment flush operation and the file deletion that is part of the replica's directory move. This race leads to a `NoSuchFileException` when the flush operation targets a file path that has just been deleted by the rebalance process. This exception incorrectly forces the broker to take the entire log directory offline.\r h1. Logs / Stack trace\r {code:java}\r 2025-07-23 19:03:30,114 WARN Failed to flush file /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot (org.apache.kafka.\r common.utils.Utils)\r java.nio.file.NoSuchFileException: /var/lib/kafka-08/topic_01-12/00000000024420850595.snapshot\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:182)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.nio.channels.FileChannel.open(FileChannel.java:292)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.nio.channels.FileChannel.open(FileChannel.java:345)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.common.utils.Utils.flushFileIfExists(Utils.java:1029)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$flushProducerStateSnapshot$2(UnifiedLog.scala:1766)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.flushProducerStateSnapshot(UnifiedLog.scala:1915)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$roll$2(UnifiedLog.scala:1679)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.Optional.ifPresent(Optional.java:183)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$roll$1(UnifiedLog.scala:1679)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.server.util.KafkaScheduler.lambda$schedule$1(KafkaScheduler.java:150)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:829)\r \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r 2025-07-23 19:03:30,114 ERROR Error while flushing log for topic_01-12 in dir /var/lib/kafka-08 with offset 24420850595 (exclusi\r ve) and recovery point 24420850595 (org.apache.kafka.storage.internals.log.LogDirFailureChannel)\r java.nio.channels.ClosedChannelException\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:150)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:452)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.common.record.FileRecords.flush(FileRecords.java:197)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment$2.call(LogSegment.java:631)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment$2.call(LogSegment.java:627)\r \u00a0 \u00a0 \u00a0 \u00a0 at com.yammer.metrics.core.Timer.time(Timer.java:91)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment.flush(LogSegment.java:627)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.LocalLog.$anonfun$flush$1(LocalLog.scala:176)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Iterable.forEach(Iterable.java:75)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.LocalLog.flush(LocalLog.scala:176)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$flush$2(UnifiedLog.scala:1719)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.flush(UnifiedLog.scala:1915)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.flushUptoOffsetExclusive(UnifiedLog.scala:1700)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$roll$1(UnifiedLog.scala:1680)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.server.util.KafkaScheduler.lambda$schedule$1(KafkaScheduler.java:150)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:829)\r \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r 2025-07-23 19:03:30,115 ERROR Uncaught exception in scheduled task 'flush-log' (org.apache.kafka.server.util.KafkaScheduler)\r org.apache.kafka.common.errors.KafkaStorageException: Error while flushing log for topic_01-12 in dir /var/lib/kafka-08 with off\r set 24420850595 (exclusive) and recovery point 24420850595\r Caused by: java.nio.channels.ClosedChannelException\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:150)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:452)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.common.record.FileRecords.flush(FileRecords.java:197)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment$2.call(LogSegment.java:631)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment$2.call(LogSegment.java:627)\r \u00a0 \u00a0 \u00a0 \u00a0 at com.yammer.metrics.core.Timer.time(Timer.java:91)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.storage.internals.log.LogSegment.flush(LogSegment.java:627)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.LocalLog.$anonfun$flush$1(LocalLog.scala:176)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Iterable.forEach(Iterable.java:75)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.LocalLog.flush(LocalLog.scala:176)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$flush$2(UnifiedLog.scala:1719)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.flush(UnifiedLog.scala:1915)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.flushUptoOffsetExclusive(UnifiedLog.scala:1700)\r \u00a0 \u00a0 \u00a0 \u00a0 at kafka.log.UnifiedLog.$anonfun$roll$1(UnifiedLog.scala:1680)\r \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.server.util.KafkaScheduler.lambda$schedule$1(KafkaScheduler.java:150)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r \u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:829)\r \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r 2025-07-23 19:03:30,117 WARN [ReplicaManager broker=32] Stopping serving replicas in dir /var/lib/kafka-08 (kafka.server.ReplicaManager) {code}\r Stack Trace Analysis\r \r The failure begins with a benign `{{{}WARN`{}}} when a scheduled task tries to flush a producer state snapshot that was moved during a disk rebalance; this {{`NoSuchFileException`}} is anticipated and handled gracefully by the code. As implemented in https://issues.apache.org/jira/browse/KAFKA-13403 to swallow the exception. \r However, the same task then attempts to flush the actual log segment, which fails with a critical, unhandled\u00a0 `{{{}ClosedChannelException{}}}` because the file handles were invalidated by the directory's move. This unhandled I/O error propagates up and terminates the background task, causing the `{{{}KafkaScheduler{}}}` to log it as an uncaught {{{}`{}}}{{{}KafkaStorageException`{}}}. As a direct consequence, the `{{{}ReplicaManager{}}}` detects this fatal storage error and triggers its safety mechanism, taking the entire log directory offline to prevent potential data corruption.\r h1. Expected Behavior\r \r A {{`NoSuchFileException`}} in this context should not cause the entire log directory to be marked as offline.\r h1. Workaround\r \r The current workaround is to manually restart the affected Kafka broker. The restart clears the in-memory state, and upon re-scanning the log directories, the broker marks the disk as healthy again.\r h1. Proposed fix\r \r The proposed solution is to address the race condition at the lowest possible level: the *{{LogSegment.flush()}}* method. The goal is to catch the {{ClosedChannelException}} that occurs during the race and intelligently differentiate it from a legitimate I/O error.\r \r The fix should be implemented within the {{catch}} block for {{ClosedChannelException}} in {{{}LogSegment.java{}}}. The logic would be as follows:\r  # When a {{ClosedChannelException}} is caught, perform a check to see if the underlying log segment file still exists ({{{}log.file().exists(){}}}).\r \r  # {*}If the file does not exist{*}, it confirms our race condition hypothesis: the replica has been moved or deleted by a rebalance operation. The exception is benign and should be ignored, with a {{WARN}} message logged for visibility.\r \r  # {*}If the file does still exist{*}, the {{ClosedChannelException}} is unexpected and could signal a real hardware or filesystem issue. In this case, the exception should be re-thrown to trigger Kafka's standard log directory failure-handling mechanism.\r \r This targeted fix would resolve the bug by gracefully handling the known race condition without masking other potentially critical storage errors.\r h2. Related issues\r  * https://issues.apache.org/jira/browse/KAFKA-13403 was fixed to swallow the first `{{{}NoSuchFileException{}}}` WARN in the above stacktrace, but not the underlying exception.\r  * https://issues.apache.org/jira/browse/KAFKA-15391 is similar but different, it swallows `NoSuchFileException` for race condition on log directory move/delete, but not on the segment file level.", "comments": ["(https://issues.apache.org/jira/browse/KAFKA-13403) was fixed to swallow the first ``NoSuchFileException`` WARN in the above stacktrace, but not the underlying exception.", "(https://issues.apache.org/jira/browse/KAFKA-15391) is similar but different, it swallows `NoSuchFileException` for race condition on log directory move/delete, but not on the segment file level."], "tasks": {"summarization": "Race condition between log segment flush and file deletion causing log dir to go offline - h1. Context\r \r We are using Kafka v3.7.1 with Zookeeper, our brokers are configured with multiple di...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19571 about?", "answer": "Race condition between log segment flush and file deletion causing log dir to go offline"}}}
{"issue_id": "KAFKA-19570", "project": "KAFKA", "title": "Implement offline migration", "status": "Resolved", "priority": "Major", "reporter": "Lucas Brutschy", "assignee": "Lucas Brutschy", "created": "2025-08-01T13:11:14.000+0000", "updated": "2025-08-26T14:19:30.000+0000", "labels": ["streams"], "description": "Offline migration essentially preserves offsets and nothing else. So effectively write tombstones for classic group type when a streams heartbeat is sent to with the group ID of an empty classic group, and write tombstones for the streams group type when a classic consumer attempts to join with a group ID of an empty streams group.\r \r AC:\r  * Implementation on the broker-side", "comments": [], "tasks": {"summarization": "Implement offline migration - Offline migration essentially preserves offsets and nothing else. So effectively write tombstones fo...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19570 about?", "answer": "Implement offline migration"}}}
{"issue_id": "KAFKA-19569", "project": "KAFKA", "title": "Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator", "status": "Resolved", "priority": "Minor", "reporter": "dyingjiecai", "assignee": "dyingjiecai", "created": "2025-08-01T02:13:17.000+0000", "updated": "2025-08-01T02:38:48.000+0000", "labels": [], "description": "There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation within the SaslServerAuthenticator class.\r Location:\r  !image-2025-08-01-10-12-04-784.png! \r The calculation sessionExpirationTimeNanos = authenticationEndNanos + 1000 * 1000 * retvalSessionLifetimeMs can potentially overflow when:\r retvalSessionLifetimeMs is very large \r authenticationEndNanos is already a large value\r The multiplication 1000 * 1000 * retvalSessionLifetimeMs exceeds Long.MAX_VALUE - authenticationEndNanos", "comments": ["Duplicated with KAFKA-14604", "OK i will close this one"], "tasks": {"summarization": "Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator  - There is a potential risk of Long.MAX_VALUE overflow in the sessionExpirationTimeNanos calculation w...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19569 about?", "answer": "Potential Long.MAX_VALUE overflow in sessionExpirationTimeNanos calculation in SaslServerAuthenticator"}}}
{"issue_id": "KAFKA-19568", "project": "KAFKA", "title": "Add disk usage threshold to prevent disk full failure", "status": "Open", "priority": "Major", "reporter": "mapan", "assignee": null, "created": "2025-07-31T11:55:35.000+0000", "updated": "2025-07-31T11:58:42.000+0000", "labels": [], "description": "Currently in our production environment, the most common problem with Kafka is disk full failure. When the failure occurs, Kafka cannot provide any services, and can only be recovered by manually deleting disk data or expanding the disk capacity and then restarting the service.\r \r In contrast, RocketMQ and RabbitMQ already support disk threshold configuration (such as {{diskMaxUsedSpaceRatio}} , {{disk_free_limit}} ). When the disk usage exceeds the limit, the system rejects new produce, but the service remains available.\r \r So we hope to implement a similar strategy in Kafka to prevent the disk full failure.\r \r KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1201%3A+Add+disk+threshold+config+to+prevent+disk+full+failure", "comments": [], "tasks": {"summarization": "Add disk usage threshold to prevent disk full failure - Currently in our production environment, the most common problem with Kafka is disk full failure. Wh...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19568 about?", "answer": "Add disk usage threshold to prevent disk full failure"}}}
{"issue_id": "KAFKA-19567", "project": "KAFKA", "title": "Purgatory should clear up after share consumers are stopped", "status": "Resolved", "priority": "Major", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "created": "2025-07-31T06:32:01.000+0000", "updated": "2025-08-10T09:15:42.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Purgatory should clear up after share consumers are stopped - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19567 about?", "answer": "Purgatory should clear up after share consumers are stopped"}}}
{"issue_id": "KAFKA-19566", "project": "KAFKA", "title": "Deprecate ClientQuotaCallback#updateClusterMetadata", "status": "Open", "priority": "Major", "reporter": "Kuan Po Tseng", "assignee": "Kuan Po Tseng", "created": "2025-07-30T16:12:02.000+0000", "updated": "2025-07-30T16:12:26.000+0000", "labels": [], "description": "In KAFKA-18225, we discovered that {{ClientQuotaCallback#updateClusterMetadata}} is not supported in KRaft mode, unlike in Zookeeper mode. Kafka 4.0 addressed this gap by implementing the method, but limitations remain:\r  * A new {{Cluster}} object (immutable and heavyweight) is passed on every metadata update, which may cause memory pressure in large clusters (see KAFKA-18239).\r \r  * Some {{Cluster}} fields are confusing or irrelevant in KRaft, such as {{controller()}} returning a random node for compatibility. Also, listener parsing differs between modes, potentially causing inconsistent partition info (see KAFKA-19122).\r \r To resolve these issues, *[KIP-1162|https://cwiki.apache.org/confluence/x/zInoF]* proposes a redesign of the method. However, given that this method remained unimplemented for years without user complaints, we believe it's not worth the complexity to fix it. Instead, we propose deprecating {{updateClusterMetadata}} now and removing it in Kafka 5.0.", "comments": [], "tasks": {"summarization": "Deprecate ClientQuotaCallback#updateClusterMetadata - In KAFKA-18225, we discovered that {{ClientQuotaCallback#updateClusterMetadata}} is not supported in...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19566 about?", "answer": "Deprecate ClientQuotaCallback#updateClusterMetadata"}}}
{"issue_id": "KAFKA-19565", "project": "KAFKA", "title": "Integration test for Streams-related Admin APIs[2/N]", "status": "Resolved", "priority": "Major", "reporter": "Lucy Liu", "assignee": "Lucy Liu", "created": "2025-07-30T15:08:30.000+0000", "updated": "2025-10-17T20:05:25.000+0000", "labels": [], "description": "Integration tests for Stream Admin related API\r \r Previous one: https://issues.apache.org/jira/browse/KAFKA-19550\r \r This one adds:\r  * Integration test for\u00a0{{Admin#listStreamsGroupOffsets}}\u00a0API\r  * Integration test fo\u00a0{{Admin#deleteStreamsGroupOffsets}}\u00a0API\r  * Integration test fo\u00a0{{Admin#alterStreamsGroupOffsets}}\u00a0API", "comments": [], "tasks": {"summarization": "Integration test for Streams-related Admin APIs[2/N] - Integration tests for Stream Admin related API\r \r Previous one: https://issues.apache.org/jira/brows...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19565 about?", "answer": "Integration test for Streams-related Admin APIs[2/N]"}}}
{"issue_id": "KAFKA-19564", "project": "KAFKA", "title": "Close Consumer in ConsumerPerformance only after metrics displayed", "status": "Resolved", "priority": "Minor", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-07-30T15:07:35.000+0000", "updated": "2025-09-03T08:56:49.000+0000", "labels": [], "description": "In {{ConsumerPerformance}} (used by {{kafka-consumer-perf-test.sh}}), the metrics are shown, but only after the {{Consumer}} has been closed. Because metrics are removed from the {{Metrics}} object on {{Consumer.close()}}, this means that the complete set of metrics is not displayed when the performance tool outputs the metrics.", "comments": ["Hi [~kirktrue], I think I could help with this issue if you haven't start to handle it."], "tasks": {"summarization": "Close Consumer in ConsumerPerformance only after metrics displayed - In {{ConsumerPerformance}} (used by {{kafka-consumer-perf-test.sh}}), the metrics are shown, but onl...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19564 about?", "answer": "Close Consumer in ConsumerPerformance only after metrics displayed"}}}
{"issue_id": "KAFKA-19563", "project": "KAFKA", "title": "Separate the controller config and admin config when add controller", "status": "Resolved", "priority": "Major", "reporter": "Luke Chen", "assignee": "Lan Ding", "created": "2025-07-30T08:22:49.000+0000", "updated": "2025-07-31T06:36:30.000+0000", "labels": ["need-kip"], "description": "Currently, when adding a controller via CLI, we have to run:\r {code:java}\r $ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-server localhost:9092 add-controller\r \r or\r \r $ bin/kafka-metadata-quorum.sh --command-config config/controller.properties --bootstrap-controller localhost:9093 add-controller{code}\r The controller.properties file is expected to be the controller property file that to be added. But if we want to pass configs to the admin client, what can we do?\r {code:java}\r bin/kafka-metadata-quorum.sh --help\r usage: kafka-metadata-quorum [-h] [--command-config COMMAND_CONFIG] (--bootstrap-server BOOTSTRAP_SERVER | --bootstrap-controller BOOTSTRAP_CONTROLLER)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0{describe,add-controller,remove-controller} ...This tool describes kraft metadata quorum status.\r \r ...\r \r  --command-config COMMAND_CONFIG\r \u00a0 \u00a0Property file containing configs to be passed to Admin Client. \u00a0For add-controller, \u00a0the file is used to specify the controller properties as well.{code}\r As this help output said, the \"--command-config\" can pass configs to admin client, but when add controller, it is also used as controller property file.\r \r For example, we want to set the \"client-id\" to the admin client, when doing the add-controller, we have to add one more line in the controller.properties file:\r {code:java}\r client.id=test-admin-client{code}\r This is not ideal to ask users to mix the client config into the controller config.\r \r \u00a0\r \r Maybe we can consider to add one more \"--add-controller-command-config\" for add controller use, or ask users to explicitly pass the controller id, dir UUID, ... like remove-controller did, but the controller advertised listener value might be a little complicated for users.", "comments": ["Improve the doc first: https://github.com/apache/kafka/pull/20261", "Hi [~showuon], if you are not working on this, may i take it. Thanks.", "Go take it [~isding_l] ! It needs a KIP, for your information.", "I filed KAFKA-18775 to simplify the command line, which can add controller or remove controller \"without\" controller.properties. That is a bit conflicted to this jira, so it would be useful if you could take a look at KAFKA-18775"], "tasks": {"summarization": "Separate the controller config and admin config when add controller - Currently, when adding a controller via CLI, we have to run:\r {code:java}\r $ bin/kafka-metadata-quor...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19563 about?", "answer": "Separate the controller config and admin config when add controller"}}}
{"issue_id": "KAFKA-19562", "project": "KAFKA", "title": "Rewrite AbortedTxn by generated protocol", "status": "Open", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Ming-Yen Chung", "created": "2025-07-30T06:37:14.000+0000", "updated": "2025-07-30T06:38:19.000+0000", "labels": [], "description": "as title", "comments": [], "tasks": {"summarization": "Rewrite AbortedTxn by generated protocol - as title...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19562 about?", "answer": "Rewrite AbortedTxn by generated protocol"}}}
{"issue_id": "KAFKA-19561", "project": "KAFKA", "title": "Request Timeout During SASL Reauthentication Due to Missed OP_WRITE  interest set", "status": "Open", "priority": "Major", "reporter": "Manikumar", "assignee": "Manikumar", "created": "2025-07-29T19:01:14.000+0000", "updated": "2025-08-15T09:45:43.000+0000", "labels": [], "description": "We've observed request timeouts occurring during SASL reauthentication, and analysis suggests the issue is caused by a race condition between request handling and SASL reauthentication on the broker side. Here\u2019s the sequence:\r  # Client sends a request (Req1) to the broker.\r  # Client initiates SASL reauthentication.\r  # Broker receives Req1.\r  # Broker also begins SASL reauthentication.\r  # While reauth is in progress:\r  ** Broker completes processing of Req1 and prepares a response (Res1).\r  ** Res1 is queued via KafkaChannel.send().\r  ** Broker sets SelectionKey.OP_WRITE to indicate write readiness.\r  ** However, Selector.attemptWrite() does not proceed because:\r  *** channel.hasSend() is true, but\r  *** channel.ready() is false (reauth is still in progress).\r  # Once reauthentication completes: Broker removes SelectionKey.OP_WRITE.\r  # At this point:\r  ** channel.hasSend() and channel.ready() are now true,\r  ** But key.isWritable() is false, so the response (Res1) is never sent.\r  # The response remains stuck in the send buffer. Client eventually hits a request timeout.\r \r The fix is to set write readiness using SelectionKey.OP_WRITE at the end of Step 6. This is similar to [what we do on client side|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java#L422].", "comments": [], "tasks": {"summarization": "Request Timeout During SASL Reauthentication Due to Missed OP_WRITE  interest set  - We've observed request timeouts occurring during SASL reauthentication, and analysis suggests the is...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19561 about?", "answer": "Request Timeout During SASL Reauthentication Due to Missed OP_WRITE  interest set"}}}
{"issue_id": "KAFKA-19560", "project": "KAFKA", "title": "leader of the __consumer_offsets topic is not balanced", "status": "Open", "priority": "Major", "reporter": "mooner", "assignee": null, "created": "2025-07-29T09:19:56.000+0000", "updated": "2025-07-29T09:53:32.000+0000", "labels": [], "description": "h3. {color:#172b4d}I have already set [auto.leader.rebalance.enable|https://kafka.apache.org/documentation/#brokerconfigs_auto.leader.rebalance.enable] to true, but the leader of the __consumer_offsets topic is not balanced at all, remaining on only one node or two nodes.{color}\r h3. {color:#172b4d}Only this internal topic is unbalanced, resulting in an uneven number of connections and uneven CPU pressure.{color}\r h3. {color:#172b4d}Except for this topic{color}{color:#172b4d}__consumer_offsets{color}{color:#172b4d}, other topics are normal, the leader is balanced to three nodes.{color}\r \r \u00a0\r \r *3.5.0 version,The leader is only on nodes 1 and 0 and will not be balanced to node 2. :*\r Topic: __consumer_offsets TopicId: 480x8Ow1Qwyb-YvuWWqwVQ PartitionCount: 50 ReplicationFactor: 3 Configs: compression.type=producer,deletion.filterDeadGroup.enable=true,min.insync.replicas=1,cleanup.policy=compact,segment.bytes=104857600,max.message.bytes=15728658,index.interval.bytes=4096,unclean.leader.election.enable=false,retention.bytes=1073741824\r Topic: __consumer_offsetsPartition: 0Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 1Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 2Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 3Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 4Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 5Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 6Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 7Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 8Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 9Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 10Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 11Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 12Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 13Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 14Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 15Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 16Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 17Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 18Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 19Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 20Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 21Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 22Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 23Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 24Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 25Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 26Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 27Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 28Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 29Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 30Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 31Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 32Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 33Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 34Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 35Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 36Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 37Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 38Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 39Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 40Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 41Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 42Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 43Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 44Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 45Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 46Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 47Leader: 0Replicas: 0,1,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 48Leader: 1Replicas: 1,0,2Isr: 0,2,1\r Topic: __consumer_offsetsPartition: 49Leader: 0Replicas: 0,1,2Isr: 0,2,1", "comments": [], "tasks": {"summarization": "leader of the __consumer_offsets topic is not balanced - h3. {color:#172b4d}I have already set [auto.leader.rebalance.enable|https://kafka.apache.org/documen...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19560 about?", "answer": "leader of the __consumer_offsets topic is not balanced"}}}
{"issue_id": "KAFKA-19559", "project": "KAFKA", "title": "ShareConsumer.close() does not remove all sensors.", "status": "Resolved", "priority": "Major", "reporter": "Shivsundar R", "assignee": "Shivsundar R", "created": "2025-07-29T07:20:10.000+0000", "updated": "2025-07-29T10:12:55.000+0000", "labels": [], "description": "This is a follow up from KAFKA-19542(https://issues.apache.org/jira/browse/KAFKA-19542) where some sensors in the share-consumer are not closed when the consumer closes.", "comments": [], "tasks": {"summarization": "ShareConsumer.close() does not remove all sensors. - This is a follow up from KAFKA-19542(https://issues.apache.org/jira/browse/KAFKA-19542) where some s...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19559 about?", "answer": "ShareConsumer.close() does not remove all sensors."}}}
{"issue_id": "KAFKA-19558", "project": "KAFKA", "title": "kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups", "status": "Open", "priority": "Blocker", "reporter": "zhangtongr", "assignee": "Ranuga Disansa", "created": "2025-07-29T02:34:34.000+0000", "updated": "2025-08-11T03:16:00.000+0000", "labels": [], "description": "Description:\r When running the following command in a Kafka cluster with a large number of consumer groups (over 380) and topics (over 500), the kafka-consumer-groups.sh --describe --all-groups operation consistently times out and fails to return results.\r \r Command used:\r \r ./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups\r Observed behavior:\r The command fails with a TimeoutException, and no consumer group information is returned. The following stack trace is observed:\r \r java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=1753170317381, tries=1, nextAllowedTryMs=1753170317482) timed out at 1753170317382 after 1 attempt(s)\r     at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45)\r     at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32)\r     ...\r Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=describeConsumerGroups, deadlineMs=..., tries=1, ...) timed out\r Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: describeConsumerGroups\r Expected behavior:\r The command should be able to return the description of all consumer groups, or at least fail more gracefully. Ideally, there should be:\r \r A way to paginate or batch the describe operation;\r \r Or configuration options to increase internal timeout thresholds;\r \r Or better recommendations for dealing with large clusters.\r \r Additional context:\r \r Manually describing individual consumer groups via --group performs as expected and returns data quickly.\r \r The issue appears to scale linearly with the number of consumer groups.", "comments": ["hi, i am looking into this, i will give a update soon!", "Thank you for looking into this."], "tasks": {"summarization": "kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups - Description:\r When running the following command in a Kafka cluster with a large number of consumer ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19558 about?", "answer": "kafka-consumer-groups.sh --describe --all-groups command times out on large clusters with many consumer groups"}}}
{"issue_id": "KAFKA-19557", "project": "KAFKA", "title": "Remove BrokerNotFoundException", "status": "Open", "priority": "Blocker", "reporter": "Matthias J. Sax", "assignee": null, "created": "2025-07-28T22:19:51.000+0000", "updated": "2025-07-28T22:22:32.000+0000", "labels": [], "description": "h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.*\r \r BrokerNotFoundException was deprecated for removal, as it's not used any longer.\r \r Cf https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=373886192", "comments": [], "tasks": {"summarization": "Remove BrokerNotFoundException - h1. *THIS TICKET CANNOT BE WORKED ON, UNTIL WE DO APACHE KAFKA 5.0 RELEASE.*\r \r BrokerNotFoundExcept...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19557 about?", "answer": "Remove BrokerNotFoundException"}}}
{"issue_id": "KAFKA-19556", "project": "KAFKA", "title": "[Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API", "status": "Open", "priority": "Major", "reporter": "Tamas Kornai", "assignee": null, "created": "2025-07-28T15:11:55.000+0000", "updated": "2025-08-15T15:26:53.000+0000", "labels": [], "description": "h3. Summary\r \r The upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for the Connect REST API. This change breaks HTTPS request forwarding between Connect workers when they connect via IP address, causing requests to fail with a 400: Invalid SNI error.\r h3. The Problem\r \r Prior to Kafka 4.0, the Jetty server used for the Connect REST API did not enforce a strict match between the TLS SNI hostname and the HTTP Host header.\r \r With the upgrade to Jetty 12, this check is now enabled by default at the HTTP level. This causes legitimate HTTPS requests to fail in environments where the client connects using an IP address or a hostname that is not listed in the server's TLS certificate.\r \r This results in the following error:\r {code:java}\r org.eclipse.jetty.http.BadMessageException: 400: Invalid SNI\r {code}\r h3. Impacted Use Case: Inter-Node Request Forwarding\r \r This change specifically breaks the request forwarding mechanism between Connect workers in a common deployment scenario:\r  # A follower Connect instance needs to forward a REST request to the leader.\r  # The follower connects directly to the leader's IP address over HTTPS.\r  # Security is handled by mTLS certificates, often managed by a custom certificate provider.\r \r This setup worked flawlessly before Kafka 4.0. Now, because the follower connects via IP, the SNI check fails, and the forwarding mechanism is broken.\r h3. Proposed Solution\r \r This behavior cannot be disabled through any existing Kafka Connect configuration. To restore the previous functionality, a SecureRequestCustomizer must be programmatically configured in RestServer.java to disable the SNI required and the SNI host check flags. This should be driven by a configuration value that allows disabling these SNI related settings.\r {code:java}\r // In RestServer.java, when building the HTTPS connector\r SecureRequestCustomizer customizer = new SecureRequestCustomizer();\r customizer.setSniRequired(false);\r customizer.setSniHostCheck(false);\r HttpConfiguration https = new HttpConfiguration();\r https.addCustomizer(customizer);\r connector = new ServerConnector(jettyServer, ssl, new HttpConnectionFactory(https)); {code}", "comments": ["if you disable sni how then can you locate different host names in the same web server ?", "We are routing by IP.", "Yes exactly so if there are multiple host names on the web server , without an sni it is possible you can get an error if the web server does not know what ssl to handshake you with. so there are some draw backs disabling this I think .", "I'm not advocating to turning it off for everyone, I just hope to get a configuration setting exposed so it can be turned off for installations where it is causing issues.", "[~tkornai] Thanks for the ticket!\r \r I believe this would require a new configuration, and so is subject to our KIP process: [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals]\u00a0\r \r This is a large time investment overall, and would only be possible to fix in a new minor version of Connect. To avoid falling behind new versions, I would recommend finding another workaround that does not involve code changes.\r \r Personally, I'm not very familiar with SNI host checking, but I assume that Jetty has enabled it by default as part of encouraging security \"best practices.\" Perhaps it is best to conform to the new default, rather than invest effort in returning to the old behavior. We can also see from other users how important this functionality is."], "tasks": {"summarization": "[Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API - h3. Summary\r \r The upgrade to Jetty 12 in Kafka 4.0 enables a strict SNI host check by default for t...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19556 about?", "answer": "[Connect] Provide a configuration to disable SNI required and SNI host checking for the REST API"}}}
{"issue_id": "KAFKA-19555", "project": "KAFKA", "title": "Restrict records acquisition post max inflight records", "status": "Resolved", "priority": "Major", "reporter": "Apoorv Mittal", "assignee": "Apoorv Mittal", "created": "2025-07-28T12:03:55.000+0000", "updated": "2025-07-29T10:12:02.000+0000", "labels": [], "description": "Currently, the check for max inflight records happen prior fetching data for some share partition but the fetched data can be acquired and have records which are past max inflight records. This is evident when some records are released from the inflight records and the next fetch results in data past max inflight records.", "comments": [], "tasks": {"summarization": "Restrict records acquisition post max inflight records - Currently, the check for max inflight records happen prior fetching data for some share partition bu...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19555 about?", "answer": "Restrict records acquisition post max inflight records"}}}
{"issue_id": "KAFKA-19554", "project": "KAFKA", "title": "Add a Kafka client parameter to limit number of messages fetched", "status": "Open", "priority": "Major", "reporter": "corkitse", "assignee": null, "created": "2025-07-28T09:48:56.000+0000", "updated": "2025-07-29T15:18:25.000+0000", "labels": ["kip-required"], "description": "h3. Description\r \r Currently, Kafka fetch requests only support limiting the total size of messages fetched ({{{}fetch.max.bytes{}}}) and the size per partition ({{{}max.partition.fetch.bytes{}}}). However, there is no way to limit the *number of messages* fetched per request\u2014neither globally nor on a per-partition basis.\r \r While Kafka was originally designed as a high-throughput distributed messaging platform and has traditionally focused more on throughput than individual message control, its role has since evolved. Kafka is now not only a leading message queue but also a core component in modern {*}data pipelines and stream processing frameworks{*}.\r \r In these newer use cases, especially for downstream services and streaming applications, *rate-limiting by message count* is a common requirement.\r \r Currently, the workaround is for clients to {*}fetch a batch of messages, manually truncate them based on count, and then adjust offsets manually{*}, which is inefficient, error-prone, and significantly reduces throughput. In practice, this forces developers to use external tools such as Redis to implement additional buffering or rate control mechanisms\u2014adding complexity and overhead.\r \r Adding *native support* for a message count limit in fetch requests would offer the following benefits:\r h3. Benefits\r  # {*}Make Kafka a more mature and production-ready stream processing platform{*}, by supporting more granular rate-limiting use cases.\r \r  # *Improve overall system throughput* for consumers that need to limit by message count, by eliminating inefficient post-processing workarounds.\r \r ----\r h3. Potential Challenges\r  # Due to compression and batching, Kafka consumers do not always have direct access to message counts in a fetch response. This means any solution would need to {*}estimate or calculate message counts indirectly{*}\u2014possibly based on batch metadata.\r \r  # Implementation must ensure that {*}Kafka\u2019s high-performance characteristics are preserved{*}. Any support for message-count limits must avoid excessive decompression or deserialization on the broker side.\r \r Moreover, from what I\u2019ve observed, {*}many capable companies have already implemented their own internal forks or wrappers of Kafka to support this feature{*}, highlighting the demand and practical importance of this functionality. Therefore, it would be highly beneficial for Kafka to provide a {*}unified and officially supported solution{*}.\r \r \u00a0\r The parameter reference from our internal version.\r \u00a0\r \u00a0\r \u00a0\r {code:java}\r ...\r \"validVersions\": \"4-18\",\r \"flexibleVersions\": \"12+\",\r \"fields\": [\r   { \"name\": \"ClusterId\", \"type\": \"string\", \"versions\": \"12+\", \"nullableVersions\": \"12+\", \"default\": \"null\",\r     \"taggedVersions\": \"12+\", \"tag\": 0, \"ignorable\": true,\r     \"about\": \"The clusterId if known. This is used to validate metadata fetches prior to broker registration.\" },\r   { \"name\": \"ReplicaId\", \"type\": \"int32\", \"versions\": \"0-14\", \"default\": \"-1\", \"entityType\": \"brokerId\",\r     \"about\": \"The broker ID of the follower, of -1 if this request is from a consumer.\" },\r   { \"name\": \"ReplicaState\", \"type\": \"ReplicaState\", \"versions\": \"15+\", \"taggedVersions\": \"15+\", \"tag\": 1,\r     \"about\": \"The state of the replica in the follower.\", \"fields\": [\r     { \"name\": \"ReplicaId\", \"type\": \"int32\", \"versions\": \"15+\", \"default\": \"-1\", \"entityType\": \"brokerId\",\r       \"about\": \"The replica ID of the follower, or -1 if this request is from a consumer.\" },\r     { \"name\": \"ReplicaEpoch\", \"type\": \"int64\", \"versions\": \"15+\", \"default\": \"-1\",\r       \"about\": \"The epoch of this follower, or -1 if not available.\" }\r   ]},\r   { \"name\": \"MaxWaitMs\", \"type\": \"int32\", \"versions\": \"0+\",\r     \"about\": \"The maximum time in milliseconds to wait for the response.\" },\r   { \"name\": \"MinBytes\", \"type\": \"int32\", \"versions\": \"0+\",\r     \"about\": \"The minimum bytes to accumulate in the response.\" },\r   { \"name\": \"MaxBytes\", \"type\": \"int32\", \"versions\": \"3+\", \"default\": \"0x7fffffff\", \"ignorable\": true,\r     \"about\": \"The maximum bytes to fetch.  See KIP-74 for cases where this limit may not be honored.\" },\r   { \"name\": \"MaxNum\", \"type\": \"int32\", \"versions\": \"18+\", \"default\": \"-1\", \"ignorable\": true,\r     \"about\": \"The maximum number of messages to fetch. -1 means no limit.\" },\r   { \"name\": \"IsolationLevel\", \"type\": \"int8\", \"versions\": \"4+\", \"default\": \"0\", \"ignorable\": true,\r     \"about\": \"This setting controls the visibility of transactional records. Using READ_UNCOMMITTED (isolation_level = 0) makes all records visible. With READ_COMMITTED (isolation_level = 1), non-transactional and COMMITTED transactional records are visible. To be more concrete, READ_COMMITTED returns all data from offsets smaller than the current LSO (last stable offset), and enables the inclusion of the list of aborted transactions in the result, which allows consumers to discard ABORTED transactional records.\" },\r   { \"name\": \"SessionId\", \"type\": \"int32\", \"versions\": \"7+\", \"default\": \"0\", \"ignorable\": true,\r     \"about\": \"The fetch session ID.\" },\r   { \"name\": \"SessionEpoch\", \"type\": \"int32\", \"versions\": \"7+\", \"default\": \"-1\", \"ignorable\": true,\r     \"about\": \"The fetch session epoch, which is used for ordering requests in a session.\" },\r   { \"name\": \"Topics\", \"type\": \"[]FetchTopic\", \"versions\": \"0+\",\r     \"about\": \"The topics to fetch.\", \"fields\": [\r     { \"name\": \"Topic\", \"type\": \"string\", \"versions\": \"0-12\", \"entityType\": \"topicName\", \"ignorable\": true,\r       \"about\": \"The name of the topic to fetch.\" },\r     { \"name\": \"TopicId\", \"type\": \"uuid\", \"versions\": \"13+\", \"ignorable\": true, \"about\": \"The unique topic ID.\"},\r     { \"name\": \"Partitions\", \"type\": \"[]FetchPartition\", \"versions\": \"0+\",\r       \"about\": \"The partitions to fetch.\", \"fields\": [\r       { \"name\": \"Partition\", \"type\": \"int32\", \"versions\": \"0+\",\r         \"about\": \"The partition index.\" },\r       { \"name\": \"CurrentLeaderEpoch\", \"type\": \"int32\", \"versions\": \"9+\", \"default\": \"-1\", \"ignorable\": true,\r         \"about\": \"The current leader epoch of the partition.\" },\r       { \"name\": \"FetchOffset\", \"type\": \"int64\", \"versions\": \"0+\",\r         \"about\": \"The message offset.\" },\r       { \"name\": \"LastFetchedEpoch\", \"type\": \"int32\", \"versions\": \"12+\", \"default\": \"-1\", \"ignorable\": false,\r         \"about\": \"The epoch of the last fetched record or -1 if there is none.\"},\r       { \"name\": \"LogStartOffset\", \"type\": \"int64\", \"versions\": \"5+\", \"default\": \"-1\", \"ignorable\": true,\r         \"about\": \"The earliest available offset of the follower replica.  The field is only used when the request is sent by the follower.\"},\r       { \"name\": \"PartitionMaxBytes\", \"type\": \"int32\", \"versions\": \"0+\",\r         \"about\": \"The maximum bytes to fetch from this partition.  See KIP-74 for cases where this limit may not be honored.\" },\r       { \"name\": \"PartitionMaxNum\", \"type\": \"int32\", \"versions\": \"18+\", \"default\": \"-1\", \"ignorable\": true,\r         \"about\": \"The maximum number of messages to fetch from this partition. -1 means no limit.\" },\r       { \"name\": \"ReplicaDirectoryId\", \"type\": \"uuid\", \"versions\": \"17+\", \"taggedVersions\": \"17+\", \"tag\": 0, \"ignorable\": true,\r         \"about\": \"The directory id of the follower fetching.\" }\r     ]}\r   ]}, \r \r ...{code}\r \u00a0\r ----", "comments": ["Hello, this requires a KIP, are you going to create one?", "To Federico Valeri: Thanks for the feedback. Yes, I will start drafting a KIP for this proposal.", "[~corkitse] Thanks for the proposal. I will review the KIP once drafted. However, just highlighting so you can reference, ShareGroups/QueuesForKafka already have that in request spec - [MaxRecords|https://github.com/apache/kafka/blob/abbb6b3c13f0b87b8e905f853b7b94282f8d355c/clients/src/main/resources/common/message/ShareFetchRequest.json#L39]."], "tasks": {"summarization": "Add a Kafka client parameter to limit number of messages fetched - h3. Description\r \r Currently, Kafka fetch requests only support limiting the total size of messages ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19554 about?", "answer": "Add a Kafka client parameter to limit number of messages fetched"}}}
{"issue_id": "KAFKA-19553", "project": "KAFKA", "title": "Add more test coverage for existing Kafka Streams examples that currently lack test files", "status": "Open", "priority": "Minor", "reporter": "Shubhangi Pandey", "assignee": "Shubhangi Pandey", "created": "2025-07-27T22:33:57.000+0000", "updated": "2025-08-17T03:34:35.000+0000", "labels": ["new-bie"], "description": "The Kafka Streams examples directory\u00a0\r {code:java}\r (streams/examples/src/main/java/org/apache/kafka/streams/examples/){code}\r \u00a0contains several example applications to help users learn Kafka Streams patterns. However, the corresponding test directory\u00a0\r {code:java}\r (streams/examples/src/test/java/org/apache/kafka/streams/examples/){code}\r \u00a0only contains tests for the\u00a0\r {code:java}\r WordCount {code}\r example, while other examples lack proper test coverage.\r \r *Current State*\r Examples WITH tests:\r {code:java}\r wordcount/WordCountDemo.java \u2192 has wordcount/WordCountTest.java\r \r {code}\r Examples WITHOUT tests:\r {code:java}\r pipe/PipeDemo.java \u2192 no corresponding test\r pageview/PageViewTypedDemo.java \u2192 no corresponding test{code}\r Other examples in the directory \u2192 no corresponding tests", "comments": ["I want to work on this issue I created. can anyone help to assign this to myself?", "You should be all set now, to assign tickets to yourself.", "Thank you :)"], "tasks": {"summarization": "Add more test coverage for existing Kafka Streams examples that currently lack test files - The Kafka Streams examples directory\u00a0\r {code:java}\r (streams/examples/src/main/java/org/apache/kafka...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19553 about?", "answer": "Add more test coverage for existing Kafka Streams examples that currently lack test files"}}}
{"issue_id": "KAFKA-19552", "project": "KAFKA", "title": "Unclean leader election fails due to precedence issue", "status": "Open", "priority": "Critical", "reporter": "Maruthi", "assignee": null, "created": "2025-07-27T05:41:51.000+0000", "updated": "2025-07-28T16:06:33.000+0000", "labels": [], "description": "When broker config is set to enable unclean leader election, unless active controller also has the config enabled, election isnt triggered\u00a0\r \r unclean leader election never gets triggered because even though its enabled on the topic level, its disregarded as its not set \"dynamically\" looks like\u00a0\r \r unclean.leader.election.enable=true sensitive=false synonyms=\\{STATIC_BROKER_CONFIG:unclean.leader.election.enable=true, DEFAULT_CONFIG:unclean.leader.election.enable=false}\r \r [https://github.com/apache/kafka/blob/3.9.0/metadata/src/main/java/org/apache/kafka/controller/ConfigurationControlManager.java#L461]\r \r Looks like a valid order - this may be desired behavior in other cases but fails when some properties are set only on the \"broker\" nodes and not on \"controller\" nodes\r \r \u00a0\r \r Proposal: should look at topic's current config(static/inherited) as well instead of(or at least before) looking at staticConfig of the node. Topic's current config would be brokers static config anyway\u00a0\r \r Alternative: alert user somehow if staticConfigs arent matching between brokers and controllers- maybe very visible warning that configs should match in documentation to start with\u00a0\r \r Workaround: set unclean.leader.election config in controller properties as well - but its unclear what other configs also need to be set/etc to avoid similar issues in other places", "comments": ["[~todo] , thanks for the report. I think at first, we can improve on the doc, to notify users about this known issue, and then discuss what's the best solution for it. WDYT?", "thanks [~showuon] - that makes sense"], "tasks": {"summarization": "Unclean leader election fails due to precedence issue - When broker config is set to enable unclean leader election, unless active controller also has the c...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19552 about?", "answer": "Unclean leader election fails due to precedence issue"}}}
{"issue_id": "KAFKA-19551", "project": "KAFKA", "title": "Remove the handling of FatalExitError in RemoteStorageThreadPool", "status": "Resolved", "priority": "Major", "reporter": "Kamal Chandraprakash", "assignee": "Lan Ding", "created": "2025-07-27T05:12:59.000+0000", "updated": "2025-07-28T12:21:25.000+0000", "labels": [], "description": "FatalExitError is not thrown after KAFKA-19425 (See: [#20007|https://github.com/apache/kafka/pull/20007]). Clean up the handling of FatalExitError in [RemoteStorageThreadPool|https://sourcegraph.com/github.com/apache/kafka@trunk/-/blob/storage/src/main/java/org/apache/kafka/storage/internals/log/RemoteStorageThreadPool.java?L59].", "comments": ["Hi [~ckamal], if you are not working on this, may i take it. Thanks."], "tasks": {"summarization": "Remove the handling of FatalExitError in RemoteStorageThreadPool  - FatalExitError is not thrown after KAFKA-19425 (See: [#20007|https://github.com/apache/kafka/pull/20...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19551 about?", "answer": "Remove the handling of FatalExitError in RemoteStorageThreadPool"}}}
{"issue_id": "KAFKA-19550", "project": "KAFKA", "title": "Integration test for Streams-related Admin APIs[1/N]", "status": "Resolved", "priority": "Major", "reporter": "Lucy Liu", "assignee": "Lucy Liu", "created": "2025-07-26T02:52:36.000+0000", "updated": "2025-09-06T23:28:08.000+0000", "labels": [], "description": "Integration tests for Stream Admin related API\r \r This one adds:\r  * Integration test for\u00a0{{Admin#describeStreamsGroups}}\u00a0API\r  * Integration test fo {{Admin#deleteStreamsGroups}} API\r \r Next one: https://issues.apache.org/jira/browse/KAFKA-19565", "comments": [], "tasks": {"summarization": " Integration test for Streams-related Admin APIs[1/N] - Integration tests for Stream Admin related API\r \r This one adds:\r  * Integration test for\u00a0{{Admin#de...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19550 about?", "answer": "Integration test for Streams-related Admin APIs[1/N]"}}}
{"issue_id": "KAFKA-19549", "project": "KAFKA", "title": "network_degrade_test.py failure", "status": "Open", "priority": "Major", "reporter": "Mickael Maison", "assignee": null, "created": "2025-07-25T15:41:16.000+0000", "updated": "2025-10-03T14:36:33.000+0000", "labels": [], "description": "The following test is consistently failing in my environment:\r kafkatest.tests.core.network_degrade_test.test_rate_device_name=eth0_latency_ms=50_metadata_quorum=COMBINED_KRAFT_rate_limit_kbit=1000000_task_name=rate-1000-latency-50\r \r Stacktrace:\r \r {noformat}\r runs 1, 2, 3, 4, 5 summary:\r AssertionError('Expected most of the measured rates to be within an order of magnitude of target 1000000. This means `tc` did not limit the bandwidth as expected.')\r Traceback (most recent call last):\r   File \"/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py\", line 351, in _do_run\r     data = self.run_test()\r   File \"/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py\", line 411, in run_test\r     return self.test_context.function(self.test)\r   File \"/usr/local/lib/python3.9/dist-packages/ducktape/mark/_mark.py\", line 438, in wrapper\r     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)\r   File \"/opt/kafka-dev/tests/kafkatest/tests/core/network_degrade_test.py\", line 146, in test_rate\r     assert len(acceptable_rates) > 5, msg\r AssertionError: Expected most of the measured rates to be within an order of magnitude of target 1000000. This means `tc` did not limit the bandwidth as expected.\r {noformat}", "comments": ["I assume your environment is MacOS with podman machine right? If that assumption is correct then the problem is that on MacOS and specifically when using *traffic control utility* with *netem* (i.e., qdisc to simulate network latency and bandwidth limits). That test case needs kernel module {*}sch_netem{*}, which is not available in Podman's Fedora CoreOS VMs on MacOS by default => so you have to enabled it => to make such test case pass.\u00a0\r \r One way how to solve it is:\r {code:java}\r $ podman machine ssh\r $ sudo modprobe sch_netem                                             // here you would see that there is no such kernel module...\r $ sudo rpm-ostree install kernel-modules-extra                        // this would install missing kernel modules\r $ sudo rm /etc/modprobe.d/sch_netem-blacklist.conf                    // you also have to remove config blocking to auto-load kernel module \r $ sudo sh -c 'echo sch_netem >/etc/modules-load.d/sch_netem.conf'     // enable loading sch_netem on start-up\r $ podman machine stop\r $ podman machine start\r $ podman machine ssh\r $ lsmod | grep sch_netem // here you would see that in your podman machine you have sch_netem installed :){code}\r After those steps, which I wrote above it should also work on MacOS with podman machine :)."], "tasks": {"summarization": "network_degrade_test.py failure - The following test is consistently failing in my environment:\r kafkatest.tests.core.network_degrade_...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19549 about?", "answer": "network_degrade_test.py failure"}}}
{"issue_id": "KAFKA-19548", "project": "KAFKA", "title": "Broker Startup: Handle Checkpoint Creation Failure via logDirFailureChannel", "status": "Closed", "priority": "Major", "reporter": "Haozhong Ma", "assignee": "Haozhong Ma", "created": "2025-07-25T07:07:50.000+0000", "updated": "2025-08-26T08:53:38.000+0000", "labels": [], "description": "In our production environment, we encountered a scenario where a broker failed to start due to checkpoint creation failure on a single disk (caused by disk corruption or filesystem errors). According to Kafka's design, such disk-level failures should be isolated via\u00a0{{{}logDirFailureChannel{}}}, allowing other healthy disks to continue serving traffic. However, upon reviewing the\u00a0{{CheckpointFileWithFailureHandler}}\u00a0implementation, we observed that while methods like\u00a0{{{}write{}}},\u00a0{{{}read{}}}, and\u00a0{{writeIfDirExists}}\u00a0handle\u00a0{{IOException}}\u00a0by routing the affected\u00a0{{log.dir}}\u00a0to\u00a0{{{}logDirFailureChannel{}}}, the checkpoint initialization process lacks this fault-tolerant behavior. Should checkpoint creation adopt the same failure-handling logic? If this is not an intentional design, I will submit a PR to fix this issue.", "comments": [], "tasks": {"summarization": "Broker Startup: Handle Checkpoint Creation Failure via logDirFailureChannel - In our production environment, we encountered a scenario where a broker failed to start due to check...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19548 about?", "answer": "Broker Startup: Handle Checkpoint Creation Failure via logDirFailureChannel"}}}
{"issue_id": "KAFKA-19547", "project": "KAFKA", "title": "Kafka Sink Improvement: Add failOnError to Control Exception Behavior", "status": "Closed", "priority": "Major", "reporter": "Ejaskhan", "assignee": null, "created": "2025-07-24T23:00:16.000+0000", "updated": "2025-08-12T04:22:02.000+0000", "labels": ["Kafka", "flink-connector-kafka"], "description": "Currently, even if the delivery guarantee is set to {*}NONE{*}, the Kafka sink throws an exception, which results in job restarts.\r \r !image-2025-07-25-00-56-15-542.png!\r \r *Proposal:* Introduce a new configuration parameter, {{{}failOnError{}}}, in {{KafkaSinkBuilder}} to suppress throwing the error and log it instead. With this change, exceptions from the Kafka cluster will no longer result in job restarts.", "comments": ["Created the issue here, https://issues.apache.org/jira/browse/FLINK-38226"], "tasks": {"summarization": "Kafka Sink Improvement: Add failOnError to Control Exception Behavior - Currently, even if the delivery guarantee is set to {*}NONE{*}, the Kafka sink throws an exception, ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19547 about?", "answer": "Kafka Sink Improvement: Add failOnError to Control Exception Behavior"}}}
{"issue_id": "KAFKA-19546", "project": "KAFKA", "title": "Rebalance should be triggered if the subscription is changed during group protocol downgrade triggered by static member replacement", "status": "Resolved", "priority": "Major", "reporter": "Dongnuo Lyu", "assignee": "Dongnuo Lyu", "created": "2025-07-24T20:28:25.000+0000", "updated": "2025-09-26T15:31:44.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Rebalance should be triggered if the subscription is changed during group protocol downgrade triggered by static member replacement - ...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19546 about?", "answer": "Rebalance should be triggered if the subscription is changed during group protocol downgrade triggered by static member replacement"}}}
{"issue_id": "KAFKA-19545", "project": "KAFKA", "title": "Remove MetadataVersionValidator", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "Shang-Hao Yang", "created": "2025-07-24T05:35:58.000+0000", "updated": "2025-07-24T14:23:47.000+0000", "labels": [], "description": "the usage was removed by KAFKA-17393, rendering it useless", "comments": [], "tasks": {"summarization": "Remove MetadataVersionValidator - the usage was removed by KAFKA-17393, rendering it useless...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19545 about?", "answer": "Remove MetadataVersionValidator"}}}
{"issue_id": "KAFKA-19544", "project": "KAFKA", "title": "Improve `MetadataVersion.fromVersionString()` to take an enableUnstableFeature flag", "status": "Resolved", "priority": "Major", "reporter": "Lan Ding", "assignee": "Lan Ding", "created": "2025-07-24T01:07:11.000+0000", "updated": "2025-10-09T16:19:54.000+0000", "labels": [], "description": "see the discussion https://github.com/apache/kafka/pull/20185#discussion_r2223756688", "comments": [], "tasks": {"summarization": "Improve `MetadataVersion.fromVersionString()` to take an enableUnstableFeature flag - see the discussion https://github.com/apache/kafka/pull/20185#discussion_r2223756688...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19544 about?", "answer": "Improve `MetadataVersion.fromVersionString()` to take an enableUnstableFeature flag"}}}
{"issue_id": "KAFKA-19543", "project": "KAFKA", "title": "The set of metrics returned from Consumer.metrics() changes after Consumer.close()", "status": "Open", "priority": "Minor", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-07-23T23:11:57.000+0000", "updated": "2025-07-31T06:02:08.000+0000", "labels": [], "description": "metrics() reflects the current state, so if you get the metrics before you close and then look at them afterward, some of the metrics are missing because some were removed.", "comments": ["Why is this a problem? Isn't it \"by design\" that the returned `Map` should become empty\"when the consumer gets closed?", "[~mjsax]\u2014should the map returned from {{metrics()}} represent a _snapshot_ of the metrics at the point in time it's called, or should it reflect _ongoing_ changes? The current behavior is the latter.\r \r Because of KAFKA-19542, _some_ of the metrics are present in the Map, post-{{close()}}, but not _all_, which is confusing to the user. So the fact that metrics will be empty on {{close()}} should be documented in the API.", "Maybe I misunderstand the ticket description? \u2013 I thought that it would be ok / expected, that:\r {code:java}\r consumer.metrics(); // return a Map of all metrics\r consumer.close();\r consumer.metrics(); // return an empty Map as all metrics got de-registered{code}\r The other question you ask, if the metrics should be a snapshot vs dynamic seem to be an orthogonal question? And I think, yes, the metrics (ie, their values) should be dynamic, and reflect ongoing changes.\r \r But re-reading the ticket it seem that the issue is as follows:\r {code:java}\r Map metrics = consumer.metrics();\r consumer.close();\r // the previously returned `metrics` Map lost some keys?{code}\r This would indeed be odd. After consumer.close() I would only expect that the metric values don't change any longer, but not that whole entries disappear.\r \r Should be an easy fix though, to make a \"deep copy\" of the metric Map? At least this is what we do in Kafka Streams.\r \r On the other hand, it seems there is two \"types\" of metrics: ones that are always present (like `time-between-poll-avg`) vs more dynamic ones like `record-lag` which depend on the currently assigned partitions, and might be added/removed during a rebalance? These changes would not be reflected if we make a deep copy of the metric Map. Following this train of though, it would make even sense if a previously returned metrics Map becomes empty on `consumer.close()` as all metrics get de-registered.\r \r Not sure what the intended / best behavior should be."], "tasks": {"summarization": "The set of metrics returned from Consumer.metrics() changes after Consumer.close() - metrics() reflects the current state, so if you get the metrics before you close and then look at th...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19543 about?", "answer": "The set of metrics returned from Consumer.metrics() changes after Consumer.close()"}}}
{"issue_id": "KAFKA-19542", "project": "KAFKA", "title": "Consumer.close() does not remove all added sensors from Metrics", "status": "In Progress", "priority": "Minor", "reporter": "Kirk True", "assignee": "Kirk True", "created": "2025-07-23T23:10:38.000+0000", "updated": "2025-10-24T00:57:16.000+0000", "labels": [], "description": "When a Consumer is created, it adds sensors to the Metrics object. Some of the classes that manage metrics implement AutoCloseable and diligently remove any Sensors it added from the Metrics object. However, not all of the sensors are removed. It feels like we should be consistent.", "comments": ["Wondering if there is any relationship to https://issues.apache.org/jira/browse/KAFKA-19529 ?"], "tasks": {"summarization": "Consumer.close() does not remove all added sensors from Metrics - When a Consumer is created, it adds sensors to the Metrics object. Some of the classes that manage m...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19542 about?", "answer": "Consumer.close() does not remove all added sensors from Metrics"}}}
{"issue_id": "KAFKA-19541", "project": "KAFKA", "title": "KRaft should handle snapshot fetches under slow networks", "status": "Open", "priority": "Major", "reporter": "Jonah Hooper", "assignee": "Jonah Hooper", "created": "2025-07-23T19:13:35.000+0000", "updated": "2025-07-25T15:50:37.000+0000", "labels": [], "description": "If a \"new\" controller does not have any Metadata logs stored and joins a Quorum it will attempt a FETCH_SNAPSHOT to active controller to receive an up to date log. It will perform this from FollowerState.\u00a0\r By default; KRaft allows for 2s to complete all requests before it considers the active-controller (leader) unavailable. If a request (including FETCH_SNAPSHOT) exceeds 2s it will timeout and the controller, if in FollowerState, will transition to CandidateState. If a controller has not fetched logs from active controller it can never become leader since it has no data. As such it will eventually transition back to follower state.\u00a0\r If the snapshot on the active controller is larger (in size on disk) than it would take to download given network conditions between active controller and new controller, then its possible that the \"new\" controller will get stuck in a loop.\u00a0\r In this state it will transition from:\r {code:java}\r Unattached -> Follower -> (Fail to FETCH_SNAPSHOT) -> Candidate -> ... -> Unattached -> Follower -> (Fail to FETCH_SNAPSHOT) -> Candidate ...{code}\r Consider snapshot `xxxx.checkpoint` = 20mb and the connection between Active controller and \"new\" controller is 2Mbs then, it would take 10s complete FETCH_SNAPSHOT of\u00a0 `xxxx.checkpoint`.\u00a0\r In this case, unless network conditions improve then \"new controller\" will be stuck in a loop forever.", "comments": [], "tasks": {"summarization": "KRaft should handle snapshot fetches under slow networks - If a \"new\" controller does not have any Metadata logs stored and joins a Quorum it will attempt a FE...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19541 about?", "answer": "KRaft should handle snapshot fetches under slow networks"}}}
{"issue_id": "KAFKA-19540", "project": "KAFKA", "title": "Fix incorrect error message", "status": "Open", "priority": "Minor", "reporter": "Matthias J. Sax", "assignee": "Ming-Yen Chung", "created": "2025-07-23T16:25:34.000+0000", "updated": "2025-07-24T02:29:16.000+0000", "labels": [], "description": "When the new `streams.group` feature is enabled broker side, but `group.coordinator.rebalance.protocols` config does disable \"streams\" protocol, the client side error message is misleading:\r {code:java}\r rg.apache.kafka.common.errors.UnsupportedVersionException: The cluster does not support the STREAMS group protocol or does not support the versions of the STREAMS group protocol used by this client (used versions: 0 to 0). {code}\r We should take a look to see if we can improve the error message. (Might not be possible easily).", "comments": [], "tasks": {"summarization": "Fix incorrect error message - When the new `streams.group` feature is enabled broker side, but `group.coordinator.rebalance.protoc...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19540 about?", "answer": "Fix incorrect error message"}}}
{"issue_id": "KAFKA-19539", "project": "KAFKA", "title": "Kafka Streams should also purge internal topics based on user commit requests", "status": "Resolved", "priority": "Major", "reporter": "Matthias J. Sax", "assignee": "Lan Ding", "created": "2025-07-23T15:38:47.000+0000", "updated": "2025-09-29T15:27:16.000+0000", "labels": [], "description": "Based on [https://stackoverflow.com/questions/79712093/kafka-streams-application-does-not-purge-repartition-topic-records-when-manual-c]\r \r Kafka Streams seems to use \"delete record request\" for internal topic only on regular commit via `commit.interval.ms`.\r \r However, if `commit.interval.ms` is set to a very high value, and users request commits manually via `context.commit()` no purging happens.", "comments": ["Hi [~mjsax], if you are not working on this, may i take it. Thanks.", "Sure. Go ahead. I did not plan to pick it up any time soon anyway :)"], "tasks": {"summarization": "Kafka Streams should also purge internal topics based on user commit requests - Based on [https://stackoverflow.com/questions/79712093/kafka-streams-application-does-not-purge-repa...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19539 about?", "answer": "Kafka Streams should also purge internal topics based on user commit requests"}}}
{"issue_id": "KAFKA-19538", "project": "KAFKA", "title": "Kafka uses vulnerable Apache Commons Lang3 version (3.12.0) \u2013 Uncontrolled Recursion (CVE-2025-48924)", "status": "Resolved", "priority": "Blocker", "reporter": "Mangesh Dushman", "assignee": null, "created": "2025-07-23T05:54:52.000+0000", "updated": "2025-07-23T07:12:22.000+0000", "labels": [], "description": "Apache Kafka currently includes the {{org.apache.commons:commons-lang3}} library version {*}3.12.0{*}, which is affected by a critical {*}Uncontrolled Recursion vulnerability (CVE-2025-48924){*}.\r \r *Vulnerability Details:*\r  * Affected Method: {{ClassUtils.getClass(String)}}\r \r  * Impact: Can throw a {{StackOverflowError}} on very long input values. Since {{Error}} types are generally not caught by applications, this can lead to unexpected application termination or denial of service.\r \r  * Affected Versions:\r \r  ** {{commons-lang3}} versions *3.0 to < 3.18.0*\r \r  ** {{commons-lang}} versions *2.0 to 2.6*\r \r *Current Kafka Status:*\r  * As of Kafka *4.0.0* and {*}3.9.1{*}, the project uses {*}Apache Commons Lang3 version 3.12.0{*}, which falls within the affected version range.", "comments": ["Duplicated with KAFKA-19520."], "tasks": {"summarization": "Kafka uses vulnerable Apache Commons Lang3 version (3.12.0) \u2013 Uncontrolled Recursion (CVE-2025-48924) - Apache Kafka currently includes the {{org.apache.commons:commons-lang3}} library version {*}3.12.0{*...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19538 about?", "answer": "Kafka uses vulnerable Apache Commons Lang3 version (3.12.0) \u2013 Uncontrolled Recursion (CVE-2025-48924)"}}}
{"issue_id": "KAFKA-19537", "project": "KAFKA", "title": "StreamsGroupCommand should return exit code", "status": "Resolved", "priority": "Minor", "reporter": "Matthias J. Sax", "assignee": "Chih-Yuan Chien", "created": "2025-07-23T05:00:33.000+0000", "updated": "2025-10-15T01:47:03.000+0000", "labels": [], "description": "The newly added `StreamsGroupCommand.java` tool, does not return a popper error code, what might break scripts which use `bin/kafka-streams-group.sh` as they cannot do proper error handling.\r \r Similar to existing `StreamsResetter.java` we should return a proper error code.", "comments": [], "tasks": {"summarization": "StreamsGroupCommand should return exit code - The newly added `StreamsGroupCommand.java` tool, does not return a popper error code, what might bre...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19537 about?", "answer": "StreamsGroupCommand should return exit code"}}}
{"issue_id": "KAFKA-19536", "project": "KAFKA", "title": "KStream-KStream leftJoin didn't consider gracePeriod for out-of-order records", "status": "Open", "priority": "Major", "reporter": "congye", "assignee": null, "created": "2025-07-23T00:32:09.000+0000", "updated": "2025-07-23T19:07:58.000+0000", "labels": [], "description": "Actual: in\u00a0KStreamKStreamJoin.java, there's code snippet:\r {code:java}\r // This condition below allows us to process the out-of-order records without the need\r // to hold it in the temporary outer store\r if (outerJoinStore.isEmpty() || timeTo < sharedTimeTracker.streamTime) {\r     context().forward(record.withValue(joiner.apply(record.key(), record.value(), null)));\r } {code}\r So for out-of-order records, when the window closed, it may forward the leftJoin result without waiting for extra gracePeriod.\r \r \u00a0\r \r Expect: change the condition to: timeTo {color:#de350b}+ joinGraceMs{color} < sharedTimeTracker.streamTime\r {code:java}\r if (outerJoinStore.isEmpty() || timeTo + joinGraceMs < sharedTimeTracker.streamTime) {            context().forward(record.withValue(joiner.apply(record.key(), record.value(), null))); \r }  {code}", "comments": [], "tasks": {"summarization": "KStream-KStream leftJoin didn't consider gracePeriod for out-of-order records - Actual: in\u00a0KStreamKStreamJoin.java, there's code snippet:\r {code:java}\r // This condition below allo...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19536 about?", "answer": "KStream-KStream leftJoin didn't consider gracePeriod for out-of-order records"}}}
{"issue_id": "KAFKA-19535", "project": "KAFKA", "title": "add integration tests for DescribeProducersOptions#brokerId", "status": "Resolved", "priority": "Minor", "reporter": "Chia-Ping Tsai", "assignee": "HongYi Chen", "created": "2025-07-22T18:35:58.000+0000", "updated": "2025-09-03T19:15:29.000+0000", "labels": [], "description": "as title", "comments": [], "tasks": {"summarization": "add integration tests for DescribeProducersOptions#brokerId - as title...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19535 about?", "answer": "add integration tests for DescribeProducersOptions#brokerId"}}}
{"issue_id": "KAFKA-19534", "project": "KAFKA", "title": "Remove MX4j support", "status": "Open", "priority": "Major", "reporter": "Federico Valeri", "assignee": "Arnav Dadarya", "created": "2025-07-22T13:51:02.000+0000", "updated": "2025-07-23T05:52:19.000+0000", "labels": ["kip"], "description": "Remove MX4j support.\r \r https://cwiki.apache.org/confluence/display/KAFKA/KIP-1193%3A+Deprecate+MX4j+support", "comments": ["Can this ticket be worked on before the 5.0 release?", "[~ardada2468] I don't think so.", "Got it, once it is eligible to work on I would like to take this task.", "Ok, fine with me."], "tasks": {"summarization": "Remove MX4j support - Remove MX4j support.\r \r https://cwiki.apache.org/confluence/display/KAFKA/KIP-1193%3A+Deprecate+MX4j...", "classification": "task", "qna": {"question": "What is the issue KAFKA-19534 about?", "answer": "Remove MX4j support"}}}
