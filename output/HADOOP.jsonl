{"issue_id": "HADOOP-8661", "project": "HADOOP", "title": "RemoteException's Stack Trace would be better returned by getStackTrace", "status": "Patch Available", "priority": "Major", "reporter": "Robert Joseph Evans", "assignee": null, "created": "2012-08-08T16:50:59.000+0000", "updated": "2017-09-11T05:21:07.000+0000", "labels": ["BB2015-05-TBR"], "description": "It looks like all exceptions produced by RemoteException include the full stack trace of the original exception in the message.  This is different from 1.0 behavior to aid in debugging, but it would be nice to actually parse the stack trace and return it through getStackTrace instead of through getMessage.", "comments": ["Should we be fixing this in Hadoop - the content of exception stack trace? Also not clear why Oozie stores exceptions.", "The issue is not with the contents of the stack trace.  The stack trace is fine.  The issue is that the message of the exception includes the stack trace.  For example:  {code} try {   Path p = new Path(file);   FileSystem fs = p.getFileSystem(conf);   fs.delete(p, true); } catch( IOException e) {   System.err.println(\"MESSAGE: \" +e.getMessage()); } {code}  If this is run on 1.0.2 and it gets a permission denied error you only get something like {noformat} MESSAGE: Permission denied: user=notme, access=EXECUTE, inode=\"/user/me/test\":me:supergroup:d--------- {noformat}  But on trunk, 2.0, and 0.23 you get {noformat} MESSAGE: Permission denied: user=notme, access=EXECUTE, inode=\"/user/me/test\":me:supergroup:d---------         at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:205)         at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:161)         at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:128)         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:3572)         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:1931)         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:1896)         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:539)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:394)         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1528)         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1524)         at java.security.AccessController.doPrivileged(Native Method)         at javax.security.auth.Subject.doAs(Subject.java:396)         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1522) {noformat}  Oozie tries to save the message in its database, but when the full stack trace is included it gets very large and can cause issues.", "Oozie only stores the value returned by Exception.getMessage() so the users can debug their workflows if the hadoop job fails. If the getMessage() returns the entire stack trace, the database cannot handle it.", "Bumping this to critical because after talking with some oozie guys the message length can cause overrun the varchar limit of the column and cause some bad errors for oozie.", "Curious, why isn't Oozie checking the length of what will be written to the database and taking appropriate action if it's too long (e.g.: truncating).", "On 2.0, the stack trace may be a little different, since we have switched to protobuf RPC engine. This change was done in HADOOP-6686. So any solution needs to consider some of the discussions from that jira.", "Echoing what Jason said, it is a good idea to handle this in Oozie. Making an assumption about the length of exception message and expecting that it would not change in upstream project is not a good idea.   That said, we could change the message back to shorter length given the thrown exception has been set with initCause() from where the stack trace can be derived.", "I agree that Oozie should not make any assumption about the length of the error message. Created a JIRA OOZIE-946", "Glad to see OOZIE-946.  Virag, is it OK for us to drop the severity on HADOOP-8661 given OOZIE-946?  I did read through HADOOP-6686 and the reason for including the entire stack trace is to improve debugging, which is obviously something we want.  Most exceptions do not have a complete stack trace in their message though.  That is what getStackTrace() is for.  I have spent a little time to write some code that can parse the stack trace and insert it back into the generated exception. I think this is the cleaner way to get the debugging, and keep the generated massage almost identical to the original message.  I wanted to know what others thought about the suggestion. I am fine with dropping it if OOZIE-946 is sufficient. I need some time to clean up the code a bit and add some more tests to be sure everything works OK even in error cases, but I don't want to spend much time on it if this is going to be contentious.", "bq. I have spent a little time to write some code that can parse the stack trace and insert it back into the generated exception Not sure what you mean here. You could still shorten the message in this jira. The cause of the exception is already in the thrown exception from where stack trace can be obtained.", "@Suresh,  Yes I completely agree about the name of the JIRA, once Virag agrees that OOZIE-946 is the real fix for the issue, I will probably close this JIRA as a dupe of OOZIE-946, and then file a separate one for parsing the stack trace as new work.  But I would like to leave this open just in case there is something that will prevent oozie form fixing the issue quickly, so that we can unblock them.", "OOZIE-946 will go in the next release of Oozie (Oozie 3.3). But as this is regression from 1.0, this JIRA needs to be fixed for Oozie 3.2 to work with 23.", "Also, in hadoop 1.0, the error message was {code} message[org.apache.hadoop.security.AccessControlException: Permission denied: user=strat_ci, access=ALL, inode=\"output-mr\":strat_ci:hdfs:r--r--r--] {code}  But in 23, the message doesn't have the Exception class name (org.apache.hadoop.security.AccessControlException).  Also, I think getMessage() should just have the message with which the exception is constructed while getStackTrace() should have the entire stack trace.  IMO, user's running their hadoop jobs through Oozie shouldn't be seeing the entire stack trace.  Oozie-946 will only ensure that a large value doesn't blow up the column in the database. But the getMessage() should be fixed in hadoop.", "bq. But in 23, the message doesn't have the Exception class name (org.apache.hadoop.security.AccessControlException). We removed this redundant information from message in HADOOP-6686. You can get back to previous message on Oozie with: {{exception.getClass().getName() + \": \" + exception.getMessage()}}", "Is the problem at the RPC layer, or is it the way that stack traces get passed around.  For Java RMI I ended up writing a wrapper class that we could be confident would be at the far end -this extracted and propagated the stack traces http://smartfrog.svn.sourceforge.net/viewvc/smartfrog/trunk/core/smartfrog/src/org/smartfrog/sfcore/common/SmartFrogExtractedException.java?revision=8882&view=markup  We did something similar in Axis, converting the stack trace to something in an axis-namespaced element (then stripping that by default in responses unless the server's debug flag is set): http://svn.apache.org/viewvc/axis/axis2/java/core/trunk/modules/kernel/src/org/apache/axis2/AxisFault.java?view=markup", "bq. We removed this redundant information from message in HADOOP-6686.   Ok..Makes sense not to have the class name in the exception message.", "This patch will split the stack trace from the message in the RemoteException.  It will also parse the stack trace and insert it into the generated exception.  If this is a critical issue then I would like to see OOZIE-946 go into OOZIE 3.2 not just 3.3, but in either case I think this patch should be OK to go in, even for 0.23.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540036/HADOOP-8661.txt   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1273//console  This message is automatically generated.", "The patch did not apply because it was based off of 0.23, and  HDFS-3504 apparently touched this file too.  I will rebase on trunk.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540050/HADOOP-8661.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1274//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1274//console  This message is automatically generated.", "@Bobby, Just realized while fixing OOZIE-946 that the column storing error message is Blob and not varchar. So the issue is not as severe as I thought was. Sorry for missing this earlier.", "OOZIE-946 was just closed as invalid, because the data is stored in a blob not a varchar.  I am changing this over to be an improvement, dropping the severity, and renaming the JIRA.  Sorry for causing others issues by filing a JIRA as a bug before totally understanding the issue. I am also updating the target to 2.2 as 0.23 is closed to new work.", "just an up-merge. It would be nice if someone could review this so I don't have to keep it up to date.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12550509/HADOOP-8661.txt   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1666//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1666//console  This message is automatically generated.", "cancel and resubmitting", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12550509/HADOOP-8661.txt   against trunk revision 1556f86.      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5131//console  This message is automatically generated.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 1s {color} | {color:blue} The patch file was not named according to hadoop's naming conventions. Please see https://wiki.apache.org/hadoop/HowToContribute for instructions. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red} 0m 4s {color} | {color:red} HADOOP-8661 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12550509/HADOOP-8661.txt | | JIRA Issue | HADOOP-8661 | | Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/8337/console |   This message was automatically generated.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} | | {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  4s{color} | {color:blue} The patch file was not named according to hadoop's naming conventions. Please see https://wiki.apache.org/hadoop/HowToContribute for instructions. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m 12s{color} | {color:red} HADOOP-8661 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Issue | HADOOP-8661 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12550509/HADOOP-8661.txt | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11974/console | | Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |   This message was automatically generated.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} | | {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  3s{color} | {color:blue} The patch file was not named according to hadoop's naming conventions. Please see https://wiki.apache.org/hadoop/HowToContribute for instructions. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HADOOP-8661 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Issue | HADOOP-8661 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12550509/HADOOP-8661.txt | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13239/console | | Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |   This message was automatically generated."], "tasks": {"summarization": "RemoteException's Stack Trace would be better returned by getStackTrace - It looks like all exceptions produced by RemoteException include the full stack trace of the origina...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8661 about?", "answer": "RemoteException's Stack Trace would be better returned by getStackTrace"}}}
{"issue_id": "HADOOP-8660", "project": "HADOOP", "title": "TestPseudoAuthenticator failing with NPE", "status": "Closed", "priority": "Major", "reporter": "Eli Collins", "assignee": "Alejandro Abdelnur", "created": "2012-08-07T20:42:17.000+0000", "updated": "2012-10-11T17:45:06.000+0000", "labels": [], "description": "This test started failing recently, on top of trunk:  testAuthenticationAnonymousAllowed(org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator)  Time elapsed: 0.241 sec  <<< ERROR! java.lang.NullPointerException         at org.apache.hadoop.security.authentication.client.PseudoAuthenticator.authenticate(PseudoAuthenticator.java:75)         at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:232)         at org.apache.hadoop.security.authentication.client.AuthenticatorTestCase._testAuthentication(AuthenticatorTestCase.java:127)         at org.apache.hadoop.security.authentication.client.TestPseudoAuthenticator.testAuthenticationAnonymousAllowed(TestPseudoAuthenticator.java:65)", "comments": ["Perhaps related to HADOOP-8660?", "Related to HADOOP-8644, missed the point that if using mockito would not propagate the received connection.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539747/HADOOP-8660.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-auth.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1261//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1261//console  This message is automatically generated.", "+1     why didn't the HADOOP-8644 test results catch this? Added in a later patch that jenkins didn't run against or the primary job doesn't run the hadoop-auth tests (seems for former given jenkins reported \"The patch passed unit tests in hadoop-common-project/hadoop-auth\")?", "@eli, I think it was a case of  'tucu' error, run the tests from the IDE and it looks I didn't force a recompile and it was pickup the previous compiled class.   Committed to trunk and branch-2.", "Integrated in Hadoop-Hdfs-trunk-Commit #2628 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2628/])     HADOOP-8660. TestPseudoAuthenticator failing with NPE. (tucu) (Revision 1370812)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370812 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Common-trunk-Commit #2563 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2563/])     HADOOP-8660. TestPseudoAuthenticator failing with NPE. (tucu) (Revision 1370812)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370812 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk-Commit #2582 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2582/])     HADOOP-8660. TestPseudoAuthenticator failing with NPE. (tucu) (Revision 1370812)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370812 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1130 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1130/])     HADOOP-8660. TestPseudoAuthenticator failing with NPE. (tucu) (Revision 1370812)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370812 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1162 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1162/])     HADOOP-8660. TestPseudoAuthenticator failing with NPE. (tucu) (Revision 1370812)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370812 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt"], "tasks": {"summarization": "TestPseudoAuthenticator failing with NPE - This test started failing recently, on top of trunk:  testAuthenticationAnonymousAllowed(org.apache....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8660 about?", "answer": "TestPseudoAuthenticator failing with NPE"}}}
{"issue_id": "HADOOP-8659", "project": "HADOOP", "title": "Native libraries must build with soft-float ABI for Oracle JVM on ARM", "status": "Closed", "priority": "Major", "reporter": "Trevor Robinson", "assignee": "Colin McCabe", "created": "2012-08-07T20:41:19.000+0000", "updated": "2012-10-11T17:45:08.000+0000", "labels": [], "description": "There was recently an ABI (application binary interface) change in most Linux distributions for modern ARM processors (ARMv7). Historically, hardware floating-point (FP) support was optional/vendor-specific for ARM processors, so for software compatibility, the default ABI required that processors with FP units copy FP arguments into integer registers (or memory) when calling a shared library function. Now that hardware floating-point has been standardized for some time, Linux distributions such as Ubuntu 12.04 have changed the default ABI to leave FP arguments in FP registers, since this can significantly improve performance for FP libraries.  Unfortunately, Oracle has not yet released a JVM (as of 7u4) that supports the new ABI, presumably since this involves some non-trivial changes to components like JNI. While the soft-float JVM can run on systems with multi-arch support (currently Debian/Ubuntu) using compatibility libraries, this configuration requires that any third-party JNI libraries also be compiled using the soft-float ABI. Since hard-float systems default to compiling for hard-float, an extra argument to GCC (and installation of a compatibility library) is required to build soft-float Hadoop native libraries that work with the Oracle JVM.  Note that OpenJDK on hard-float systems does use the new ABI, and expects JNI libraries to use it as well. Therefore the fix for this issue requires detecting the float ABI of the current JVM.", "comments": ["The attached patch factors out platform-specific build configuration for various native libraries (e.g. HADOOP-8538) into a single included file and adds support for building soft-float libraries on hard-float ARM systems when using a soft-float JVM.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539758/HADOOP-8659.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      -1 javac.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1262//console  This message is automatically generated.", "I don't know the cmake stuff quite well enough to review, but one question: why does this affect us despite not having any calls in libhadoop that use float arguments? Is the calling convention different even for non-float args?", "I don't think it's different for non-float args. The problem is all of the transitive dependencies, such as using a different libc. When trying to load a JNI native library with the wrong float ABI, the JVM usually crashes silently with exit code 1. For instance, the build currently dies on hard-float ARM with the Oracle JVM running hadoop-hdfs-project/hadoop-hdfs/target/native/test_libhdfs_threaded.", "Updated patch based on testing with hard-float OpenJDK. Also verified unchanged behavior on x86-64.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539768/HADOOP-8659.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      -1 javac.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1263//console  This message is automatically generated.", "Update patch to remove unnecessary dependency on JAVA_JVM_LIBRARY from hadooppipes, which caused build failure in Jenkins:  {noformat}CMake Error: The following variables are used in this project, but they are set to NOTFOUND. Please set them or make sure they are set and tested correctly in the CMake files: JAVA_JVM_LIBRARY (ADVANCED)     linked by target \"hadooppipes\" in directory /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-tools/hadoop-pipes/src {noformat}", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539913/HADOOP-8659.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1269//console  This message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539957/HADOOP-8659.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS                   org.apache.hadoop.hdfs.TestFileConcurrentReader                   org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics                   org.apache.hadoop.hdfs.server.namenode.TestFsck      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1271//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1271//console  This message is automatically generated.", "This looks good overall.  {code} +    else () +        # On hard-float systems, soft-float compatibility dev packages are required, +        # e.g. libc6-dev-armel on Ubuntu 12.04. +        message(\"Soft-float JVM detected; ensure that soft-float dev packages are installed\") +        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -mfloat-abi=softfp\") +    endif () {code}  Would it be possible to use {{CHECK_SYMBOL_EXISTS}} or {{CHECK_LIBRARY_EXISTS}} to ensure that the soft-float dev packages are installed?  I'm not too familiar with soft-float libraries on ARM, so I'm just guessing here.  {code} +    execute_process( +        COMMAND readelf -A ${JAVA_JVM_LIBRARY} +        OUTPUT_VARIABLE JVM_ELF_ARCH +        ERROR_QUIET) +    if (JVM_ELF_ARCH MATCHES \"Tag_ABI_VFP_args: VFP registers\") +        message(\"Hard-float JVM detected\") +        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -mfloat-abi=hard\") {code}  I wish there were a way to use CHECK_C_SOURCE_COMPILES or something here to determine if the JVM library was soft-float or hard-float.  I don't know if everyone has {{readelf}} installed by default, and it's preferable to reduce the number of dependencies we have.  However, if you can't find an easy way to do this, then feel free to ignore this comment.", "It's not as easy as just CHECK_SYMBOL_EXISTS/CHECK_LIBRARY_EXISTS, since the soft-float libraries are identical to the hard-float ones, but are installed in different directories. However, I can do a test compilation against an arbitrary libc symbol with the soft-float flag:  {code} include(CMakePushCheckState) cmake_push_check_state() set(CMAKE_REQUIRED_FLAGS ${CMAKE_REQUIRED_FLAGS} -mfloat-abi=softfp) include(CheckSymbolExists) check_symbol_exists(exit stdlib.h SOFTFP_AVAILABLE) cmake_pop_check_state() {code}  Unfortunately, there is currently no good way to determine the JVM's float ABI. It's not reported at all by the Oracle EJRE or OpenJDK. The current behavior of linking against the JVM library with the wrong ABI doesn't report an error. What I can do is restrict this code path to Linux (since this issue is Linux-specific for now), where readelf is part of binutils (like ld), so it should always be available. But I'll also check for it and issue a warning if it's not found. For example:  {code} find_program(READELF readelf) if (READELF MATCHES \"NOTFOUND\")     message(WARNING \"readelf not found; JVM float ABI detection disabled\") endif () {code}", "Attached updated patch based on Colin's comments.", "bq. Unfortunately, there is currently no good way to determine the JVM's float ABI...  Yeah, I was afraid of that.  That aborting at runtime behavior is really nasty.  I'm glad your change prevents people from being exposed to that.  +1.", "+1, looks good to me too. Thanks, Trevor.", "Committed to trunk and branch-2, thanks Trevor!", "Integrated in Hadoop-Hdfs-trunk-Commit #2634 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2634/])     HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. Contributed by Trevor Robinson. (Revision 1371507)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1371507 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/CMakeLists.txt", "Integrated in Hadoop-Common-trunk-Commit #2569 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2569/])     HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. Contributed by Trevor Robinson. (Revision 1371507)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1371507 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/CMakeLists.txt", "Integrated in Hadoop-Mapreduce-trunk-Commit #2590 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2590/])     HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. Contributed by Trevor Robinson. (Revision 1371507)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1371507 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/CMakeLists.txt", "This might break the builds in Jenkins.  Please take a look. https://builds.apache.org/job/PreCommit-HDFS-Build/2988/artifact/trunk/patchprocess/patchJavacWarnings.txt", "What's the difference between https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/ and https://builds.apache.org/job/PreCommit-HDFS-Build/? The former is passing but the latter is failing. Does the former not build native libraries? Also https://builds.apache.org/job/PreCommit-HDFS-Build/2983/ included this change but appears to have built successfully. I'm baffled right now, but it's also past 3am for me.", "I think this patch will fix it... let me run it past Jenkins", "So the build issue here is basically a re-introduction of HADOOP-8489.  The issue is that when you do a 32-bit compile on a 64-bit machine, with both 32 and 64-bit JVM libraries in your path, you need to make sure you choose the 32-bit JVM libraries.  The way we do this is by setting {{CMAKE_SYSTEM_PROCESOR}}.  However, you must do this *before* {{find_package(JNI REQUIRED)}}; otherwise, the 64-bit libraries will be found and used, which results in a linker error when you try to link them with the code which was compiled with {{-m32}}.", "should add {{-m32}} to CMAKE_CXX_FLAGS as well.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540690/HADOOP-8659-fix-001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1281//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1281//console  This message is automatically generated.", "Thanks for fixing this, Colin.  bq. we do this is by setting CMAKE_SYSTEM_PROCESOR. However, you must do this before find_package(JNI REQUIRED)  So that's why CMAKE_SYSTEM_PROCESSOR was being set... This subtlety screams for a comment in the code. :-)", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540712/HADOOP-8659-fix-001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1282//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1282//console  This message is automatically generated.", "* add a comment about CMAKE_SYSTEM_PROCESSOR", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540736/HADOOP-8659-fix-002.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1285//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1285//console  This message is automatically generated.", "+1 to the latest fix. Colin, in the future please file a new jira for issues like this.", "I've committed the fix and merged to branch-2.", "Integrated in Hadoop-Mapreduce-trunk-Commit #2595 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2595/])     Amend HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. (Revision 1372583)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372583 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake", "Integrated in Hadoop-Hdfs-trunk-Commit #2639 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2639/])     Amend HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. (Revision 1372583)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372583 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake", "Integrated in Hadoop-Common-trunk-Commit #2574 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2574/])     Amend HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. (Revision 1372583)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372583 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake", "Integrated in Hadoop-Hdfs-trunk #1135 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1135/])     Amend HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. (Revision 1372583)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372583 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake", "Integrated in Hadoop-Mapreduce-trunk #1167 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1167/])     Amend HADOOP-8659. Native libraries must build with soft-float ABI for Oracle JVM on ARM. (Revision 1372583)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372583 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/JNIFlags.cmake"], "tasks": {"summarization": "Native libraries must build with soft-float ABI for Oracle JVM on ARM - There was recently an ABI (application binary interface) change in most Linux distributions for mode...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8659 about?", "answer": "Native libraries must build with soft-float ABI for Oracle JVM on ARM"}}}
{"issue_id": "HADOOP-8658", "project": "HADOOP", "title": "Add support for configuring the encryption algorithm used for Hadoop RPC", "status": "Open", "priority": "Major", "reporter": "Aaron Myers", "assignee": null, "created": "2012-08-07T20:31:28.000+0000", "updated": "2018-08-13T19:13:20.000+0000", "labels": [], "description": "HDFS-3637 recently introduced the ability to encrypt actual HDFS block data on the wire, including the ability to choose which encryption algorithm is used. It would be great if Hadoop RPC similarly had support for choosing the encryption algorithm.", "comments": ["Aron can you explain me what it actually means?Means I didnot get the meaning of \"HDFS block on wire\".", "Hi Amol, if you haven't already, you should read through the dialogue on HDFS-3637.  What I mean by \"on the wire\" is as opposed to \"at rest,\" i.e. the data is encrypted while it is being transmitted over the network between clients and DNs, but not encrypted when the data is stored on disk.  By \"block data,\" I mean as opposed to the RPC data. When security is enabled, encrypting the RPC traffic between clients and the NN, or DNs to the NN can already be encrypted. But, until HDFS-3637 was implemented, there was no support for encrypting the actual file data being read or written by clients and DNs.  The purpose of this JIRA is to add support for configuring the encryption algorithm used for encrypting RPC traffic."], "tasks": {"summarization": "Add support for configuring the encryption algorithm used for Hadoop RPC - HDFS-3637 recently introduced the ability to encrypt actual HDFS block data on the wire, including t...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8658 about?", "answer": "Add support for configuring the encryption algorithm used for Hadoop RPC"}}}
{"issue_id": "HADOOP-8657", "project": "HADOOP", "title": "TestCLI fails on Windows because it uses hardcoded file length of test files committed to the source code", "status": "Resolved", "priority": "Major", "reporter": "Bikas Saha", "assignee": "Bikas Saha", "created": "2012-08-06T23:27:16.000+0000", "updated": "2012-08-30T02:20:36.000+0000", "labels": [], "description": "The actual length of the file would depend on the character encoding used and hence cannot be hard-coded.", "comments": ["The test was failing because it was checking for file sizes on disk and the sizes were hardcoded in the test. text file sizes can be different on different platforms based on character encodings etc. The fix was to read the actual file size from disk and then check values based on that instead of some hardcoded value. The test files are actually checked into the source as resources. Ideally, the test would generate these files on the fly instead of checking them in but I am leaving that re-organization of the code tree for later when the branch is merged back so as to simplify the merge.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542699/HADOOP-8657.branch-1-win.1.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1372//console  This message is automatically generated.", "I just committed this. Thanks Bikas!"], "tasks": {"summarization": "TestCLI fails on Windows because it uses hardcoded file length of test files committed to the source code - The actual length of the file would depend on the character encoding used and hence cannot be hard-c...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8657 about?", "answer": "TestCLI fails on Windows because it uses hardcoded file length of test files committed to the source code"}}}
{"issue_id": "HADOOP-8656", "project": "HADOOP", "title": "backport forced daemon shutdown of HADOOP-8353 into branch-1", "status": "Closed", "priority": "Minor", "reporter": "Steve Loughran", "assignee": "Roman Shaposhnik", "created": "2012-08-06T18:37:24.000+0000", "updated": "2012-10-17T18:27:27.000+0000", "labels": [], "description": "the init.d service shutdown code doesn't work if the daemon is hung -backporting the portion of HADOOP-8353 that edits bin/hadoop-daemon.sh corrects this", "comments": ["If you suspend a hadoop daemon with a {{kill -STOP $pid}}, the {{service hadoop-namenode stop}} operation will not actually terminate the process -it issues a kill -TERM \"hint\".   HADOOP-8353 forces the shutdown with a sleep followed by a kill -9 if there is a process with that pid still listed. This forces the shutdown, and should have no adverse side effects provided that Pid doesn't get reused during the sleep phase.  This JIRA covers backporting it to 1.1 and trunk", "+1 voting it in myself after a week's grace.", "committing -and crediting Roman as it is his patch being backported over.", "Closed upon release of Hadoop-1.1.0."], "tasks": {"summarization": "backport forced daemon shutdown of HADOOP-8353 into branch-1 - the init.d service shutdown code doesn't work if the daemon is hung -backporting the portion of HADO...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8656 about?", "answer": "backport forced daemon shutdown of HADOOP-8353 into branch-1"}}}
{"issue_id": "HADOOP-8655", "project": "HADOOP", "title": "In TextInputFormat, while specifying textinputformat.record.delimiter the character/character sequences in data file similar to starting character/starting character sequence in delimiter were found missing in certain cases in the Map Output", "status": "Closed", "priority": "Major", "reporter": "Arun A K", "assignee": null, "created": "2012-08-06T11:01:33.000+0000", "updated": "2014-09-04T00:59:38.000+0000", "labels": ["hadoop", "mapreduce", "textinputformat", "textinputformat.record.delimiter"], "description": "Set textinputformat.record.delimiter as \"</entity>\"  Suppose the input is a text file with the following content <entity><id>1</id><name>User1</name></entity><entity><id>2</id><name>User2</name></entity><entity><id>3</id><name>User3</name></entity><entity><id>4</id><name>User4</name></entity><entity><id>5</id><name>User5</name></entity>  Mapper was expected to get value as   Value 1 - <entity><id>1</id><name>User1</name> Value 2 - <entity><id>2</id><name>User2</name> Value 3 - <entity><id>3</id><name>User3</name> Value 4 - <entity><id>4</id><name>User4</name> Value 5 - <entity><id>5</id><name>User5</name>  According to this bug Mapper gets value  Value 1 - entity><id>1</id><name>User1</name> Value 2 - <entity>id>2</id><name>User2</name> Value 3 - <entity><id>3id><name>User3</name> Value 4 - <entity><id>4</id><name>User4name> Value 5 - <entity><id>5</id><name>User5</name>  The pattern shown above need not occur for value 1,2,3 necessarily. The bug occurs at some random positions in the map input.", "comments": ["I have found a similar Bug And a fix, MAPREDUCE-4512. Please reffer the patch, and kindly encorporate the same. While fixing I too have encounted such a senario, I think this occur at the end of the buffer which would capture 4096 Charactors. My understanding is the ending and begining of next buffer can and the delimiter indexses are not properly handled. This is resulting in some or the other bugs.  Tried solving , but the fix resulted in some new bugs. The once all the senario is caught we can ensure a posible fix.", "A few lines of change in LineReader, also incorporaed the MAPREDUCE-4512 patch", "A few lines of change in LineReader, also incorporaed the MAPREDUCE-4512 patch", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539291/MAPREDUCE-4519.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2709//testReport/ Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2709//console  This message is automatically generated.", "As with MAPREDUCE-4512, I moved this to project Hadoop Common since that's where the patch needs to be applied.  In the future, please don't set the Reviewed flag unless the patch has been reviewed and approved by someone in the community. I see no record of that occurring, so I've cleared that flag. Also the Fix versions field is intended to mark where the patch has been integrated, so please don't set this field. If you'd like to indicate what versions you'd like to have the patch committed to, use the Target Versions field instead.", "Thank you Meria for including my patch as well. Thats Jason, for have a closer look, and merging it. Please guide about howt to categorize , the bug As this was a issue faced in MAP REDUCE And was supposed to be raised in HADOOP", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539291/MAPREDUCE-4519.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1253//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1253//console  This message is automatically generated.", "Finding the right JIRA project is a straightforward mapping from the top-level projects in the code base:  * Anything under hadoop-common-project maps to Hadoop Common * Anything under hadoop-hdfs-project maps to Hadoop HDFS * Anything under hadoop-mapreduce-project maps to Hadoop Map/Reduce", "The issue occurs when the buffer that reads the input file content, at a particular instance, ends with a character or character sequence that matches the head of the record delimiter.  For example, in the above case, while reading the file, the buffer's end bytes at an instance might be as follows,  ........</name></entity><entity><id>3</  causing it to skip the last two characters considering it as a part of the delimiter </entity>.  The default buffer size is 4096 bytes.Hence the input should be more than 4096 bytes and the last bytes of the buffer should match the head of the delimiter...Please guide how to create test case for the patch..", "Patch with test case. This patch holds good with the test case of HADOOP-8654.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541222/HADOOP-8654.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1318//console  This message is automatically generated.", "Comments from a quick perusal of the patch:  * Please post patches with appropriate names, this is HADOOP-8655 but the patch name implies it's for HADOOP-8654. * Patch needs to be updated to trunk. HADOOP-8654 has been committed since this patch was posted. * Patch contains tabs, please convert to spaces. * Why were the {{InterfaceAudience}} and {{InterfaceStability}} decorators removed?", "Patch is commited aganist 04ba22681a494bf718dff7926e783c75bf64c2c7 taking care of HADOOP-8654,", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541232/HADOOP-8655.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      -1 javac.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1320//console  This message is automatically generated.", "The code looks good, but it looks like you put TestLineReader.java under the main directory when it should be under the test directory.  It will not compile under main.  I also haven't had a chance to look at it in depth.", "Revised the patch as per Robert Joseph Evans comments", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541726/HADOOP-8655.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1339//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1339//console  This message is automatically generated.", "Could any body clarify about  org.apache.hadoop.ha.TestZKFailoverController Unit Test", "The TestZKFailoverController failure is unrelated, see HADOOP-8591.", "Thanks Robert Joseph Evans & Jason Lowe , for providing the info, If I am not wrong, ZKFailoverController itself has a problem , and that is being reflected here. If so, I hope this could be closed, Lets listen from Arun AK, as well, Hope his data sets would respond positevely.", "Gelesh,  The new patch looks better, but I still have a few comments.   # Please make sure you follow the style guide. It should follow [Sun's code conventions|http://java.sun.com/docs/codeconv/] except indentation is 2 spaces, not 4.  There are still tabs everywhere throughout the code and there are many lines that go over 80 characters in length.  Comments are included in the 80 character limit.  # In the test getTestData method is only called once, and is very specific to the single test method.  I would prefer to see it inlined in testCustomDeliminator.  # I appreciate that you want to explain what is happening in your code, but I don't think you need quite so many comments.  For example you don't need to reference HADOOP-8654.  There should be test cases added with HADOOP-8654 to validate that there were no regression.", "Thank you Robert Joseph Evans, This patch is updated as per your comments", "Since Hadoop_QA automated testing has not acted upon the previous patch, re uploading the same", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542076/HADOOP-8655%20%282%29.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1348//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1348//console  This message is automatically generated.", "Gelesh :I had tried out the patch that you have posted herein. That really solves my problem. Thanks a lot for the patch. Is the patch that you re uploaded same as before? Do I need to apply this new patch?", "Thanks for the patch Gelesh +1.  I checked this into trunk and branch-2.", "Integrated in Hadoop-Common-trunk-Commit #2627 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2627/])     HADOOP-8655. Fix TextInputFormat for large deliminators. (Gelesh via  bobby) (Revision 1376592)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376592 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2691 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2691/])     HADOOP-8655. Fix TextInputFormat for large deliminators. (Gelesh via  bobby) (Revision 1376592)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376592 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2655 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2655/])     HADOOP-8655. Fix TextInputFormat for large deliminators. (Gelesh via  bobby) (Revision 1376592)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376592 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Hdfs-trunk #1144 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1144/])     HADOOP-8655. Fix TextInputFormat for large deliminators. (Gelesh via  bobby) (Revision 1376592)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376592 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Mapreduce-trunk #1175 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1175/])     HADOOP-8655. Fix TextInputFormat for large deliminators. (Gelesh via  bobby) (Revision 1376592)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376592 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java"], "tasks": {"summarization": "In TextInputFormat, while specifying textinputformat.record.delimiter the character/character sequences in data file similar to starting character/starting character sequence in delimiter were found missing in certain cases in the Map Output - Set textinputformat.record.delimiter as \"</entity>\"  Suppose the input is a text file with the follo...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8655 about?", "answer": "In TextInputFormat, while specifying textinputformat.record.delimiter the character/character sequences in data file similar to starting character/starting character sequence in delimiter were found missing in certain cases in the Map Output"}}}
{"issue_id": "HADOOP-8654", "project": "HADOOP", "title": "TextInputFormat delimiter  bug:- Input Text portion ends with & Delimiter starts with same char/char sequence", "status": "Closed", "priority": "Major", "reporter": "Gelesh", "assignee": null, "created": "2012-08-03T15:56:08.000+0000", "updated": "2014-09-04T00:59:39.000+0000", "labels": ["patch"], "description": "TextInputFormat delimiter  bug scenario , a character sequence of the input text,  in which the first character matches with the first character of delimiter, and the remaining input text character sequence  matches with the entire delimiter character sequence from the  starting position of the delimiter.  eg   delimiter =\"record\"; and Text =\" record 1:- name = Gelesh e mail = gelesh.hadoop@gmail.com Location Bangalore record 2: name = sdf  ..  location =Bangalorrecord 3: name .... \"   Here string \"=Bangalorrecord 3: \" satisfy two conditions  1) contains the delimiter \"record\" 2) The character / character sequence immediately before the delimiter (ie ' r ') matches with first character (or character sequence ) of delimiter.  (ie \"=Bangalor\" ends with and Delimiter starts with same character/char sequence 'r' ),  Here the delimiter is not encountered by the program resulting in improper value text in map that contains the delimiter", "comments": ["just one line of code change @ LineReader, would do. Tested Any issues please let me know to help further gelesh.hadoop@gmail.com", "Just One line code change at LineRecord. Tested  in case there is any issue please mail me gelesh.hadoop@gmail.com", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539059/MAPREDUCE-4512.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2706//testReport/ Console output: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/2706//console  This message is automatically generated.", "Test case input file text record 1 name: Java Location:UAErecord 2 name:Gelesh Location:Bangalorrecord 3 name Hadoop Location:Kerala  Delimiter = \"record\"  expected values in map  1 name: Java Location:UAE  2 name:Gelesh Location:Bangalor  3 name Hadoop Location:Kerala  Actual values received in map  1 name: Java Location:UAE  2 name:Gelesh Location:Bangalorrecord 3 name Hadoop Location:Kerala", "I am also facing the similar issue, Please help me to re create the fixed code using patch", "Please update the patch with a Testcase.", "Moving to project Hadoop Common since that's where the patch needs to be applied.  In the future, please don't set the Reviewed flag unless the patch has been reviewed and approved by someone in the community.  I see no record of that occurring, so I've cleared that flag.  Also the Fix versions flag is intended to mark where the patch has been integrated, please don't set this field.  If you'd like to indicate what versions you'd like to have the patch committed to, use the Target Versions field.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539059/MAPREDUCE-4512.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1252//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1252//console  This message is automatically generated.", "Kindly provide the details or URL to access the details , for the failed test case org.apache.hadoop.ha.TestZKFailoverController Including src code, input supplied , expected output etc. Thank you", "Hi Gelesh,  If there is any test failure, one can access them through *Test results* URL.  bq. -1 core tests. The patch failed these unit tests in hadoop-common-project/hadoop-common:org.apache.hadoop.ha.TestZKFailoverController  The above test failure seems to be unrelated to this patch.   The patch does not contain any testcase. Please update your a patch with a testcase.", "Could you please share a Java Test file or a link to refer the same. The confusion is, this error is inPut file based, and we need to supply a error case based input. A link for the existing test case, which is as per the would help, which follows new the test case rules as per Apache-wiki", "I could write a Map Reduce, for testing with the below code in Map Reduce Driver       Path inputDirectory = new Path(\"TestDirectory\", \"input\");     Path file = new Path(inputDirectory, \"InputFile.txt\");     Writer writer = new OutputStreamWriter(localFs.create(file));     writer.write(\"The Reruired Very Big Input String\");  // Fingers crossed       Path outFile  =  new Path(outputTestDirectory, \"part-r-00000\");     Reader reader =  new InputStreamReader(localFs.open(outFile));  Is this okay ?", "I was searching for resolved issue, And for that I clicked on Resolved issue. My appologise", "Since by my mistake , I clicked on Resolved button, I have reopned the issue. To change the Status to Patch Available I am re submiting the same patch, I appologize", "bq. The confusion is, this error is inPut file based, and we need to supply a error case based input.  We don't need a full-blown MapReduce job to perform a unit test of the fix.  The issue is localized to LineReader, so let's write a unit test for that.  Rather than using a file as input, we can feed it a string of characters written into the test code directly.  I've attached an updated patch with a testcase.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541140/HADOOP-8654.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1311//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1311//console  This message is automatically generated.", "Thanks Jason Lowe , I have run the test case you have uploaded. The error and the solution holds good. Hope we can close this.", "Thanks Gelesh and Jason,  +1   I put this into trunk and branch-2", "Integrated in Hadoop-Common-trunk-Commit #2587 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2587/])     HADOOP-8654. TextInputFormat delimiter bug (Gelesh and Jason Lowe via bobby) (Revision 1373859)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1373859 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2652 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2652/])     HADOOP-8654. TextInputFormat delimiter bug (Gelesh and Jason Lowe via bobby) (Revision 1373859)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1373859 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2617 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2617/])     HADOOP-8654. TextInputFormat delimiter bug (Gelesh and Jason Lowe via bobby) (Revision 1373859)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1373859 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Hdfs-trunk #1137 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1137/])     HADOOP-8654. TextInputFormat delimiter bug (Gelesh and Jason Lowe via bobby) (Revision 1373859)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1373859 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Integrated in Hadoop-Mapreduce-trunk #1169 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1169/])     HADOOP-8654. TextInputFormat delimiter bug (Gelesh and Jason Lowe via bobby) (Revision 1373859)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1373859 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LineReader.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLineReader.java", "Thanks Arun..."], "tasks": {"summarization": "TextInputFormat delimiter  bug:- Input Text portion ends with & Delimiter starts with same char/char sequence - TextInputFormat delimiter  bug scenario , a character sequence of the input text,  in which the firs...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8654 about?", "answer": "TextInputFormat delimiter  bug:- Input Text portion ends with & Delimiter starts with same char/char sequence"}}}
{"issue_id": "HADOOP-8653", "project": "HADOOP", "title": "FTPFileSystem rename broken", "status": "Patch Available", "priority": "Major", "reporter": "Karel Kolman", "assignee": null, "created": "2012-08-06T11:08:28.000+0000", "updated": "2015-05-06T03:33:41.000+0000", "labels": ["BB2015-05-TBR"], "description": "The FTPFileSystem.rename(FTPClient client, Path src, Path dst) method is broken.  The changeWorkingDirectory command underneath is being passed a string with file:// uri prefix (which FTP server does not understand obviously)  {noformat}  INFO [2012-08-06 12:59:39] (DefaultSession.java:297) - Received command: [CWD ftp://localhost:61246/tmp/myfile]  WARN [2012-08-06 12:59:39] (AbstractFakeCommandHandler.java:213) - Error handling command: Command[CWD:[ftp://localhost:61246/tmp/myfile]]; org.mockftpserver.fake.filesystem.FileSystemException: /ftp://localhost:61246/tmp/myfile org.mockftpserver.fake.filesystem.FileSystemException: /ftp://localhost:61246/tmp/myfile \tat org.mockftpserver.fake.command.AbstractFakeCommandHandler.verifyFileSystemCondition(AbstractFakeCommandHandler.java:264) \tat org.mockftpserver.fake.command.CwdCommandHandler.handle(CwdCommandHandler.java:44) \tat org.mockftpserver.fake.command.AbstractFakeCommandHandler.handleCommand(AbstractFakeCommandHandler.java:76) \tat org.mockftpserver.core.session.DefaultSession.readAndProcessCommand(DefaultSession.java:421) \tat org.mockftpserver.core.session.DefaultSession.run(DefaultSession.java:384) \tat java.lang.Thread.run(Thread.java:680)  {noformat}  The solution would be this:  {noformat} --- a/FTPFileSystem.java +++ b/FTPFileSystem.java @@ -549,15 +549,15 @@ public class FTPFileSystem extends FileSystem {        throw new IOException(\"Destination path \" + dst            + \" already exist, cannot rename!\");      } -    String parentSrc = absoluteSrc.getParent().toUri().toString(); -    String parentDst = absoluteDst.getParent().toUri().toString(); +    URI parentSrc = absoluteSrc.getParent().toUri(); +    URI parentDst = absoluteDst.getParent().toUri();      String from = src.getName();      String to = dst.getName(); -    if (!parentSrc.equals(parentDst)) { +    if (!parentSrc.toString().equals(parentDst.toString())) {        throw new IOException(\"Cannot rename parent(source): \" + parentSrc            + \", parent(destination):  \" + parentDst);      } -    client.changeWorkingDirectory(parentSrc); +    client.changeWorkingDirectory(parentSrc.getPath().toString());      boolean renamed = client.rename(from, to);      return renamed;    }  {noformat}", "comments": ["You mention that file:// is being added to the path, but in the stack trace it appears the problem is {{ftp://localhost:61246/tmp/myfile}} is having a leading slash prepended because that's probably the cwd of the server.  Thus the real fix in your patch is calling {{path.getPath}} to remove the scheme+authority.  I may suggest implementing {{setWorkingDirectory(Path)}} to pass only {{path#getPath()}} through to the server, although I question why many of the methods are implicitly changing the cwd?", "@Daryn i don't think you are right here, the first log line from ftp server reads:  {noformat}  Received command: [CWD ftp://localhost:61246/tmp/myfile] {noformat}  while it should receive only [CWD /tmp/myfile] from hadoop's FTPFileSystem  Also all other calls to changeWorkingDirectory in FTPFileSystem use a variant of parent.toUri().getPath() (parent.toUri().getPath().toString()) {noformat} FTPFileSystem.java-6393-    // FSDataInputStream. FTPFileSystem.java:6419:    client.changeWorkingDirectory(parent.toUri().getPath()); -- FTPFileSystem.java-8320-    // FSDataOutputStream. FTPFileSystem.java:8347:    client.changeWorkingDirectory(parent.toUri().getPath()); -- FTPFileSystem.java-17201-        String parentDir = parent.toUri().getPath(); FTPFileSystem.java:17254:        client.changeWorkingDirectory(parentDir); -- FTPFileSystem.java-19688-    } FTPFileSystem.java:19694:    client.changeWorkingDirectory(parentSrc); {noformat}  so i think it should be patched like this  I'm sorry im not providing a unit/integration testcase, is there one ? Basically this was caught by our integration tests trying to use FTPFileSystem for some file renaming.", "My apologies if I was confusing.  We're saying the same thing - the fully qualified path, not the fully qualified uri, should be passed.  I suggested a common method since many places seem to do the same logic to invoke with just the path.  Note this may just be a test case bug:  the next line after the CWD you cite is {noformat}org.mockftpserver.fake.filesystem.FileSystemException: /ftp://...{noformat} which means it may just be the mock prepending the /?", "the \"/\" is the ftp user's home directory, which is set to / in the mock server setup. Changing the home to \"/home/test/\", the next error line reads {noformat} org.mockftpserver.fake.filesystem.FileSystemException: /home/test/ftp://localhost:61246/tmp/myfile {noformat} /home/test/ftp://localhost:61246/tmp/myfile  I'm not so sure it makes sense to create an additional method for this, changeWorkingDirectory(String) method is a public one and I don't really have a clue about hadoop's file systems, so all that Path to URI to Path conversion happening in this close is a mystery to me.  The patch had a unneeded toString() at parentSrc.getPath().toString() {noformat} -    client.changeWorkingDirectory(parentSrc); +    client.changeWorkingDirectory(parentSrc.getPath()); {noformat}", "Ok, that's what I thought.  Does the test fail if run against a live {{FTPFileSystem}}?  It looks like it's maybe a mock problem, although the patch is an improvement.  BTW, you need to attach patches as a file to the jira and submit them for the pre-commit build to test it.  Related, but perhaps for another jira: you may consider having {{makeAbsolute(Path)}} return {{new Path(null, null, makeQualified(path).getPath())}}.  That will return just the absolute path of the URI, which will allow the code in rename and many other methods to be simplified.", "patch making the argument to changeWorkingDirectory() in rename() correct - without ftp:// scheme prefix", "well testing on live ftp, the \"CWD file://path\" is still being sent to it (and of course failing on the ftp server side), no mock problem, ftp server commands have no clue about schemes...    attaching the patch against trunk", "patch for FTPFileSystem rename method fix", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539796/HDFS-8653-1.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1264//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1264//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539796/HDFS-8653-1.patch   against trunk revision 14e2639.      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4717//console  This message is automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12539796/HDFS-8653-1.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6360/console |   This message was automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12539796/HDFS-8653-1.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6363/console |   This message was automatically generated."], "tasks": {"summarization": "FTPFileSystem rename broken - The FTPFileSystem.rename(FTPClient client, Path src, Path dst) method is broken.  The changeWorkingD...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8653 about?", "answer": "FTPFileSystem rename broken"}}}
{"issue_id": "HADOOP-8652", "project": "HADOOP", "title": "Change website to reflect new user@hadoop.apache.org mailing list", "status": "Resolved", "priority": "Major", "reporter": "Arun Murthy", "assignee": "Arun Murthy", "created": "2012-08-05T04:06:31.000+0000", "updated": "2016-09-02T15:45:07.000+0000", "labels": [], "description": "Change website to reflect new user@hadoop.apache.org mailing list since we've merged the user lists per discussion on general@: http://s.apache.org/hv", "comments": ["Straight-fwd changes to the site.", "Looks good.", "Thanks for the review Das, I've just committed this.", "Doug, since you are on the watch list for the jira, I'm having trouble updating the site after the commit:  {noformat} acmurthy@minotaur:~$ cd /www/hadoop.apache.org/ acmurthy@minotaur:/www/hadoop.apache.org$ svn up Updating '.': svn: E000013: Can't open file '/x1/www/hadoop.apache.org/.svn/pristine/51/51ebac8c438965bd0e9c2d54552ac4878b74575d.svn-base': Permission denied acmurthy@minotaur:/www/hadoop.apache.org$ ls -ld /x1/www/hadoop.apache.org/.svn drwxrwsr-x  4 cutting  hadoop  7 Aug  8 01:26 /x1/www/hadoop.apache.org/.svn acmurthy@minotaur:/www/hadoop.apache.org$ date Wed Aug  8 01:26:39 UTC 2012 {noformat}  Any idea? I've seen this behaviour for over 24 hrs now. Could you pls look? Thanks.", "'svn up' works without complaint for me.  Is it still failing for you?  The protections look right to me: the group is 'hadoop' and all directories are group writable."], "tasks": {"summarization": "Change website to reflect new user@hadoop.apache.org mailing list - Change website to reflect new user@hadoop.apache.org mailing list since we've merged the user lists ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8652 about?", "answer": "Change website to reflect new user@hadoop.apache.org mailing list"}}}
{"issue_id": "HADOOP-8651", "project": "HADOOP", "title": "Error reading task output Server returned HTTP response code: 400 for URL: http://hadoop03:8080/tasklog?plaintext=true&attemptid=attempt_1344047400780_0002_m_000000_0&filter=stdout", "status": "Resolved", "priority": "Major", "reporter": "jiafeng.zhang", "assignee": null, "created": "2012-08-04T03:22:05.000+0000", "updated": "2012-08-04T03:36:50.000+0000", "labels": [], "description": "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-0.23.1.jar teragen 1000000 /in_test 12/08/04 11:01:47 WARN conf.Configuration: fs.default.name is deprecated. Instead, use fs.defaultFS 12/08/04 11:01:47 WARN conf.Configuration: mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used 12/08/04 11:01:49 INFO terasort.TeraSort: Generating 1000000 using 2 12/08/04 11:01:50 INFO mapreduce.JobSubmitter: number of splits:2 12/08/04 11:01:52 INFO mapred.ResourceMgrDelegate: Submitted application application_1344047400780_0002 to ResourceManager at hadoop01/192.168.37.101:8032 12/08/04 11:01:52 INFO mapreduce.Job: The url to track the job: http://hadoop01:50030/proxy/application_1344047400780_0002/ 12/08/04 11:01:52 INFO mapreduce.Job: Running job: job_1344047400780_0002 12/08/04 11:02:11 INFO mapreduce.Job: Job job_1344047400780_0002 running in uber mode : false 12/08/04 11:02:11 INFO mapreduce.Job:  map 0% reduce 0% 12/08/04 11:02:19 INFO mapreduce.Job: Task Id : attempt_1344047400780_0002_m_000000_0, Status : FAILED 12/08/04 11:02:20 WARN mapreduce.Job: Error reading task output Server returned HTTP response code: 400 for URL: http://hadoop03:8080/tasklog?plaintext=true&attemptid=attempt_1344047400780_0002_m_000000_0&filter=stdout 12/08/04 11:02:20 WARN mapreduce.Job: Error reading task output Server returned HTTP response code: 400 for URL: http://hadoop03:8080/tasklog?plaintext=true&attemptid=attempt_1344047400780_0002_m_000000_0&filter=stderr 12/08/04 11:02:25 INFO mapreduce.Job:  map 9% reduce 0% 12/08/04 11:02:30 INFO mapreduce.Job:  map 13% reduce 0% 12/08/04 11:02:33 INFO mapreduce.Job:  map 15% reduce 0% 12/08/04 11:02:40 INFO mapreduce.Job:  map 17% reduce 0% 12/08/04 11:02:46 INFO mapreduce.Job:  map 18% reduce 0% 12/08/04 11:02:52 INFO mapreduce.Job:  map 25% reduce 0% 12/08/04 11:02:56 INFO mapreduce.Job:  map 29% reduce 0% 12/08/04 11:03:01 INFO mapreduce.Job:  map 31% reduce 0% 12/08/04 11:03:08 INFO mapreduce.Job:  map 34% reduce 0% 12/08/04 11:03:11 INFO mapreduce.Job:  map 38% reduce 0% 12/08/04 11:03:14 INFO mapreduce.Job:  map 42% reduce 0% 12/08/04 11:03:15 INFO mapreduce.Job:  map 46% reduce 0% 12/08/04 11:03:17 INFO mapreduce.Job:  map 51% reduce 0% 12/08/04 11:03:18 INFO mapreduce.Job:  map 55% reduce 0% 12/08/04 11:03:20 INFO mapreduce.Job:  map 56% reduce 0% 12/08/04 11:03:24 INFO mapreduce.Job:  map 58% reduce 0% 12/08/04 11:03:25 INFO mapreduce.Job:  map 59% reduce 0% 12/08/04 11:03:26 INFO mapreduce.Job:  map 62% reduce 0% 12/08/04 11:03:28 INFO mapreduce.Job:  map 67% reduce 0% 12/08/04 11:03:29 INFO mapreduce.Job:  map 71% reduce 0% 12/08/04 11:03:32 INFO mapreduce.Job:  map 73% reduce 0% 12/08/04 11:03:33 INFO mapreduce.Job:  map 74% reduce 0% 12/08/04 11:03:35 INFO mapreduce.Job:  map 76% reduce 0% 12/08/04 11:03:36 INFO mapreduce.Job:  map 78% reduce 0% 12/08/04 11:03:38 INFO mapreduce.Job:  map 79% reduce 0% 12/08/04 11:03:39 INFO mapreduce.Job:  map 81% reduce 0% 12/08/04 11:03:41 INFO mapreduce.Job:  map 84% reduce 0% 12/08/04 11:03:44 INFO mapreduce.Job:  map 87% reduce 0% 12/08/04 11:03:48 INFO mapreduce.Job:  map 90% reduce 0% 12/08/04 11:03:51 INFO mapreduce.Job:  map 100% reduce 0% 12/08/04 11:03:52 INFO mapreduce.Job: Job job_1344047400780_0002 completed successfully 12/08/04 11:03:52 INFO mapreduce.Job: Counters: 28         File System Counters                 FILE: Number of bytes read=240                 FILE: Number of bytes written=118412                 FILE: Number of read operations=0                 FILE: Number of large read operations=0                 FILE: Number of write operations=0                 HDFS: Number of bytes read=167                 HDFS: Number of bytes written=100000000                 HDFS: Number of read operations=8                 HDFS: Number of large read operations=0                 HDFS: Number of write operations=4         Job Counters                  Failed map tasks=1                 Launched map tasks=3                 Other local map tasks=3                 Total time spent by all maps in occupied slots (ms)=193607         Map-Reduce Framework                 Map input records=1000000                 Map output records=1000000                 Input split bytes=167                 Spilled Records=0                 Failed Shuffles=0                 Merged Map outputs=0                 GC time elapsed (ms)=34470                 CPU time spent (ms)=9510                 Physical memory (bytes) snapshot=224739328                 Virtual memory (bytes) snapshot=3888279552                 Total committed heap usage (bytes)=63832064         org.apache.hadoop.examples.terasort.TeraGen$Counters                 CHECKSUM=2148987642402270         File Input Format Counters                  Bytes Read=0         File Output Format Counters                  Bytes Written=100000000", "comments": [], "tasks": {"summarization": " Error reading task output Server returned HTTP response code: 400 for URL: http://hadoop03:8080/tasklog?plaintext=true&attemptid=attempt_1344047400780_0002_m_000000_0&filter=stdout - bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-0.23.1.jar teragen 1000000 /in_test ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8651 about?", "answer": "Error reading task output Server returned HTTP response code: 400 for URL: http://hadoop03:8080/tasklog?plaintext=true&attemptid=attempt_1344047400780_0002_m_000000_0&filter=stdout"}}}
{"issue_id": "HADOOP-8650", "project": "HADOOP", "title": "/bin/hadoop-daemon.sh to add \"-f <timeout>\" arg for forced shutdowns", "status": "Resolved", "priority": "Minor", "reporter": "Steve Loughran", "assignee": null, "created": "2012-08-03T21:52:46.000+0000", "updated": "2015-02-08T18:07:49.000+0000", "labels": [], "description": "Add a timeout for the daemon script to trigger a kill -9 if the clean shutdown fails.", "comments": ["in HA environments, and other situations, you may want to forcibly shut down a hadoop service -even if it is hung. Currently, hadoop-daemon.sh sends a normal SIGTERM signal -one that the process picks up and reacts to.   If the process is completely hung, it is possible that this signal is not acted on, so it stays up. The only way to deal with this is by waiting a while, finding the pid and kill -9'ing it. This must be done by hand, or in an external script. The latter is brittle to changes in HADOOP_PID_DIR values, and requires everyone writing such scripts to code and test it themselves.  To replicate this:   # start a daemon: {{hadoop-daemon.sh start namenode}}  # issue a {{kill -STOP <pid>}} to it's PID  # try to stop the daemon via the {{hadoop-daemon.sh stop namenode}} command.  # observe that the NN process remains present.  We could extend hadoop-daemon to support a \"-f timeout\" argument, which provides a timeout after which the process must be terminated, else a kill -9 signal is issued.", "In hadoop-daemon.sh already script is there to abruptlty kill the process after HADOOP_STOP_TIMEOUT {code}(stop)      if [ -f $pid ]; then       TARGET_PID=`cat $pid`       if kill -0 $TARGET_PID > /dev/null 2>&1; then         echo stopping $command         kill $TARGET_PID         sleep $HADOOP_STOP_TIMEOUT         if kill -0 $TARGET_PID > /dev/null 2>&1; then           echo \"$command did not stop gracefully after $HADOOP_STOP_TIMEOUT seconds: killing with kill -9\"           kill -9 $TARGET_PID         fi       else         echo no $command to stop       fi     else       echo no $command to stop     fi     ;;{code}  may be we can improve as follows 1. Consider the timeout specified by the *-f <timeout>* option 2. Instead of sleeping for timeout, periodically check for process status in a loop till timeout, after that issue *kill -9*", "Well spotted, that is in trunk from HADOOP-8353; backporting that to branch-1 could be a first step. adding the sleep & poll is feature creep, but one that would deliver a faster shutdown", "Fixed in trunk, 1.x is dead. 2.x might as well be."], "tasks": {"summarization": "/bin/hadoop-daemon.sh to add \"-f <timeout>\" arg for forced shutdowns  - Add a timeout for the daemon script to trigger a kill -9 if the clean shutdown fails....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8650 about?", "answer": "/bin/hadoop-daemon.sh to add \"-f <timeout>\" arg for forced shutdowns"}}}
{"issue_id": "HADOOP-8649", "project": "HADOOP", "title": "ChecksumFileSystem should have an overriding implementation of listStatus(Path, PathFilter) for improved performance", "status": "Resolved", "priority": "Major", "reporter": "Karthik Kambatla", "assignee": "Karthik Kambatla", "created": "2012-08-03T21:31:22.000+0000", "updated": "2014-11-03T18:34:09.000+0000", "labels": [], "description": "Currently, ChecksumFileSystem implements only listStatus(Path).   The other form of listStatus(Path, customFilter) results in parsing the list twice to apply each of the filters - custom and checksum filter.  By using a composite filter instead, we limit the parsing to once.", "comments": ["Uploading patch from branch-1. The patch: - implements ChecksumFileSystem#listStatus(Path, PathFilter) - adds test for listStatus in TestChecksumFileSystem - cleans up Test file to use junit4.", "Good catch!  In {{ChecksumFileSystem#listStatus(Path, PathFilter)}}: # I question the null check, although useful, because {{FileSystems}} doesn't allow a null filter.  This would make a {{FilterFileSystem}} behave differently.  {{FileSystem}} itself should probably be changed to ignore a null since it appears to cause a NPE. # The invocation order of the filters should be flipped from (user, default) to (default, user) to prevent a user supplied filter ever seeing a checksum file.  I've seen user cases where the filter takes action on a matching path to avoid waiting for the entire listing to return.", "Thanks for the review, Daryn. Great points, I have missed them.  bq. I question the null check, although useful, because FileSystems doesn't allow a null filter. This would make a FilterFileSystem behave differently. FileSystem itself should probably be changed to ignore a null since it appears to cause a NPE.  Agree. FileSystem should check for null. However, I believe ChecksumFileSystem should also check for a null, otherwise the joinFilter would be non-null and have a constituent null filter.  Fixed the invocation order - will upload the new patch soon.", "Yes, we're in agreement.  I intended to convey that a null should be handled in both {{FileSystem}} and {{ChecksumFileSystem}}, or neither.", "Updated patch: - Fixes invocation order in joinFilter - FileSystem#listStatus() checks for null PathFilter - TestFileSystem has a new test for the same.  New test passes, but another test - TestFileSystem#testFS - fails. Unable to find why.", "Wrong placement of null check in FileSystem. Will fix it, the test and update the patch.", "Updated patch to fix wrong placement of null check in FileSystem.  TestFileSystem#writeTest still fails.", "Turns out ChecksumFileSystem#listStatus() was buggy and was causing TestFileSystem to fail. Updated the patch accordingly.  - Fixed ChecksumFileSystem#listStatus() - TestFileSystem and TestChecksumFileSystem pass just fine.", "If someone can review the patch and give a +1 for the branch-1 patch, I will replicate the changes on trunk and upload corresponding patch.  Thanks.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539336/HADOOP-8649_branch1.patch_v3   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1254//console  This message is automatically generated.", "I just eyeballed the fs classes on trunk.  The {{listStatus}} variants all depend on the {{listStatus(Path)}} primitive.  {{ChecksumFileSystem}} is overriding this to filter out crcs. {{FileSystem#listStatus(Path, PathFilter)}} calls {{CheckSumFileSystem#listStatus(Path)}} and then applies the user given filter.  It looks like it should already work.  Where exactly is the bug and is it only in branch-1?", "Hi Daryn,  The trunk code seems to be the roughly the same as branch-1. So, I believe both have the bug.  Let me illustrate the issue that I see through some psuedo-code. Let me know if I am missing something here.  {code} DistributedFileSystem dfs = ... // some initialization of distributed file system. ChecksumFileSystem cfs = new ChecksumFileSystem(dfs); // cfs.fs is set to dfs.  Path randomPath = new Path(\"random-path\"); // some path with 'random' in it.  PathFilter randomFilter = new PathFilter() {    boolean accept(Path file) {return !file.toString().contains(\"random\");}    };  FileStatus[] listWithoutFilter = cfs.listStatus(randomPath); // in turn calls dfs.listStatus(randomPath, ChecksumFileSystem.DEFAULT_FILTER) FileStatus[] listWithFilter = cfs.listStatus(randomPath, randomFilter); // in turn calls dfs.listStatus(randomPath, randomFilter) {code}  dfs.listStatus(Path, PathFilter) calls FileSystem.listStatus(Path, PathFilter), which first calls dfs.listStatus(path) and then applies PathFilter. Hence, while checksum filter is used in the first cfs.listStatus, it is not used in the second call to listStatus().", "What I _think_ I see in trunk is: # (A) {{ChecksumFileSystem#listStatus(Path, PathFilter)}} calls (B) {{ChecksumFileSystem#listStatus(Path)}} # (B) {{ChecksumFileSystem#listStatus(Path)}} calls (C) {{fs.listStatus(Path, ChecksumFileSystem.DEFAULT_FILTER)}} to filter out crcs # (A) {{ChecksumFileSystem#listStatus(Path, PathFilter)}} further filters the crc filtered results with the custom {{PathFilter}}  Do your test cases show this analysis is wrong?  Or did you notice it through casual observation of the code?  Perhaps a composite {{PathFilter}} is more efficient on large directory listings, but I'm curious if there's actually a bug.", "Hi Daryn, thanks for your comments. I noticed it through casual observation. Let me put together a test case to test/explain this perceived bug better. Will update my patch soon with the new test case.", "Sorry for the false alarm. As per Daryn's suggestion, I wrote a test to check the same that I am uploading here.  Daryn's description of the flow is right, and there is no bug. Sorry again.  Also, as Daryn commented, using a composite fiter would improve the performance.  I ll update the description of the JIRA to reflect the same and upload patches for branch-1 and test including this test.  Thanks again for your thorough review, Daryn.", "Uploading patches for trunk and branch-1 addressing Daryn's comments.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539893/trunk-HADOOP-8649.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 3 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      -1 javadoc.  The javadoc tool appears to have generated 1 warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:                    org.apache.hadoop.fs.TestLocalFSFileContextMainOperations                   org.apache.hadoop.fs.TestFileContextDeleteOnExit                   org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem                   org.apache.hadoop.hdfs.web.TestWebHDFS                   org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1266//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1266//console  This message is automatically generated.", "Found the javadoc warning. The noticed test failures seem to be due to one of the patch's tests creating a file and not deleting it. Running all the tests locally to make sure these issues are fixed. Will upload updated patch soon.", "Uploading updated patches for branch1 and trunk.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540032/trunk-HADOOP-8649.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 3 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient:                    org.apache.hadoop.hdfs.TestFileConcurrentReader                   org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1272//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1272//console  This message is automatically generated.", "I don't think the patch has anything to do with the two failing tests, these tests fail on the latest trunk as well. Casual code examination shows no intersection between the patch and failing tests.", "You may want to test if there's any incompatibilities with the chrooted filesystem.  If so, I wonder if it would be better as in more generalized, to push the change down into {{FilterFileSystem}} or {{FileSystem}} itself.  Haven't thought it all the way through, but a compound filter may use an array and each filesystem is given the opportunity to add additional filters.  If there's no problem with chroot, and you feel that's too much work, perhaps it could be something for another jira.", "Thanks for the review, Daryn.  - I don't think it is incompatible with ChRootedFileSystem as it does not filter out any files. - +1 on generalizing and pushing the change down to FileSystem itself. -- We can add {{protected/public FileSystem#listStatus(Path f, List<PathFilter> filters)}} and use {{MultiPathFilter}} as in {{o.a.h.m.FileInputFormat}} -- All FileSystems can use this to build a list of {{PathFilter}}s to be evaluated. -- {{o.a.h.m.FileInputFormat}} can use the common version of {{MultiPathFilter}}  If we decide on this, I can go ahead and make the required changes.", "I'm just generally concerned about the implications of stacking filesystems.  Ie. a {{FilterFileSystem}} over a {{ChRootedFileSystem}} over a {{FilterFileSystem}}, etc.  I'm not sure it's a problem, but you should make sure there are tests that prove the stacking works.  I conceptually like the approach suggested.  Throw something up and let's see how it looks!", "It has been close to 2 years since any activity. Closing this as \"Won't Fix\" for inactivity. Can re-open or create new one if needed."], "tasks": {"summarization": "ChecksumFileSystem should have an overriding implementation of listStatus(Path, PathFilter) for improved performance - Currently, ChecksumFileSystem implements only listStatus(Path).   The other form of listStatus(Path,...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8649 about?", "answer": "ChecksumFileSystem should have an overriding implementation of listStatus(Path, PathFilter) for improved performance"}}}
{"issue_id": "HADOOP-8648", "project": "HADOOP", "title": "libhadoop:  native CRC32 validation crashes when io.bytes.per.checksum=1", "status": "Closed", "priority": "Major", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-08-03T19:41:40.000+0000", "updated": "2012-10-11T17:45:04.000+0000", "labels": [], "description": "The native CRC32 code, found in {{pipelined_crc32c}}, crashes when chunksize is set to 1.  {code} 12:27:14,886  INFO NativeCodeLoader:50 - Loaded the native-hadoop library # # A fatal error has been detected by the Java Runtime Environment: # #  SIGSEGV (0xb) at pc=0x00007fa00ee5a340, pid=24100, tid=140326058854144 # # JRE version: 6.0_29-b11 # Java VM: Java HotSpot(TM) 64-Bit Server VM (20.4-b02 mixed mode linux-amd64 compressed oops) # Problematic frame: # C  [libhadoop.so.1.0.0+0x8340]  pipelined_crc32c+0xa0 # # An error report file with more information is saved as: # /h/hs_err_pid24100.log # # If you would like to submit a bug report, please visit: #   http://java.sun.com/webapps/bugreport/crash.jsp # Aborted {code}  The Java CRC code works fine in this case.  Choosing blocksize=1 is a __very__ odd choice.  It means that we're storing a 4-byte checksum for every byte.  {code} -rw-r--r--  1 cmccabe users  49398 Aug  3 11:33 blk_4702510289566780538 -rw-r--r--  1 cmccabe users 197599 Aug  3 11:33 blk_4702510289566780538_1199.meta {code}  However, obviously crashing is never the right thing to do.", "comments": ["you mean checksum chunk size, right? not blocksize.", "yeah, I meant to write chunk size (io.bytes.per.checksum).", "note: I tested this patch with libhadoop.so copied to hadoop-hdfs-project/hadoop-hdfs/target/native/target/usr/local/lib/ to work around bug HDFS-3753", "In the test, re-create the dfsclient each time, to make sure the new settings take effect.  The value of DFS_BYTES_PER_CHECKSUM_KEY is cached in DfsClient.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539101/HADOOP-8648.002.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:                    org.apache.hadoop.util.TestDataChecksum                   org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1248//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1248//console  This message is automatically generated.", "{code} +  if (unlikely((uintptr_t)(sums_addr + sums_offset) & 0x3)) { +    char buf[256]; +    snprintf(buf, sizeof(buf), \"sums_addr + sums_offset must be aligned \" +             \"to a 4-byte boundary!  sums_addr = %p, sums_offset = %d.\", +             sums_addr, sums_offset); +    THROW(env, \"java/lang/IllegalArgumentException\", buf); +    return; +  } {code}  Is that true? I don't think we always fit this requirement. I thought the native code worked even with unaligned pointers.  ----  - Can you add a direct unit test to TestDataChecksum for these cases? I don't think testing this from HDFS is necessarily the best way when you could trigger the condition explicitly in the checksum code.  I'm also a little confused by the changes in the C code. eg: {code} - *   block_size : The size of each block in bytes. + *   block_size : The size of each block in bytes.  Must be >= 8 {code}  Why is that the case? It seems like the code path should still work fine there, just setting counter = 0 and skipping the initial while loop.", "* pipelined_crc32c: fix bug in inline assembly for \"remainder\" bytes  * add test_bulk_crc32 unit test  * bulk_crc32.c: use constant rather than function to initialize crc32  * add bulk_calculate_crc function for unit testing purposes  * always initialize crc1, crc2, crc3 (not a bug, but using uninitialized values is confusing)", "bq. Is that true? I don't think we always fit this requirement. I thought the native code worked even with unaligned pointers.  I checked out the Intel reference manual, and you're correct.  Also, it seems that unaligned instructions have a smaller penalty on the new i7 CPUs, according to reports online.  So I got rid of the alignment check.  The new patch has a different approach which fixes the inline assembly rather than disallowing it when there were remainder bytes.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540604/HADOOP-8648.003.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1280//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1280//console  This message is automatically generated.", "The bug in the inline assembly is that there are extra parameters.  You can see this clearly in the patch: {code} @@ -433,7 +456,7 @@ static void pipelined_crc32c(uint32_t *crc1, uint32_t *crc2, uint32_t *crc3, con          \"crc32b (%5), %0;\\n\\t\"          \"crc32b (%5,%4,1), %1;\\n\\t\"           : \"=r\"(c1), \"=r\"(c2)  -         : \"r\"(c1), \"r\"(c2), \"r\"(c3), \"r\"(block_size), \"r\"(bdata) +         : \"r\"(c1), \"r\"(c2), \"r\"(block_size), \"r\"(bdata)          );          bdata++;          remainder--; {code}  You can see that it doesn't make sense for the assembly to have 7 parameters, because only 6 are actually used.  And indeed, the fact that 'c3' is inserted in the parameter list is the bug.  Another thing to keep in mind is that c3 is actually uninitialized at this point-- another clue that we should not be using it.  Incidentally, this patch unconditionally initializes c3 to 0xffffffff just to avoid heisenbugs in the future.", "I should say \"c3 is *possibly* uninitialized at this point\".", "I didn't understand why this code was wrong before, so I looked into it in more depth and I agree with Colin's analysis and patch.  In the interest of making this easier for others to understand, here are a few references.  http://www.ibiblio.org/gferg/ldp/GCC-Inline-Assembly-HOWTO.html explains the GCC inline assembly syntax, and in particular how the {{asm(\"some assembly\" : inputconstraints : outputconstraints : clobbers)}} syntax is parsed, and how the constraints map to the {{%n}} in the assembly string.  http://asm.sourceforge.net/articles/rmiyagi-inline-asm.txt describes the x86 indexed addressing modes, in particular explaining how {{(%5,%4,1)}} is interpreted as \"the word of memory at {{%5 + 1 * %4}}\".  http://softwarecommunity.intel.com/userfiles/en-us/d9156103.pdf describes the details of the SSE4 CRC32 instruction in mind-numbing detail, but that's not especially relevant to this bug.  All we need to know is that {{crc32}}_size_ operates on 8, 32, or 64 bits depending on _size_, and its first argument is read-only while its second argument is used as an accumulator (read, modify, write).  Finally, the comments in bulk_crc32.c are very helpful.  Critically, the {{pipelined_crc32c}} routine optimizes by computing the CRC of up to 3 blocks in parallel.  The block size is passed in to {{pipelined_crc32c}} as {{block_size}}.  As we can see by looking at one of the other asm blocks in pipelined_crc32c, the core idea is that we maintain {{bdata}} as a pointer to the word being CRCed in the first block, and then use indexed addressing to compute the appropriate address for the word being CRCed in the second (and possibly third) blocks.  With all that under our belt, the bug in this code becomes clear: {code}         \"crc32b (%5), %0;\\n\\t\"         \"crc32b (%5,%4,1), %1;\\n\\t\"          : \"=r\"(c1), \"=r\"(c2)          : \"r\"(c1), \"r\"(c2), \"r\"(c3), \"r\"(block_size), \"r\"(bdata) {code} The first crc32b instruction dereferences %5 which is {{block_size}}, but comparing to any other example of the similar asm block such as: {code}         \"crc32q (%7), %0;\\n\\t\"         \"crc32q (%7,%6,1), %1;\\n\\t\"         \"crc32q (%7,%6,2), %2;\\n\\t\"          : \"=r\"(c1), \"=r\"(c2), \"=r\"(c3)          : \"r\"(c1), \"r\"(c2), \"r\"(c3), \"r\"(block_size), \"r\"(data) {code} it should be dereferencing {{bdata}}.  And this is caused because the output constraints list includes {{c3}} even though the input constraints list does not, also different from all other examples of the asm block.  Therefore, Colin's fix to remove c3 from the list causes the %4 and %5 references to refer to their intended operands {{block_size}} and {{bdata}} respectively.", "I've reviewed the patch closely and agree that it's right.  The only tiny improvement I'd make is to fix this misleading comment in the 64-bit version of {{bulk_crc32.c}}: {code} 374   int remainder = block_size % sizeof(uint64_t); ... 401       /* Take care of the remainder. They are only up to three bytes, 402        * so performing byte-level crc32 won't take much time. 403        */ 404       bdata = (uint8_t*)data; 405       while (likely(remainder)) { {code} The comment says \"up to three bytes\" but since this is a uint64_t at a time, it should say \"up to seven bytes\".  This came from a copy-and-paste from the 32-bit version.  Ideally we could refactor the 32-bit and 64-bit versions to one using a {{#define WORD_T uint32_t}} or similar, but let's do that in a followup jira.  I've also reviewed the original patch in HADOOP-7446 and confirmed that there weren't any other similar bugs added.  Note that the erroneous asm is only hit if the HDFS blocksize is not a multiple of the wordsize, which AFAICS can only happen for blocksize<7.  And, the bug lurked because the remainder codepath didn't have any tests, so thanks for adding those.  Overall, LGTM.  Fix the comment if you choose, else ship it.", "* fix comment", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12543386/HADOOP-8648.004.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1396//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1396//console  This message is automatically generated.", "The new native_tests execution should be in the \"native\" profile not the \"ApacheDS KDC server\" profile. Otherwise looks great.  Excellent work fixing and thanks Andy for the detailed explanation.", "* add test_bulk_crc32 execution to the native profile, not the kerberos profile.  (Good catch, Eli.)", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12543909/HADOOP-8648.005.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1408//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1408//console  This message is automatically generated.", "+1 lgtm", "Integrated in Hadoop-Common-trunk-Commit #2688 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2688/])     HADOOP-8648. libhadoop: native CRC32 validation crashes when io.bytes.per.checksum=1. Contributed by Colin Patrick McCabe (Revision 1381419)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1381419 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.c * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.h * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util/test_bulk_crc32.c", "Integrated in Hadoop-Hdfs-trunk-Commit #2751 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2751/])     HADOOP-8648. libhadoop: native CRC32 validation crashes when io.bytes.per.checksum=1. Contributed by Colin Patrick McCabe (Revision 1381419)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1381419 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.c * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.h * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util/test_bulk_crc32.c", "I've committed this and merged to branch-2. Thanks Colin!", "Integrated in Hadoop-Mapreduce-trunk-Commit #2712 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2712/])     HADOOP-8648. libhadoop: native CRC32 validation crashes when io.bytes.per.checksum=1. Contributed by Colin Patrick McCabe (Revision 1381419)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1381419 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.c * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.h * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util/test_bulk_crc32.c", "Integrated in Hadoop-Hdfs-trunk #1157 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1157/])     HADOOP-8648. libhadoop: native CRC32 validation crashes when io.bytes.per.checksum=1. Contributed by Colin Patrick McCabe (Revision 1381419)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1381419 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.c * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.h * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util/test_bulk_crc32.c", "Integrated in Hadoop-Mapreduce-trunk #1188 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1188/])     HADOOP-8648. libhadoop: native CRC32 validation crashes when io.bytes.per.checksum=1. Contributed by Colin Patrick McCabe (Revision 1381419)       Result = ABORTED eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1381419 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.c * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/bulk_crc32.h * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/native/src/test/org/apache/hadoop/util/test_bulk_crc32.c"], "tasks": {"summarization": "libhadoop:  native CRC32 validation crashes when io.bytes.per.checksum=1 - The native CRC32 code, found in {{pipelined_crc32c}}, crashes when chunksize is set to 1.  {code} 12...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8648 about?", "answer": "libhadoop:  native CRC32 validation crashes when io.bytes.per.checksum=1"}}}
{"issue_id": "HADOOP-8647", "project": "HADOOP", "title": "BlockCompressionStream won't work with BlockDecompressionStream when there are several write", "status": "Patch Available", "priority": "Minor", "reporter": "Sean Zhong", "assignee": null, "created": "2012-08-03T06:27:50.000+0000", "updated": "2015-05-06T03:31:09.000+0000", "labels": ["BB2015-05-TBR"], "description": "BlockDecompressionStream can not read compressed data using BlockCompressionStream when there are multiple writes to BlockCompressionStream.", "comments": ["Unit Test.", "patch supplied in attachment.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539197/BlockCompressorStream.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1250//console  This message is automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12539197/BlockCompressorStream.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6257/console |   This message was automatically generated."], "tasks": {"summarization": "BlockCompressionStream won't work with BlockDecompressionStream when there are several write - BlockDecompressionStream can not read compressed data using BlockCompressionStream when there are mu...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8647 about?", "answer": "BlockCompressionStream won't work with BlockDecompressionStream when there are several write"}}}
{"issue_id": "HADOOP-8645", "project": "HADOOP", "title": "Stabilize branch-1-win", "status": "Resolved", "priority": "Major", "reporter": "Bikas Saha", "assignee": null, "created": "2012-08-03T05:25:21.000+0000", "updated": "2015-02-13T21:20:23.000+0000", "labels": [], "description": "Most of the code changes to make Hadoop branch 1 work natively on Windows are done. This jira in intended to track the work needed to achieve 100% test pass for the dev tests.", "comments": ["Changes are being reviewed on the HWX JIRA at https://hwxmonarch.atlassian.net/browse/HADOOP. Two outstanding items remain to be completed:  a. TODO: Perform internal hsync on a smaller bounded queue when it approaches capacity approaching capacity. b. TODO: The I/O queue is single threaded. Mostafa claims that this is for correctness. I have to investigate whether we can in fact do overlapped parallel I/O\u2019s since the addresses of the log blocks are already determined by the streaming interface.  The temporary fix with the unbounded queue is coded and now works with large tables in YCSB.", "Stabilization for Windows on branch-1-win completed a while ago, so I'm resolving this."], "tasks": {"summarization": "Stabilize branch-1-win  - Most of the code changes to make Hadoop branch 1 work natively on Windows are done. This jira in int...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8645 about?", "answer": "Stabilize branch-1-win"}}}
{"issue_id": "HADOOP-8644", "project": "HADOOP", "title": "AuthenticatedURL should be able to use SSLFactory", "status": "Closed", "priority": "Critical", "reporter": "Alejandro Abdelnur", "assignee": "Alejandro Abdelnur", "created": "2012-08-02T21:54:55.000+0000", "updated": "2012-10-11T17:45:07.000+0000", "labels": [], "description": "This is required to enable the use of HTTPS with SPNEGO using Hadoop configured keystores. This is required by HADOOP-8581.", "comments": ["+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538964/HADOOP-8644.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 3 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-auth hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1246//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1246//console  This message is automatically generated.", "This appears to be regressing {{URLUtils.openConnection}} that sets connect and read timeouts?", "@Daryn, I don't see how that is possible as {{URLUtils}} it is not using {{AuthenticatedURL}} class.", "That particular class/method doesn't necessarily have to be used, but we need to carry over the setting of timeouts.", "It makes sense, this should be done in all places where AuthenticatedURL is being used. But I think it should be done as part of a separate JIRA as this one is not introducing such regression. Makes sense?", "I suppose it can be a separate jira, but it needs to be a high prio too since the timeouts were added to address serious problems.", "Filed JIRA HDFS-3761. Other than that, are we good with this JIRA?", "I'm not sure I fully grok the change, but I think it looks ok.  One question, why does {{SSLFactory#configure}} need to check if the connection is an instance of {{HttpsURLConnection}}?  Given that this is a ssl factory, when would it legitimately be invoked with a non-https connection?", "{{AuthenticatedURL}} can be used over clear HTTP, that is how it has been used until now, just to provide authentication. This change is to be able to use with HTTPS in order to add encryption. Configuring Hadoop to user Kerberos and to use encryption will still be 2 different knobs as I assume many folks don't want to pay the performance price of wire encryption. Hope this clarifies.", "Thanks, that clarifies the intention, although I'm still confused why {{SSLFactory}} is involved in a non-ssl connection?  If the scheme isn't https, shouldn't it not even be passed in order to avoid other possible unintended consequences?", "If you have a {{AuthenticatedURL}} instance that is being used to connect to both HTTP and HTTPS endpoints, you'll need to configure the connection with the SSLFactory only if it is HTTPS. This scenario does not currently happen in Hadoop, it is just a a safeguard for now. I don't see unintended consequences as it is a NOP if HTTP. Makes sense?", "Patch looks good to me. Two small nits:   # You can probably do away with the TestConnectorConfigurator class entirely, and just use a mock and verify(). # Please indent 4 spaces when continuing a line instead of 2, e.g.: {code} +  public HttpURLConnection configure(HttpURLConnection conn) +    throws IOException { {code} and: {code} +      HttpsURLConnection sslConn = +        (HttpsURLConnection) new URL(\"https://foo\").openConnection(); {code}  +1 once these are addressed.", "+1 for me too", "new patch Addressing ATM's comments.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539359/HADOOP-8644.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1255//console  This message is automatically generated.", "Integrated in Hadoop-Hdfs-trunk-Commit #2622 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2622/])     HADOOP-8644. AuthenticatedURL should be able to use SSLFactory. (tucu) (Revision 1370045)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370045 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/Authenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/ConnectionConfigurator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/KerberosAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/PseudoAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestAuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/TestSSLFactory.java", "Integrated in Hadoop-Common-trunk-Commit #2557 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2557/])     HADOOP-8644. AuthenticatedURL should be able to use SSLFactory. (tucu) (Revision 1370045)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370045 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/Authenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/ConnectionConfigurator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/KerberosAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/PseudoAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestAuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/TestSSLFactory.java", "Committed to trunk and branch-2.", "Alejandro, can you please wait for Jenkins to +1 before committing the patch. In this case I do not see +1 for your updated patch.", "Integrated in Hadoop-Mapreduce-trunk-Commit #2576 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2576/])     HADOOP-8644. AuthenticatedURL should be able to use SSLFactory. (tucu) (Revision 1370045)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370045 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/Authenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/ConnectionConfigurator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/KerberosAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/PseudoAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestAuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/TestSSLFactory.java", "@suresh, my bad, committed the patch after addressing ATM's comments which where minor. Jenkins failed because it tried to apply the patch on a revision the patch has been already committed. Even if trivial, I'll wait next time to avoid this mis-failures and to ensure I'm not introducing any test-patch warning in the new revision.", "@Alejandro, sounds good :)", "Integrated in Hadoop-Hdfs-trunk #1128 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1128/])     HADOOP-8644. AuthenticatedURL should be able to use SSLFactory. (tucu) (Revision 1370045)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1370045 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/AuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/Authenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/ConnectionConfigurator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/KerberosAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/main/java/org/apache/hadoop/security/authentication/client/PseudoAuthenticator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/AuthenticatorTestCase.java * /hadoop/common/trunk/hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/client/TestAuthenticatedURL.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/ssl/SSLFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/ssl/TestSSLFactory.java"], "tasks": {"summarization": "AuthenticatedURL should be able to use SSLFactory - This is required to enable the use of HTTPS with SPNEGO using Hadoop configured keystores. This is r...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8644 about?", "answer": "AuthenticatedURL should be able to use SSLFactory"}}}
{"issue_id": "HADOOP-8643", "project": "HADOOP", "title": "hadoop-client should exclude hadoop-annotations from hadoop-common dependency", "status": "Open", "priority": "Minor", "reporter": "Alejandro Abdelnur", "assignee": null, "created": "2012-08-02T19:27:38.000+0000", "updated": "2015-07-21T17:54:35.000+0000", "labels": [], "description": "When reviewing HADOOP-8370 I've missed that changing the scope to compile for hadoop-annotations in hadoop-common it would make hadoop-annotations to bubble up in hadoop-client. Because of this we need to explicitly exclude it.", "comments": ["Patch attached. Excludes hadoop-annotations from hadoop-client's compile scope.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538987/hadoop-8643.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-client.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1247//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1247//console  This message is automatically generated.", "I'm afraid this won't work as expected.  {{hadoop-client}} has {{hadoop-project-dist}} as parent and {{hadoop-project-dist}} has {{hadoop-annotations}} as dependency. Because of this, {{hadoop-client}} cannot exclude {{hadoop-annotations}}.  Currently {{hadoop-client}} show {{hadoop-annotations}} with {{provided}} scope, this means that it should not be pulled during packaging, thus we are good for now.   Still, we should see how to untangle this in a better way.  For now we lower the priority of this JIRA.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538987/hadoop-8643.txt   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-client.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3497//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3497//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538987/hadoop-8643.txt   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-client.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4478//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4478//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538987/hadoop-8643.txt   against trunk revision 1556f86.      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.        {color:red}-1 javac{color}.  The applied patch generated 1229 javac compiler warnings (more than the trunk's current 1224 warnings).      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-client.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5137//testReport/ Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/5137//artifact/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5137//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538987/hadoop-8643.txt   against trunk revision c3003eb.      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-client.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5904//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5904//console  This message is automatically generated."], "tasks": {"summarization": "hadoop-client should exclude hadoop-annotations from hadoop-common dependency - When reviewing HADOOP-8370 I've missed that changing the scope to compile for hadoop-annotations in ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8643 about?", "answer": "hadoop-client should exclude hadoop-annotations from hadoop-common dependency"}}}
{"issue_id": "HADOOP-8642", "project": "HADOOP", "title": "Document that io.native.lib.available only controls native bz2 and zlib compression codecs", "status": "Closed", "priority": "Major", "reporter": "Eli Collins", "assignee": "Akira Ajisaka", "created": "2012-08-02T17:13:26.000+0000", "updated": "2016-04-06T06:57:32.000+0000", "labels": [], "description": "Per core-default.xml {{io.native.lib.available}} indicates \"Should native hadoop libraries, if present, be used\" however it looks like it only affects bzip2 and zlib. Even if we set {{io.native.lib.available}} to false, native libraries are loaded and the libraries other than bzip2 and zlib are actually used. We should document that.", "comments": ["this is a very important feature. Just spent a few hours trying to figure out why NativeIO was still being invoked by RawLocalFileSystem even though i had turned io.native.lib.available off.", "Attaching a patch. Now NativeCodeLoader always try to load native library in static method. The patch will check configuration in the method. If io.native.lib.available is false, NativeCodeLoader never try to load native library.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12622832/HADOOP-8642.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.io.compress.TestCodec                   org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3431//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3431//console  This message is automatically generated.", "Renewed the patch to pass the failed tests.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12622852/HADOOP-8642.2.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3432//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3432//console  This message is automatically generated.", "Update the patch to # discard the change in {{getLoadNativeLibraries(Configuration)}} # clean up the code", "Probably, {{TestNativeCodeLoader}} will be skipped in Jenkins. The test can be run manually by {{mvn test -Pnative -Dtest=TestNativeCodeLoader -Drequire.test.libhadoop=true}}", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12662733/HADOOP-8642.3.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestActiveStandbyElector                   org.apache.hadoop.ha.TestZKFailoverController      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4509//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4509//console  This message is automatically generated.", "The test failures seem to be unrelated to the patch.", "It looks like this is the right thing to do, although the patch would need a rebase at this point.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12662733/HADOOP-8642.3.patch   against trunk revision 1a0f508.      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5689//console  This message is automatically generated.", "Thank you Chris! Rebased and refactored the patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12698829/HADOOP-8642.4.patch   against trunk revision 8752568.      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.security.token.delegation.web.TestWebDelegationToken  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5691//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5691//console  This message is automatically generated.", "Akira, sorry to go back and forth, but I'm now having second thoughts about this change after a deeper review.  Digging into revision history, I can see that this functionality was first introduced in HADOOP-1570.  The git commit hash is f9f6143876a1ff6e8eb1266917f4992f642aaea7.  The property was named {{hadoop.native.lib}} then.  This was a point in the project's history when the only native code in Hadoop was native compression codecs.  The current codebase of course has a lot more native code that's important for a lot of different things beyond the compression codecs.  I'm concerned about possible backwards incompatibility.  Existing deployments might be using this property to disable the native compression codecs, but are still dependent on the rest of the native code for other functionality.  If we were to push this change to those existing clusters, then they'd suddenly lose the rest of that important native code.  Another concern is the instantiation of yet another {{Configuration}} and all of the parsing that entails.  I don't think there would be any way around that, because this is all driven from static initialization.  All things considered, I have to vote -1 on changing the behavior of this property, at least within the 2.x line where we need to maintain backwards compatibility.  I would be +1 for a patch that updates the description in core-default.xml to clarify that it really only controls the native bz2 and zlib compression codecs.", "bq. If we were to push this change to those existing clusters, then they'd suddenly lose the rest of that important native code. I agree with you. I'll update the patch to update the description in core-default.xml. Thanks [~cnauroth] for the deeper review!", "Attaching a patch to update core-default.xml.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12698866/HADOOP-8642.5.patch   against trunk revision 2efb234.      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/5692//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5692//console  This message is automatically generated.", "+1 for the patch.  Thanks for taking care of this, Akira.  Let's hold off on committing until next week.  I see a lot of watchers, and this has gone in a different direction than originally described, so let's give them a chance to comment if they want.  bq. -1 tests included. The patch doesn't appear to include any new or modified tests.  This is a documentation patch only updating an XML file, so no new tests are required.", "Thanks, guys.  I agree with the patch as it's written here.  It is a good solution for 2.x.  I wonder if we can get rid of this property completely in 3.x?  I don't see a lot of use for it, to be honest.  Don't we (semi) silently omit the native support for bz2 and zlib during the build if those libraries are not installed?  Stepping back even further than that, we have too many options for doing partial or weirdo builds that nobody actually wants.  Would that many tears be shed if we just made zlib and bz2 mandatory for {{\\-Pnative}}?  I bet that probably fewer tears would be shed because fewer people would accidentally get incomplete builds (nobody in the world actually wants a {{\\-Pnative}} build without bz2 and zlib.... it's a square wheel)", "bq. I wonder if we can get rid of this property completely in 3.x? Agree with you. I'm +1 for getting rid of the property in trunk.", "+1 for deprecating the property in branch-2 and removing it in trunk.  Reading the ancient history, it appears the property was introduced to offer a quick deploy-time safety net in case the native compression codecs (very new code at the time) encountered bugs.  At this point, the native compression codec code has been relatively stable for a while, so I see little use for this flag in practice.  As you said, there are alternative workarounds.  +1 for reducing the build permutations on trunk too.  I personally have no use case for building native without bz2 and zlib, and it can only cause confusion if builds pass but then fail in system test due to lack of those symbols linked into libhadoop.so/hadoop.dll.  I also have no use case for building without Snappy, though I imagine removing the options for that one could be a little more controversial.", "HADOOP-10409 and HADOOP-10452 are further evidence that all of the native build variants can cause confusion.", "At the time we added snappy support to Hadoop, Linux distros had only just started to include the snappy libs.  If we did that work today, we probably wouldn't put so many build knobs and dials in, since nearly every Linux distro now has snappy support easily available as standard.  I would not oppose making snappy mandatory for the native build in 3.0.0, although I think it's too late for 2.x.", "Committing this. Thanks [~cnauroth] and [~cmccabe] for reviews and discussions. I'll create a separate jira to remove the property from trunk.", "Committed this to trunk and branch-2. Thanks all.", "FAILURE: Integrated in Hadoop-trunk-Commit #7181 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/7181/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml * hadoop-common-project/hadoop-common/CHANGES.txt", "Filed HADOOP-11627 to remove the property from trunk.", "FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #114 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/114/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml * hadoop-common-project/hadoop-common/CHANGES.txt", "SUCCESS: Integrated in Hadoop-Yarn-trunk #848 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/848/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml * hadoop-common-project/hadoop-common/CHANGES.txt", "FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #105 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/105/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/CHANGES.txt * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "SUCCESS: Integrated in Hadoop-Hdfs-trunk #2046 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/2046/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/CHANGES.txt * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #114 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/114/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/CHANGES.txt * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "FAILURE: Integrated in Hadoop-Mapreduce-trunk #2064 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2064/]) HADOOP-8642. Document that io.native.lib.available only controls native bz2 and zlib compression codecs. (aajisaka) (aajisaka: rev ab5976161f3afaaf2ace60bab400e0d8dbc61923) * hadoop-common-project/hadoop-common/src/main/resources/core-default.xml * hadoop-common-project/hadoop-common/CHANGES.txt"], "tasks": {"summarization": "Document that io.native.lib.available only controls native bz2 and zlib compression codecs - Per core-default.xml {{io.native.lib.available}} indicates \"Should native hadoop libraries, if prese...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8642 about?", "answer": "Document that io.native.lib.available only controls native bz2 and zlib compression codecs"}}}
{"issue_id": "HADOOP-8641", "project": "HADOOP", "title": "handleConnectionFailure(..) in Client.java should properly handle interrupted exception", "status": "Resolved", "priority": "Major", "reporter": "suja s", "assignee": null, "created": "2012-08-02T14:15:31.000+0000", "updated": "2012-08-03T01:41:52.000+0000", "labels": [], "description": "If connection retries are happening and thread is interrupted the interruption is not happening and retries will continue till max number of retries configured.", "comments": ["Hey Suja, Could you provide more details and perhaps client logs showing the behavior you're describing?", "I think this is the same as HADOOP-6221 - interrupts get swallowed during the connection cycle.   If that's the case -mark this as duplicate & get the patch in the original bugrep in.", "Agree.  Let's resolve this as a duplicate of HADOOP-6221."], "tasks": {"summarization": "handleConnectionFailure(..) in Client.java should properly handle interrupted exception - If connection retries are happening and thread is interrupted the interruption is not happening and ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8641 about?", "answer": "handleConnectionFailure(..) in Client.java should properly handle interrupted exception"}}}
{"issue_id": "HADOOP-8640", "project": "HADOOP", "title": "DU thread transient failures propagate to callers", "status": "Resolved", "priority": "Major", "reporter": "Todd Lipcon", "assignee": null, "created": "2012-08-02T07:28:40.000+0000", "updated": "2018-05-12T05:42:04.000+0000", "labels": [], "description": "When running some stress tests, I saw a failure where the DURefreshThread failed due to the filesystem changing underneath it:  {code} org.apache.hadoop.util.Shell$ExitCodeException: du: cannot access `/data/4/dfs/dn/current/BP-1928785663-172.20.90.20-1343880685858/current/rbw/blk_4637779214690837894': No such file or directory {code} (the block was probably finalized while the du process was running, which caused it to fail)  The next block write, then, called {{getUsed()}}, and the exception got propagated causing the write to fail. Since it was a pseudo-distributed cluster, the client was unable to pick a different node to write to and failed.  The current behavior of propagating the exception to the next (and only the next) caller doesn't seem well-thought-out.", "comments": ["Seems to affect 1.2.1, during normal 'hadoop fs -put', too.  from datanode.log: {quote} 2013-10-08 08:20:27,288 WARN org.apache.hadoop.util.Shell: Could not get disk usage information org.apache.hadoop.util.Shell$ExitCodeException: du: cannot access `/..../hdfs/datanode/blocksBeingWritten/blk_2086885445451145306': No such file or directory          at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)         at org.apache.hadoop.util.Shell.run(Shell.java:182)         at org.apache.hadoop.fs.DU.access$200(DU.java:29)         at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:84)         at java.lang.Thread.run(Thread.java:662) {quote}  output on the console: {quote} 13/10/08 08:20:27 INFO hdfs.DFSClient: Exception in createBlockOutputStream 192.168.x.y:50010 java.io.EOFException 13/10/08 08:20:27 INFO hdfs.DFSClient: Abandoning blk_-605554355196703343_69209 13/10/08 08:20:27 INFO hdfs.DFSClient: Excluding datanode 192.168.x.y:50010 {quote}", "Getting this in 2.6.0 too, during TestDFSIO {quote} 2014-12-15 20:40:19,944 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver constructor. Cause is org.apache.hadoop.util.Shell$ExitCodeException: du: cannot access `/data3/dfs/dn/current/BP-1172228382-36.0.0.112-1418564551057/current/rbw/blk_1073754273': No such file or directory     at org.apache.hadoop.util.Shell.runCommand(Shell.java:511)     at org.apache.hadoop.util.Shell.run(Shell.java:424)     at org.apache.hadoop.fs.DU.run(DU.java:190)     at org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:119)     at java.lang.Thread.run(Thread.java:745) {quote}", "HADOOP-12973 unintentionally fixed this bug.", "Given that the refactor in HADOOP-12973 unintentionally eliminated this problem in 2.8.0 and above, I'll mark this as a won't fix."], "tasks": {"summarization": "DU thread transient failures propagate to callers - When running some stress tests, I saw a failure where the DURefreshThread failed due to the filesyst...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8640 about?", "answer": "DU thread transient failures propagate to callers"}}}
{"issue_id": "HADOOP-8639", "project": "HADOOP", "title": "Shell doesn't give specific details about why it can't create a file", "status": "Open", "priority": "Major", "reporter": "Kathleen Ting", "assignee": null, "created": "2011-05-10T22:18:52.000+0000", "updated": "2012-08-01T03:31:25.000+0000", "labels": ["newbie"], "description": "It would be a lot more useful to print an error message indicating why that you can't create the file - e.g. can't create a directory with the name of the file you're trying to write to.  Also, the log file isn't much better. You'll see exceptions that look like this: 2011-04-14 12:29:37,757 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user ___ org.apache.hadoop.util.Shell$ExitCodeException: id: ___: No such user at org.apache.hadoop.util.Shell.runCommand(Shell.java:255) at org.apache.hadoop.util.Shell.run(Shell.java:182) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375) at org.apache.hadoop.util.Shell.execCommand(Shell.java:461) at org.apache.hadoop.util.Shell.execCommand(Shell.java:444) at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:66) at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:43) at org.apache.hadoop.security.Groups.getGroups(Groups.java:79) at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1022) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:50) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4920) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkTraverse(FSNamesystem.java:4903) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:1878) at org.apache.hadoop.hdfs.server.namenode.NameNode.getFileInfo(NameNode.java:795) at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1416) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1412) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1410)", "comments": ["Actually, for this stack trace, it does a pretty good job of telling you what happened:  got exception trying to get groups for user ___ org.apache.hadoop.util.Shell$ExitCodeException: id: ___: No such user  In other words, the username that hadoop is trying to use doesn't exist on the machine and therefore can't get any group information.", "Yes, you can determine that there was a problem regarding users there, but the problem in this case was that a sub-directory did not exist.  And that problem was not adequately reported in the exception or the log stack trace."], "tasks": {"summarization": "Shell doesn't give specific details about why it can't create a file - It would be a lot more useful to print an error message indicating why that you can't create the fil...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8639 about?", "answer": "Shell doesn't give specific details about why it can't create a file"}}}
{"issue_id": "HADOOP-8638", "project": "HADOOP", "title": "TestUlimit fails locally (on some machines)", "status": "Resolved", "priority": "Major", "reporter": "Karthik Kambatla", "assignee": "Karthik Kambatla", "created": "2012-08-01T00:20:45.000+0000", "updated": "2014-11-03T18:33:54.000+0000", "labels": [], "description": "ant clean test -Dtestcase=TestUlimit -Dtest.output=yes fails locally  Attaching the dump.", "comments": ["Interestingly, the exit code from the default task controller is different across machines, I have seen 1 or 134 so far.", "MAPREDUCE-4036 addresses the same issue - the corresponding patch there seems to solve the issue on the local machine.   We might have to re-open this should anyone experience the same in other (new) environments."], "tasks": {"summarization": "TestUlimit fails locally (on some machines) - ant clean test -Dtestcase=TestUlimit -Dtest.output=yes fails locally  Attaching the dump....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8638 about?", "answer": "TestUlimit fails locally (on some machines)"}}}
{"issue_id": "HADOOP-8637", "project": "HADOOP", "title": "FilterFileSystem#setWriteChecksum is broken", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-31T17:49:12.000+0000", "updated": "2016-05-12T18:27:19.000+0000", "labels": [], "description": "{{FilterFileSystem#setWriteChecksum}} is being passed through as {{fs.setVERIFYChecksum}}.  Example of impact is checksums cannot be disabled for LFS if a filter fs (like {{ChRootedFileSystem}}) is applied.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538581/HADOOP-8637.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1238//console  This message is automatically generated.", "Change looks good +1 assuming Jenkins comes back OK.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538588/HADOOP-8637.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1239//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1239//console  This message is automatically generated.", "Thanks Daryn,  I put this into trunk, branch-2, and branch-0.23", "Integrated in Hadoop-Common-trunk-Commit #2539 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2539/])     HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367702)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367702 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2603 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2603/])     HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367702)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367702 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2559 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2559/])     HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367702)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367702 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Hdfs-0.23-Build #331 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/331/])     svn merge -c 1367702 FIXES: HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367705)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367705 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Hdfs-trunk #1122 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1122/])     HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367702)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367702 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk #1154 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1154/])     HADOOP-8637. FilterFileSystem#setWriteChecksum is broken (daryn via bobby) (Revision 1367702)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367702 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java"], "tasks": {"summarization": "FilterFileSystem#setWriteChecksum is broken - {{FilterFileSystem#setWriteChecksum}} is being passed through as {{fs.setVERIFYChecksum}}.  Example ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8637 about?", "answer": "FilterFileSystem#setWriteChecksum is broken"}}}
{"issue_id": "HADOOP-8635", "project": "HADOOP", "title": "Cannot cancel paths registered deleteOnExit", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-30T15:44:57.000+0000", "updated": "2016-05-12T18:26:02.000+0000", "labels": [], "description": "{{FileSystem#deleteOnExit}} does not have a symmetric method to unregister files.  Since it's used to register temporary files during a copy operation, this can lead to a lot of unnecessary rpc operations for files successfully copied when the {{FileSystem}} is closed.", "comments": ["Add {{cancelDeleteOnExit}}.  Add a lot of tests to cover {{deleteOnExit}} behavior.", "The code is fairly simple, and looks good to me +1, assuming Jenkins comes back OK.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538388/HADOOP-8635.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.fs.TestFilterFileSystem      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1232//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1232//console  This message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538398/HADOOP-8635-1.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 2 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1233//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1233//console  This message is automatically generated.", "Thanks Daryn +1,  I put this into trunk, branch-2, and branch-0.23", "Integrated in Hadoop-Common-trunk-Commit #2533 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2533/])     HADOOP-8635. Cannot cancel paths registered deleteOnExit (daryn via bobby) (Revision 1367296)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367296 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2597 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2597/])     HADOOP-8635. Cannot cancel paths registered deleteOnExit (daryn via bobby) (Revision 1367296)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367296 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2553 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2553/])     HADOOP-8635. Cannot cancel paths registered deleteOnExit (daryn via bobby) (Revision 1367296)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367296 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java", "Integrated in Hadoop-Hdfs-0.23-Build #330 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/330/])     svn merge -c 1367296 FIXES: HADOOP-8635. Cannot cancel paths registered deleteOnExit (daryn via bobby) (Revision 1367298)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367298 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java"], "tasks": {"summarization": "Cannot cancel paths registered deleteOnExit - {{FileSystem#deleteOnExit}} does not have a symmetric method to unregister files.  Since it's used t...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8635 about?", "answer": "Cannot cancel paths registered deleteOnExit"}}}
{"issue_id": "HADOOP-8634", "project": "HADOOP", "title": "Ensure FileSystem#close doesn't squawk for deleteOnExit paths", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-30T14:25:49.000+0000", "updated": "2016-05-12T18:26:00.000+0000", "labels": [], "description": "{{FileSystem#deleteOnExit}} doesn't check if the path exists before attempting to delete.  Errors may cause unnecessary INFO log squawks.", "comments": ["Will submit patch after HADOOP-8627 is committed else tests will fail.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538349/HADOOP-8634.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1230//console  This message is automatically generated.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538359/HADOOP-8634.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1231//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1231//console  This message is automatically generated.", "The patch looks good and the tests pass +1.", "Thanks Daryn, I put this into trunk, branch-2, and branch-0.23", "Integrated in Hadoop-Common-trunk-Commit #2533 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2533/])     HADOOP-8634. Ensure FileSystem#close doesn't squawk for deleteOnExit paths (daryn via bobby) (Revision 1367196)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367196 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2597 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2597/])     HADOOP-8634. Ensure FileSystem#close doesn't squawk for deleteOnExit paths (daryn via bobby) (Revision 1367196)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367196 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2553 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2553/])     HADOOP-8634. Ensure FileSystem#close doesn't squawk for deleteOnExit paths (daryn via bobby) (Revision 1367196)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367196 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Hdfs-0.23-Build #330 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/330/])     svn merge -c 1367196 FIXES: HADOOP-8634. Ensure FileSystem#close doesn't squawk for deleteOnExit paths (daryn via bobby) (Revision 1367200)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367200 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java"], "tasks": {"summarization": "Ensure FileSystem#close doesn't squawk for deleteOnExit paths - {{FileSystem#deleteOnExit}} doesn't check if the path exists before attempting to delete.  Errors ma...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8634 about?", "answer": "Ensure FileSystem#close doesn't squawk for deleteOnExit paths"}}}
{"issue_id": "HADOOP-8633", "project": "HADOOP", "title": "Interrupted FsShell copies may leave tmp files", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-30T14:17:13.000+0000", "updated": "2016-05-12T18:25:59.000+0000", "labels": [], "description": "Interrupting a copy, ex. via SIGINT, may cause tmp files to not be removed.  If the user is copying large files then the remnants will eat into the user's quota.", "comments": ["{{deleteOnExit}} issues contribute and/or are related to the problem", "Will submit after deps are checked in.", "Looks good to me. I like the use of {{TargetFileSystem}}.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538590/HADOOP-8633.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1240//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1240//console  This message is automatically generated.", "The changes look OK. I'm not sure I really like TargetFileSystem. I just don't see a lot of benefit from making TargetFileSystem.  I think it would be cleaner to just have your own list of Paths to delete and then delete them in the finally block.  Having it be a FileSystem just seems confusing to me.   Because TargetFileSystem is never added to the FileSystem cache I assume that if a System.exit is called somewhere while processing the command the TargetFileSystem will not be closed and the files will not actually be deleted on exit.  Is this important?", "The {{TargetFileSystem}} is just a shim over a real filesystem that is registering and canceling temp paths for deletion.  The shim simplifies all the code performing the copy and helps ensure the temp files are cancelled and/or deleted immediately.  I originally did what you suggest and I wound up with multiple nested try blocks and conditions that made the code (imho) harder to read, understand, and difficult to test.  It's true that a call to {{System.exit}} won't cleanup the filesystem, but it's only called as the last line in {{main}}.  Calling it in other places would break functionality and be a bug.  (I did manually run a copy with 100 iterations and pounded on control-c and no remnants where left)", "Ok +1", "I went ahead and committed this. Thanks Daryn, Bobby, and Kihwal!", "Integrated in Hadoop-Hdfs-trunk-Commit #2612 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2612/])     HADOOP-8633. Interrupted FsShell copies may leave tmp files (Daryn Sharp via tgraves) (Revision 1368002)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java", "Integrated in Hadoop-Common-trunk-Commit #2547 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2547/])     HADOOP-8633. Interrupted FsShell copies may leave tmp files (Daryn Sharp via tgraves) (Revision 1368002)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2565 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2565/])     HADOOP-8633. Interrupted FsShell copies may leave tmp files (Daryn Sharp via tgraves) (Revision 1368002)       Result = FAILURE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java", "Integrated in Hadoop-Hdfs-0.23-Build #332 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/332/])     merge -r 1368002:1368003 from branch-2. FIXES: HADOOP-8633 (Revision 1368004)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368004 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java", "Integrated in Hadoop-Hdfs-trunk #1123 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1123/])     HADOOP-8633. Interrupted FsShell copies may leave tmp files (Daryn Sharp via tgraves) (Revision 1368002)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java", "Integrated in Hadoop-Mapreduce-trunk #1155 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1155/])     HADOOP-8633. Interrupted FsShell copies may leave tmp files (Daryn Sharp via tgraves) (Revision 1368002)       Result = FAILURE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestCopy.java"], "tasks": {"summarization": "Interrupted FsShell copies may leave tmp files - Interrupting a copy, ex. via SIGINT, may cause tmp files to not be removed.  If the user is copying ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8633 about?", "answer": "Interrupted FsShell copies may leave tmp files"}}}
{"issue_id": "HADOOP-8632", "project": "HADOOP", "title": "Configuration leaking class-loaders", "status": "Closed", "priority": "Major", "reporter": "Costin Leau", "assignee": "Costin Leau", "created": "2012-07-30T14:16:11.000+0000", "updated": "2014-09-04T00:59:39.000+0000", "labels": [], "description": "The newly introduced CACHE_CLASSES leaks class loaders causing associated classes to not be reclaimed.  One solution is to remove the cache itself since each class loader implementation caches the classes it loads automatically and preventing an exception from being raised is just a micro-optimization that, as one can tell, causes bugs instead of improving anything. In fact, I would argue in a highly-concurrent environment, the weakhashmap synchronization/lookup probably costs more then creating the exception itself.  Another is to prevent the leak from occurring, by inserting the loadedclass into the WeakHashMap wrapped in a WeakReference. Otherwise the class has a strong reference to its classloader (the key) meaning neither gets GC'ed. And since the cache_class is static, even if the originating Configuration instance gets GC'ed, its classloader won't.", "comments": ["Before we do anything we probably want to set up a micro-benchmark to validate any changes that happen.  I personally don't have much of a problem leaking classes in Hadoop itself so long as we document that it might happen.  We typically don't use one class and then drop it.  In almost all cases we will load a class that is used throughout the life of a process.  I am not sure about other projects that also use Configuration.", "Hi Costin. We added the {{CACHE_CLASSES}} member in HADOOP-6133 to solve a performance regression which we saw in practice. HADOOP-6502 also talks about another case where the performance of this code was critical. So I don't think removing the cache is a good idea.   That said, I do think changing the cache to have weak values as well as weak keys makes sense. Guava's MapMaker has a nice utility to do this, or we could simply change the class values to be Map<String, WeakReference<Class<?>>>.", "@Robert  My issue is not with the cache itself but with the leakage. If a client submits several big jobs, she has to either launch a new JVM for each submission or somehow patch the leak from outside. Or face OOM. Addressing this in the framework directly obviously is much better.  @Todd Wrapping the value with a WeakReference probably it's the easiest solution since it doesn't introduce a new library dependency. It can later be upgraded to MapMaker if the pattern occurs often.", "Costin,  I understand your issue more fully now, and I am fine if you want to add in WeakReferences to the ClassLoaders.  If you have a patch for this leak, I would be happy to review it.", "I've attached my patch. I picked it up from my fork on GitHub (of Hadoop Commons) - based it on hadoop-2.0.1 branch. See the code here: https://github.com/costin/hadoop-common/commit/57d9df37e600dd588a737d67b271657561ebfea2", "git patch", "By the way, in the same vein, ReflectionUtils#CONSTRUCTOR_CACHE also leaks classes (see HADOOP-8605 - I'm happy to fix that as well if you want).", "Kicking Jenkins so it will test the patch.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540677/0001-wrapping-classes-with-WeakRefs-in-CLASS_CACHE.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1286//console  This message is automatically generated.", "attaching result of: git diff --no-prefix fa23683720 57d9df3", "Since the previous patch (the standard git/GitHub patch [1]) didn't apply correctly, I've attached the git diff result as specified on the Hadoop wiki [2]  [1] https://github.com/costin/hadoop-common/commit/57d9df37e600dd588a737d67b271657561ebfea2.patch [2] http://wiki.apache.org/hadoop/GitAndHadoop", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540887/HADOOP-8632.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1295//console  This message is automatically generated.", "Guys I'm not sure why the patch doesn't apply - I've followed verbatim the instructions from the wiki. Any ideas on what's missing?", "When I try to apply the patch to trunk I get {noformat} $ patch -p 0 < HADOOP-8632.patch  patching file hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java Hunk #3 succeeded at 1532 (offset 55 lines). patching file hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java Hunk #2 FAILED at 1044. 1 out of 2 hunks FAILED -- saving rejects to file hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java.rej {noformat}  You may want to try upmerging the patch to trunk.", "Attaching the patch against the trunk (the previous patch as mentioned, was against branch-2.1.0-alpha). Ran git diff --no-prefix 7a3427de68 518c39814e  The github link is: http://j.mp/Nor44u  Did a cherry pick of the commit against 2.1.0-alpha.", "By the way, tried patch -p 0 < HADOOP-8632-trunk.patch and it applied cleanly against the trunk.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541186/HADOOP-8632-trunk.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1312//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1312//console  This message is automatically generated.", "The ZK failures seem to be caused by the CI host, independently from this patch.", "The code looks good and the existing tests all seem to pass.  Please remove the tabs from your patch, our coding standard requires the use of spaces instead, and then I am a +1.", "the patch w/o any tabs.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542100/HADOOP-8632-trunk-no-tabs.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1351//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1351//console  This message is automatically generated.", "Thanks Constin,  I merged this into trunk, and branch-2.  If you wanted it in another release please add a comment to indicate it.", "Integrated in Hadoop-Common-trunk-Commit #2626 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2626/])     HADOOP-8632. Configuration leaking class-loaders (Costin Leau via bobby) (Revision 1376543)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376543 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2690 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2690/])     HADOOP-8632. Configuration leaking class-loaders (Costin Leau via bobby) (Revision 1376543)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376543 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2654 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2654/])     HADOOP-8632. Configuration leaking class-loaders (Costin Leau via bobby) (Revision 1376543)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376543 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-trunk #1144 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1144/])     HADOOP-8632. Configuration leaking class-loaders (Costin Leau via bobby) (Revision 1376543)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376543 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Mapreduce-trunk #1175 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1175/])     HADOOP-8632. Configuration leaking class-loaders (Costin Leau via bobby) (Revision 1376543)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1376543 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Costin, I added you as a contributor to Hadoop and assigned this jira to you. Thanks for contributing to Hadoop.", "Thanks!"], "tasks": {"summarization": "Configuration leaking class-loaders - The newly introduced CACHE_CLASSES leaks class loaders causing associated classes to not be reclaime...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8632 about?", "answer": "Configuration leaking class-loaders"}}}
{"issue_id": "HADOOP-8631", "project": "HADOOP", "title": "The description of net.topology.table.file.name in core-default.xml is misleading", "status": "Resolved", "priority": "Minor", "reporter": "Han Xiao", "assignee": null, "created": "2012-07-30T03:02:17.000+0000", "updated": "2015-03-09T20:04:08.000+0000", "labels": [], "description": "The net.topology.table.file.name is used when net.topology.node.switch.mapping.impl property is set to org.apache.hadoop.net.TableMapping. However, in the description in core-default.xml, net.topology.script.file.name property is asked to set to org.apache.hadoop.net.TableMapping. This could mislead user into wrong configuration.", "comments": ["{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538299/core-default.xml.patch   against trunk revision de1101c.      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/5895//console  This message is automatically generated."], "tasks": {"summarization": "The description of net.topology.table.file.name in core-default.xml is misleading - The net.topology.table.file.name is used when net.topology.node.switch.mapping.impl property is set ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8631 about?", "answer": "The description of net.topology.table.file.name in core-default.xml is misleading"}}}
{"issue_id": "HADOOP-8630", "project": "HADOOP", "title": "rename isSingleSwitch() methods in new topo base class to isFlatTopology()", "status": "Resolved", "priority": "Trivial", "reporter": "Steve Loughran", "assignee": "Tsuyoshi Ozawa", "created": "2012-07-28T00:30:18.000+0000", "updated": "2016-05-12T18:24:43.000+0000", "labels": [], "description": "The new topology logic that is not yet turned on in HDFS uses the method {{isSingleSwitch()}} for implementations to declare whether or not they are single switch.   The use of \"switch\" is an implementation issue; the big VM-based patch shows that really it's about \"flat\" vs \"hierarchical\", with Hadoop assuming that subtrees in the hierarchy have better bandwidth (good) but correlated failures (bad).   Renaming the method now -before it's fixed and used- is time time to do it.", "comments": ["There are some files and methods which include the term\"Switch\"(AbstractDNSToSwitchMapping.java, CachedDNSToSwitchMapping.java, DNSToSwitchMapping.java). Should these files and methods be renamed with the term \"Hierarchicy\"(ex. AbstractDNSToHierarchyMapping.java)?", "renaming classes would break things. This method is new in 2.x and we can do it, especially as I haven't flipped the namenode into using it.   Compatibility beats purity", "Looking at the code, what I can do is rename the new base class in the 2.x branch to {{AbstractTopologyMapping}}, and the methods and documents in it.", "current code is in https://github.com/steveloughran/hadoop-trunk/tree/HDFS-2492-blocks", "Renamed AbstractDNSToSwitchMapping to AbstractTopologyMapping and isSingleSwitch() to isFlatTopology() by the attached patch.", "# that patch does a complete file replace; looks like CRLF grief, which I've encountered myself regularly. # can you grab the github fork I linked to and use that as a starting point as I did a bit more changes, including renaming the (new) base class. It needs more review and a test run, but I think it's effectively a superset of your patch.  -steve", "Steve,  Thank you for you review. Okey, I'll fix it. Should I send a pull request to your repository or attach the patch to this jira?", "patch + a pull is best. Note that that specific branch on github includes a place in DFS where it's actually used -that bit isn't part of the patch for common; two patches need to be created from the source tree", "Attached patch is the patch based on trunk by using git format-patch command.  And, I sent pull request based on your github.", "I've just had a look at this again and think that it's simpler to stick with the {{isSingleSwitch()}} terms as the code has been out there too long. I should have reviewed it earlier.  marking as WONTFIX.", "Marking as wontfix but crediting Tsuyoshi Ozawa for his contribution"], "tasks": {"summarization": "rename isSingleSwitch() methods in new topo base class to isFlatTopology() - The new topology logic that is not yet turned on in HDFS uses the method {{isSingleSwitch()}} for im...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8630 about?", "answer": "rename isSingleSwitch() methods in new topo base class to isFlatTopology()"}}}
{"issue_id": "HADOOP-8629", "project": "HADOOP", "title": "Add option for TableMapping to reload mapping file on match failure", "status": "Resolved", "priority": "Minor", "reporter": "Steve Loughran", "assignee": null, "created": "2012-07-28T00:18:09.000+0000", "updated": "2021-01-21T13:35:32.000+0000", "labels": [], "description": "As commented in HADOOP-7030, the table mapping topology mapper handles new node addition worse than the script mapping, because the table mapping is frozen for the life of the service.  I propose adding an option (true by default?) for the class to look at the timestamp of the mapping file, and reload it on a change -if a hostname lookup failed.", "comments": ["Auto reload of the table mapping is not completely straightforward as * need to make upload thread safe. * reload logic needs policy on failure to reload the file. Retain the old mapping? * reload logic needs policy on removal of entries from the file and changed values. With caching in front of the class, these changes won't get picked up anyway, but if we strip that out by not caching in-VM lookups, then it may become an issue.", "I prefer to provide an admin cmd to reload by administrator"], "tasks": {"summarization": "Add option for TableMapping to reload mapping file on match failure - As commented in HADOOP-7030, the table mapping topology mapper handles new node addition worse than ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8629 about?", "answer": "Add option for TableMapping to reload mapping file on match failure"}}}
{"issue_id": "HADOOP-8628", "project": "HADOOP", "title": "TableMapping init sets initialized flag prematurely", "status": "Resolved", "priority": "Minor", "reporter": "Steve Loughran", "assignee": null, "created": "2012-07-28T00:11:55.000+0000", "updated": "2021-01-21T13:35:41.000+0000", "labels": [], "description": "As reported in HADOOP-7030; the TableMapping class sets the initialized flag to true before attempting to load the table. This means that it is set even if the load failed, so preventing other attempts to load the file from working without restarting the service.", "comments": ["IMHO, it's not a problem, cause the load() method has catch all exception..."], "tasks": {"summarization": "TableMapping init sets initialized flag prematurely - As reported in HADOOP-7030; the TableMapping class sets the initialized flag to true before attempti...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8628 about?", "answer": "TableMapping init sets initialized flag prematurely"}}}
{"issue_id": "HADOOP-8627", "project": "HADOOP", "title": "FS deleteOnExit may delete the wrong path", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-27T21:07:22.000+0000", "updated": "2016-05-12T18:24:36.000+0000", "labels": [], "description": "{{FilterFileSystem}} is incorrectly delegating {{deleteOnExit}} to the raw underlying fs.  This is wrong, because each fs instance is intended to maintain its own pool of temp files.  Worse yet, this means registering a file via {{ChRootedFileSystem#deleteOnExit}} will delete the file w/o the chroot path prepended!", "comments": ["Remove the problematic method so {{delete}} is called by the close/shutdown hook which will correctly translate the path.", "The patch looks good to me.  I like it when we fix bugs by removing code :) +1 pending Jenkins.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538214/HADOOP-8627.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.fs.TestFilterFileSystem      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1227//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1227//console  This message is automatically generated.", "Add test I missed while teasing about patches.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538345/HADOOP-8627-1.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 2 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1228//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1228//console  This message is automatically generated.", "Test failure is unrelated.  Failover tests randomly timeout.", "The new patch looks good, and the tests pass +1.  I'll check it in.", "Thanks Daryn,  I put this into trunk, branch-2, and branch-0.23", "Integrated in Hadoop-Mapreduce-trunk-Commit #2552 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2552/])     HADOOP-8627. FS deleteOnExit may delete the wrong path (daryn via bobby) (Revision 1367114)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367114 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Common-trunk-Commit #2532 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2532/])     HADOOP-8627. FS deleteOnExit may delete the wrong path (daryn via bobby) (Revision 1367114)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367114 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2596 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2596/])     HADOOP-8627. FS deleteOnExit may delete the wrong path (daryn via bobby) (Revision 1367114)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367114 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Hdfs-0.23-Build #330 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/330/])     svn merge -c 1367114 FIXES: HADOOP-8627. FS deleteOnExit may delete the wrong path (daryn via bobby) (Revision 1367119)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1367119 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java"], "tasks": {"summarization": "FS deleteOnExit may delete the wrong path - {{FilterFileSystem}} is incorrectly delegating {{deleteOnExit}} to the raw underlying fs.  This is w...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8627 about?", "answer": "FS deleteOnExit may delete the wrong path"}}}
{"issue_id": "HADOOP-8626", "project": "HADOOP", "title": "Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user", "status": "Closed", "priority": "Major", "reporter": "Jonathan Natkins", "assignee": "Jonathan Natkins", "created": "2012-07-26T21:55:14.000+0000", "updated": "2012-10-11T17:45:06.000+0000", "labels": [], "description": "(&amp;(objectClass=user)(sAMAccountName={0}) should have a trailing parenthesis at the end", "comments": ["Updates core-default.xml with a valid default setting", "Marking PA for Natty.  The patch looks good to me. +1 pending Jenkins.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538085/HADOOP-8626.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1222//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1222//console  This message is automatically generated.", "I've just committed this to trunk and branch-2. Thanks a lot for the contribution, Natty.", "Integrated in Hadoop-Hdfs-trunk-Commit #2591 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2591/])     HADOOP-8626. Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user. Contributed by Jonathan Natkins. (Revision 1366410)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366410 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "Integrated in Hadoop-Common-trunk-Commit #2527 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2527/])     HADOOP-8626. Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user. Contributed by Jonathan Natkins. (Revision 1366410)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366410 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "Integrated in Hadoop-Mapreduce-trunk-Commit #2547 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2547/])     HADOOP-8626. Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user. Contributed by Jonathan Natkins. (Revision 1366410)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366410 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "Integrated in Hadoop-Hdfs-trunk #1118 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1118/])     HADOOP-8626. Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user. Contributed by Jonathan Natkins. (Revision 1366410)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366410 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "Integrated in Hadoop-Mapreduce-trunk #1150 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1150/])     HADOOP-8626. Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user. Contributed by Jonathan Natkins. (Revision 1366410)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366410 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/resources/core-default.xml"], "tasks": {"summarization": "Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user - (&amp;(objectClass=user)(sAMAccountName={0}) should have a trailing parenthesis at the end...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8626 about?", "answer": "Typo in default setting for hadoop.security.group.mapping.ldap.search.filter.user"}}}
{"issue_id": "HADOOP-8625", "project": "HADOOP", "title": "Use GzipCodec to decompress data in ResetableGzipOutputStream test", "status": "Resolved", "priority": "Major", "reporter": "Mike Percy", "assignee": "Mike Percy", "created": "2012-07-26T05:28:37.000+0000", "updated": "2016-08-05T23:30:21.000+0000", "labels": [], "description": "Use GzipCodec to decompress data in ResetableGzipOutputStream test.", "comments": ["Please see the following JIRA issue & comment for context: https://issues.apache.org/jira/browse/HADOOP-8522?focusedCommentId=13398854&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13398854"], "tasks": {"summarization": "Use GzipCodec to decompress data in ResetableGzipOutputStream test - Use GzipCodec to decompress data in ResetableGzipOutputStream test....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8625 about?", "answer": "Use GzipCodec to decompress data in ResetableGzipOutputStream test"}}}
{"issue_id": "HADOOP-8624", "project": "HADOOP", "title": "ProtobufRpcEngine should log all RPCs if TRACE logging is enabled", "status": "Closed", "priority": "Minor", "reporter": "Todd Lipcon", "assignee": "Todd Lipcon", "created": "2012-07-26T00:41:47.000+0000", "updated": "2016-05-12T18:24:09.000+0000", "labels": [], "description": "Since all RPC requests/responses are now ProtoBufs, it's easy to add a TRACE level logging output for ProtobufRpcEngine that actually shows the full content of all calls. This is very handy especially when writing/debugging unit tests, but might also be useful to enable at runtime for short periods of time to debug certain production issues.", "comments": ["This patch implements the above for the client side. I didn't instrument the server side with extra tracing as of yet, but we could do that separately. In the context of unit tests where I\"m using this, having one side of the connection is enough to be very useful.  It doesn't include new tests, but here is some example output:  {code} 2012-07-25 17:38:10,313 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(197)) - 46: Call -> null@/127.0.0.1:48275: getJournalState {jid { identifier: \"testQuorumJournalManager\" }} 2012-07-25 17:38:10,313 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(197)) - 45: Call -> null@/127.0.0.1:46462: getJournalState {jid { identifier: \"testQuorumJournalManager\" }} 2012-07-25 17:38:10,313 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(197)) - 47: Call -> null@/127.0.0.1:60394: getJournalState {jid { identifier: \"testQuorumJournalManager\" }} 2012-07-25 17:38:10,354 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(73)) - Initializing journal in directory build/test/data/dfs/journalnode-0/testQuorumJournalManager 2012-07-25 17:38:10,354 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(73)) - Initializing journal in directory build/test/data/dfs/journalnode-1/testQuorumJournalManager 2012-07-25 17:38:10,354 INFO  server.JournalNode (JournalNode.java:getOrCreateJournal(73)) - Initializing journal in directory build/test/data/dfs/journalnode-2/testQuorumJournalManager 2012-07-25 17:38:10,361 INFO  common.Storage (Storage.java:analyzeStorage(429)) - Storage directory /home/todd/git/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/build/test/data/dfs/journalnode-0/testQuorumJournalManager does not exist. 2012-07-25 17:38:10,361 INFO  common.Storage (Storage.java:analyzeStorage(429)) - Storage directory /home/todd/git/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/build/test/data/dfs/journalnode-1/testQuorumJournalManager does not exist. 2012-07-25 17:38:10,361 INFO  common.Storage (Storage.java:analyzeStorage(429)) - Storage directory /home/todd/git/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/build/test/data/dfs/journalnode-2/testQuorumJournalManager does not exist. 2012-07-25 17:38:10,363 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getJournalState took 240ms 2012-07-25 17:38:10,363 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getJournalState took 240ms 2012-07-25 17:38:10,363 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getJournalState took 240ms 2012-07-25 17:38:10,364 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(232)) - 47: Response <- null@/127.0.0.1:60394: getJournalState {lastPromisedEpoch: 0 httpPort: 60071} 2012-07-25 17:38:10,364 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(232)) - 45: Response <- null@/127.0.0.1:46462: getJournalState {lastPromisedEpoch: 0 httpPort: 48186} 2012-07-25 17:38:10,364 TRACE ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(232)) - 46: Response <- null@/127.0.0.1:48275: getJournalState {lastPromisedEpoch: 0 httpPort: 50970} {code}  The logs make it very clear what's going on under the hood, which helped me debug an issue.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537936/hadoop-8624.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      -1 javadoc.  The javadoc tool appears to have generated -2 warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1221//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1221//console  This message is automatically generated.", "+1, the patch looks good to me. I'm confident that the test failure is unrelated.  The javadoc warning is a little curious, though. Any explanation for that?", "The javadoc output must be unrelated - it says I fixed 2 warnings, but I didn't modify any javadoc in this patch at all. I'll commit this momentarily.", "This can be closed right?", "Yep, I think this was committed during one of the JIRA outages last month. IT was committed in branch-2@1366128 and trunk@1366127"], "tasks": {"summarization": "ProtobufRpcEngine should log all RPCs if TRACE logging is enabled - Since all RPC requests/responses are now ProtoBufs, it's easy to add a TRACE level logging output fo...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8624 about?", "answer": "ProtobufRpcEngine should log all RPCs if TRACE logging is enabled"}}}
{"issue_id": "HADOOP-8623", "project": "HADOOP", "title": "hadoop jar command should respect HADOOP_OPTS", "status": "Closed", "priority": "Minor", "reporter": "Steven Willis", "assignee": "Steven Willis", "created": "2012-07-25T22:29:49.000+0000", "updated": "2014-09-04T00:56:21.000+0000", "labels": [], "description": "The jar command to the hadoop script should use any set HADOOP_OPTS and HADOOP_CLIENT_OPTS environment variables like all the other commands.", "comments": ["Like HADOOP-7491 which added these environment variables to the case where the hadoop script is run with a classname, they should also be used when run with jar as the command. All the other commands use these env vars, I can't see why jar doesn't.", "Since every case in the big if block have would now had: {code}HADOOP_OPTS=\"$HADOOP_OPTS $HADOOP_CLIENT_OPTS\"{code}  I just removed each individual line and added a single line below the block. It has the nice side affect of making that big block more readable", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537914/HADOOP-8623.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1220//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1220//console  This message is automatically generated.", "the same is being repeated in hadoop-hdfs-project/hadoop-hdfs/src/main/bin/hdfs and hadoop-mapreduce-project/bin/mapred scripts. I think we can move assignment of HADOOP_OPTS to before the case statement and remove the line from various if statements in bin/mapred and bin/hdfs.", "Actually thinking about it more, hdfs and mapred could have separate configs and can be invoked independently from the bin/hadoop script so the current changes look good to go.  +1", "+1 ditto", "Thanks for contributing the patch Steven. I have added you as a contributor to Hadoop common. You can now assign the jiras to yourself.  I committed the patch to trunk. Will merge it into 2.x next.", "I merged this to branch-0.23 as well.", "Integrated in Hadoop-Hdfs-0.23-Build #388 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/388/])     svn merge -c 1366126 FIXES: HADOOP-8623. hadoop jar command should respect HADOOP_OPTS. Contributed by Steven Willis. (Revision 1391276)       Result = UNSTABLE jlowe : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1391276 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/bin/hadoop", "I am out of the office until Tuesday October 16th. Please contact Seth Madison for any urgent issues."], "tasks": {"summarization": "hadoop jar command should respect HADOOP_OPTS - The jar command to the hadoop script should use any set HADOOP_OPTS and HADOOP_CLIENT_OPTS environme...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8623 about?", "answer": "hadoop jar command should respect HADOOP_OPTS"}}}
{"issue_id": "HADOOP-8621", "project": "HADOOP", "title": "FileUtil.symLink fails if spaces in path", "status": "Resolved", "priority": "Minor", "reporter": "Robert Fuller", "assignee": "Andras Bokor", "created": "2012-07-25T14:22:32.000+0000", "updated": "2017-10-06T09:13:43.000+0000", "labels": [], "description": "the 'ln -s' command fails in the current implementation if there is a space in the path for the target or linkname. A small change resolves the issue.  String cmd = \"ln -s \" + target + \" \" + linkname; //Process p = Runtime.getRuntime().exec(cmd, null); //broken Process p = Runtime.getRuntime().exec(new String[]{\"ln\",\"-s\",target,linkname}, null);", "comments": ["example fix attached.", "Hey Robert. Thanks for noticing the bug and the fix. It looks good to me.  Would you mind adding a simple unit test in ./src/test/java/org/apache/hadoop/fs/TestFileUtil.java? It seems like it should be trivial to mkdir a directory with a space in the path and try to create a symlink.  Also, when you generate a patch, best to generate it relative to the SVN root so the automatic patch tester can build it. Thanks!", "Integrated in Hadoop-Hdfs-trunk-Commit #2586 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2586/])     Amend previous commit of HDFS-3626: accidentally included a hunk from HADOOP-8621 in svn commit. Reverting that hunk (Revision 1365817)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365817 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java", "Integrated in Hadoop-Common-trunk-Commit #2522 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2522/])     Amend previous commit of HDFS-3626: accidentally included a hunk from HADOOP-8621 in svn commit. Reverting that hunk (Revision 1365817)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365817 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2542 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2542/])     Amend previous commit of HDFS-3626: accidentally included a hunk from HADOOP-8621 in svn commit. Reverting that hunk (Revision 1365817)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365817 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java", "Adding patch with unit test and fix.", "Integrated in Hadoop-Hdfs-trunk #1116 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1116/])     Amend previous commit of HDFS-3626: accidentally included a hunk from HADOOP-8621 in svn commit. Reverting that hunk (Revision 1365817)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365817 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java", "Integrated in Hadoop-Mapreduce-trunk #1148 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1148/])     Amend previous commit of HDFS-3626: accidentally included a hunk from HADOOP-8621 in svn commit. Reverting that hunk (Revision 1365817)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365817 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} | | {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  2s{color} | {color:blue} The patch file was not named according to hadoop's naming conventions. Please see https://wiki.apache.org/hadoop/HowToContribute for instructions. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HADOOP-8621 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Issue | HADOOP-8621 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12537980/hadoop-8621.txt | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/12904/console | | Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |   This message was automatically generated.", "HADOOP-8562 does the same change. It should not be an issue now."], "tasks": {"summarization": "FileUtil.symLink fails if spaces in path - the 'ln -s' command fails in the current implementation if there is a space in the path for the targ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8621 about?", "answer": "FileUtil.symLink fails if spaces in path"}}}
{"issue_id": "HADOOP-8620", "project": "HADOOP", "title": "Add -Drequire.fuse and -Drequire.snappy", "status": "Closed", "priority": "Minor", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-24T21:56:33.000+0000", "updated": "2012-10-11T17:45:12.000+0000", "labels": [], "description": "We have some optional build components which don't get built if they are not installed on the build machine.  One of those components is fuse_dfs.  Another is the snappy support in libhadoop.so.  Unfortunately, since these components are silently ignored if they are not present, it's easy to unintentionally create an incomplete build.  We should add two flags, -Drequire.fuse and -Drequire.snappy, that do exactly what the names suggest.  This will make the build more repeatable.  Those who want a complete build can specify these system properties to maven.  If the build cannot be created as requested, it will be a hard error.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537762/HADOOP-8620.002.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.hdfs.TestDFSClientRetries                   org.apache.hadoop.hdfs.TestPersistBlocks      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1216//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1216//console  This message is automatically generated.", "The test failures are not related to the patch-- this is purely a build system thing.", "I've tried the *mvn clean test -DskipTests -Drequire.snappy=true -Dsnappy.prefix=/home/tucu/src/snappy-1.0.3/build/usr/local -Pnative* on Ubuntu 10.4 and the build is failing and I have snappy in the specified directory:  {code} /home/tucu/src/snappy-1.0.3/build/usr/local/ /home/tucu/src/snappy-1.0.3/build/usr/local/share /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/INSTALL /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/ChangeLog /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/COPYING /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/README /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/format_description.txt /home/tucu/src/snappy-1.0.3/build/usr/local/share/doc/snappy/NEWS /home/tucu/src/snappy-1.0.3/build/usr/local/include /home/tucu/src/snappy-1.0.3/build/usr/local/include/snappy.h /home/tucu/src/snappy-1.0.3/build/usr/local/include/snappy-stubs-public.h /home/tucu/src/snappy-1.0.3/build/usr/local/include/snappy-c.h /home/tucu/src/snappy-1.0.3/build/usr/local/include/snappy-sinksource.h /home/tucu/src/snappy-1.0.3/build/usr/local/lib /home/tucu/src/snappy-1.0.3/build/usr/local/lib/libsnappy.a /home/tucu/src/snappy-1.0.3/build/usr/local/lib/libsnappy.so /home/tucu/src/snappy-1.0.3/build/usr/local/lib/libsnappy.so.1 /home/tucu/src/snappy-1.0.3/build/usr/local/lib/libsnappy.la /home/tucu/src/snappy-1.0.3/build/usr/local/lib/libsnappy.so.1.1.1 {code}", "* remove -Dbundle.snappy because we don't support it.  This has been obsolete for a long, long time.  * remove runas.home.  runas was removed by HADOOP-8450, but we forgot to remove runas.home from the pom.xml file.  * fix -Dsnappy.prefix, -Dsnappy.lib, -Dsnappy.include.  They were broken by HADOOP-8368.", "+1. run build requiring with/without snappy, works as advertised.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538811/HADOOP-8620.003.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl                   org.apache.hadoop.hdfs.server.datanode.TestBlockReport                   org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1243//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1243//console  This message is automatically generated.", "Test failures are unrelated. I've committed this and merged to branch-2. Thanks Colin.", "Integrated in Hadoop-Hdfs-trunk-Commit #2613 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2613/])     HADOOP-8620. Add -Drequire.fuse and -Drequire.snappy. Contributed by Colin Patrick McCabe (Revision 1368251)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368251 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/pom.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt", "Integrated in Hadoop-Common-trunk-Commit #2548 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2548/])     HADOOP-8620. Add -Drequire.fuse and -Drequire.snappy. Contributed by Colin Patrick McCabe (Revision 1368251)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368251 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/pom.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt", "Integrated in Hadoop-Mapreduce-trunk-Commit #2566 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2566/])     HADOOP-8620. Add -Drequire.fuse and -Drequire.snappy. Contributed by Colin Patrick McCabe (Revision 1368251)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368251 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/pom.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt", "Integrated in Hadoop-Hdfs-trunk #1123 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1123/])     HADOOP-8620. Add -Drequire.fuse and -Drequire.snappy. Contributed by Colin Patrick McCabe (Revision 1368251)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368251 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/pom.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt", "Integrated in Hadoop-Mapreduce-trunk #1155 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1155/])     HADOOP-8620. Add -Drequire.fuse and -Drequire.snappy. Contributed by Colin Patrick McCabe (Revision 1368251)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1368251 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/pom.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/CMakeLists.txt"], "tasks": {"summarization": "Add -Drequire.fuse and -Drequire.snappy - We have some optional build components which don't get built if they are not installed on the build ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8620 about?", "answer": "Add -Drequire.fuse and -Drequire.snappy"}}}
{"issue_id": "HADOOP-8619", "project": "HADOOP", "title": "WritableComparator must implement no-arg constructor", "status": "Resolved", "priority": "Major", "reporter": "Radim Kolar", "assignee": "Christopher Douglas", "created": "2012-07-24T20:47:03.000+0000", "updated": "2016-05-12T18:22:53.000+0000", "labels": [], "description": "Because of reasons listed here: http://findbugs.sourceforge.net/bugDescriptions.html#SE_COMPARATOR_SHOULD_BE_SERIALIZABLE  comparators should be serializable. To make deserialization work, it is required that all superclasses have no-arg constructor. http://findbugs.sourceforge.net/bugDescriptions.html#SE_NO_SUITABLE_CONSTRUCTOR  Simply add no=arg constructor to  WritableComparator.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537747/writable-comparator.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1215//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1215//console  This message is automatically generated.", "Thanks for finding the issue. The patch should fix the problem.  Actually in Hadoop core components, instead of java Serialization, interface \"Writable\" is wildly used to do in-memory(or on disk persistence) serialization and Protobuf is used for remote communication.", "In Hadoop, the serialization/deserialization is implemented by Writable interface. I wonder whether there are some requirements to use the Java serialization system. If all the serialization system is accomplished by Writable interface, I think this issue is not necessary.", "{quote}..., I think this issue is not necessary.{quote} Agree to resolve it as \"won't fix\".", "Hadoop serialization framework aka Writable is not only possible way for serializing object. We use Thrift on top on Hadoop Writables.   Without serializable comparator its not possible to have serializable collections with ordering.", "@Radim Kolar There are different ways to serialize data include Writable, Thrift and Protocol Buffer. But I don't think it's necessary to serialize and transport comparator through the wire. Because in both sides of the communication, they should have the same code which include the comparator code. I think the comparator is part of the code rather than the data.", "There is difference between think and know. To make object serialization work, all referenced objects must be deserializable, otherwise there is no way to reconstruct object tree back. You can not deserialize object if you can no create it. Plain and simple.  Lets take another approach: Hadoop is using findbugs, isnt it? It means that you guys have some trust and confidence in findbugs authors. Then trust them if they are telling you that deserialization do not works if you cant call constructor.", "Java serialization do not capture constructor arguments into output stream. More detail about protocol is here http://docs.oracle.com/javase/6/docs/platform/serialization/spec/protocol.html  If you create own constructor, java will not create default zero arg constructor http://xahlee.info/java-a-day/constructer.html - section \"Java Technicality: Default Constructors\"", "Hi Radim,  Semantically, the patch is a good patch. It's not necessary to patch it for the following reason:  {quote}...all referenced objects must be deserializable, otherwise there is no way to reconstruct object tree back...{quote} We are aware of that. One of the reasons that Interface \"Writeable\" was used from the early days is it's simple/lightweight. If you are interested in how it's used, you can take a look of some examples, such as org.apache.hadoop.io.WritableComparable and the classes implementing it.  Usually it may not be needed to make an interface/class be able to support several serialization approaches when one of them is sufficient for the usage.  :-)  Additionally, leaving it not patched can prevent developers form using Java Serialization in Hadoop by mistake. With that said, we can always come back apply this patch when we see Java Serialization is a must in the future. Agree? :-)", "I know how to use Writeable interface. I coded about 110k lines in hadoop of code in last 8 months. Its unusable beyond easy stuff. If Writable is that good, why you are not using it for serializing communication in YARN. You use avro and protobuf. Isnt one serialization approach sufficient for usage? It is, but its not effective. Thats why you are not using Writable everywhere.  But my point is not to discuss if Writable sucks or not. Point is why you are trying on purpose put obstacles in using other serialization frameworks. You call it \"protection\" but its some kind of protection as you can get by communist government. It was proved by history that communist protection failed, it created sick society where everybody were poor.  Do not try to lock developers into one right thing. Give them freedom.  I do not have infinite time in hand. I could not wait years until you guys decide, maybe we should commit these 3 lines of code. I need to act now, my project needs it.", "I don't see this breaking anything, and it lets people chose how they serialise stuff, that's their choice.  +1", "Before committing this, I'm going to add one more bit of homework:  add a test that serializes then deserializes something of the std. writables.  Why? Your goal is to ser/deser things -the ctor is just a means to that end. Add a test of the desired behaviour and you can be confident that it doesn't break in future.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540087/8619-0.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1275//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1275//console  This message is automatically generated.", "+1 for the patch.  I committed the patch to trunk. Thank you Chris.  @Radim, Fix Version/s was set to 3.0.0, 2.0.0-alpha and 0.23. I am not sure if you intended that to indicate that you need this change on those branches. If you did, let me know.   Also, in future, to indicate other branches that need a change, use Target Version/s field.", "Integrated in Hadoop-Hdfs-trunk-Commit #2713 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2713/])     HADOOP-8619. WritableComparator must implement no-arg constructor. Contributed by Chris Douglas. (Revision 1378120)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1378120 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestWritableSerialization.java", "Integrated in Hadoop-Common-trunk-Commit #2649 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2649/])     HADOOP-8619. WritableComparator must implement no-arg constructor. Contributed by Chris Douglas. (Revision 1378120)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1378120 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestWritableSerialization.java", "I need fix in 0.23 and 2.1.0-alpha", "Committed the patch to 0.23 and branch-2 as well.", "Integrated in Hadoop-Mapreduce-trunk-Commit #2678 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2678/])     HADOOP-8619. WritableComparator must implement no-arg constructor. Contributed by Chris Douglas. (Revision 1378120)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1378120 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestWritableSerialization.java", "I need 2.1.0-alpha branch as well. I have plans to test it soon after i will fix compile error.", "Integrated in Hadoop-Hdfs-trunk #1149 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1149/])     HADOOP-8619. WritableComparator must implement no-arg constructor. Contributed by Chris Douglas. (Revision 1378120)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1378120 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestWritableSerialization.java", "Integrated in Hadoop-Mapreduce-trunk #1180 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1180/])     HADOOP-8619. WritableComparator must implement no-arg constructor. Contributed by Chris Douglas. (Revision 1378120)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1378120 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/WritableComparator.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestWritableSerialization.java"], "tasks": {"summarization": "WritableComparator must implement no-arg constructor - Because of reasons listed here: http://findbugs.sourceforge.net/bugDescriptions.html#SE_COMPARATOR_S...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8619 about?", "answer": "WritableComparator must implement no-arg constructor"}}}
{"issue_id": "HADOOP-8618", "project": "HADOOP", "title": "Windows build failing after 1.0.3 got merged into branch-1-win", "status": "Resolved", "priority": "Major", "reporter": "Bikas Saha", "assignee": "Bikas Saha", "created": "2012-07-24T19:01:29.000+0000", "updated": "2012-07-24T21:53:44.000+0000", "labels": [], "description": "", "comments": ["Issue with calling autoreconf. Fixed in patch.", "I ran native build on Linux. Both the targets you are invoking using antcall are run under the target compile-core-native. One minor comment - change the order of antcalls - do compile-core-classes first.  +1 with that change.", "patch attached. Thanks!", "I committed the patch."], "tasks": {"summarization": "Windows build failing after 1.0.3 got merged into branch-1-win - ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8618 about?", "answer": "Windows build failing after 1.0.3 got merged into branch-1-win"}}}
{"issue_id": "HADOOP-8617", "project": "HADOOP", "title": "backport pure Java CRC32 calculator changes to branch-1", "status": "Closed", "priority": "Major", "reporter": "Brandon Li", "assignee": "Brandon Li", "created": "2012-07-24T17:25:19.000+0000", "updated": "2012-10-17T18:27:25.000+0000", "labels": [], "description": "Multiple efforts have been made gradually to improve the CRC performance in Hadoop. This JIRA is to back port these changes to branch-1, which include HADOOP-6166, HADOOP-6148, HADOOP-7333.  The related HDFS and MAPREDUCE patches are uploaded to their original JIRAs HDFS-496 and MAPREDUCE-782.", "comments": ["I ran teragen/terasort/teravalidate tests with the branch-1 patches from this JIRA and that of HDFS-496 and MAPREDUCE-782.  Here is the result of test-patch with all the patches: {noformat} BUILD SUCCESSFUL Total time: 4 minutes 17 seconds  -1 overall.        +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 3 new or modified tests.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      -1 findbugs.  The patch appears to introduce 174 new Findbugs (version 2.0.0) warnings.{noformat}  The findbug warnings are not introduced by these patches.", "+1 patch looks good.", "I have committed this.  Thanks, Brandon!", "I merged this to branch-1.1 too.", "Closed upon release of Hadoop-1.1.0."], "tasks": {"summarization": "backport pure Java CRC32 calculator changes to branch-1 - Multiple efforts have been made gradually to improve the CRC performance in Hadoop. This JIRA is to ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8617 about?", "answer": "backport pure Java CRC32 calculator changes to branch-1"}}}
{"issue_id": "HADOOP-8616", "project": "HADOOP", "title": "ViewFS configuration requires a trailing slash", "status": "Closed", "priority": "Major", "reporter": "Eli Collins", "assignee": "Sandy Ryza", "created": "2012-07-24T03:00:09.000+0000", "updated": "2013-02-15T13:12:43.000+0000", "labels": [], "description": "If the viewfs config doesn't have a trailing slash commands like the following fail:  {noformat} bash-3.2$ hadoop fs -ls -ls: Can not create a Path from an empty string Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...] {noformat}  We hit this problem with the following configuration because \"hdfs://ha-nn-uri\" does not have a trailing \"/\".  {noformat}   <property>   <name>fs.viewfs.mounttable.foo.link./nameservices/ha-nn-uri</name>   <value>hdfs://ha-nn-uri</value>   </property> {noformat}", "comments": ["Thanks to Stephen Chu for identifying this bug.", "The problem occurs when the ViewFileSystem attempts to create a ChRootedFileSystem as its target file system.  It calls getPath() on the URI object passed to it, which, for a URI like \"hdfs://ha-nn-uri\", returns an empty string.  Where does it make the most sense to handle this?  Where the ViewFileSystem reads the config (in the InodeTree constructor)? Where the ViewFileSystem creates the URI (inside the createLink() method in InodeTree)? In the ChRootedFileSystem constructor?", "Based on my understanding, I'd saying {{ChRootedFileSystem}} is the best place for the fix.  Anywhere else would be working around its bug.  The handling of the chroot path has always been a bit complex so maybe this is a good time to simplify it.", "Is \"/\" the root of all filesystems, i.e. is [scheme]://[authority]/ always a valid path?", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12544264/HADOOP-8616.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1424//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1424//console  This message is automatically generated.", "The patch looks pretty good to me. Two little comments:  # I'd recommend using {{String#isEmpty()}} instead of {{String#length() == 0}}. # Does a similar change not need to be made to the FileContext side of the house, i.e. ChRootedFs?", "The ChRootedFs constructor takes a Path, not a URI, like the ChRootedFileSystem.  The same problem could occur if ViewFs were passed a URI without a trailing slash, but this would have to be handled in ViewFs.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12544499/HADOOP-8616.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 2 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1426//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1426//console  This message is automatically generated.", "+1, the latest patch looks good to me. I'm going to commit this momentarily.", "Right after I initially committed the patch, I realized that the new test file was missing the Apache license header. Here's an updated patch which just adds that missing license.  I've reverted my initial commit, and am going to commit this updated patch now, since it differs only in the comment.", "I've just committed this to trunk and branch-2.  Thanks a lot for the contribution, Sandy.", "Integrated in Hadoop-Common-trunk-Commit #2801 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2801/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392707) Revert an errant commit of HADOOP-8616. (Revision 1392705) HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392703)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392707 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsURIs.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392705 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392703 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2863 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2863/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392707) Revert an errant commit of HADOOP-8616. (Revision 1392705) HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392703)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392707 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsURIs.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392705 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392703 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2823 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2823/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392703)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392703 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2824 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2824/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392707) Revert an errant commit of HADOOP-8616. (Revision 1392705)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392707 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsURIs.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392705 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Hdfs-trunk #1183 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1183/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392707) Revert an errant commit of HADOOP-8616. (Revision 1392705) HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392703)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392707 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsURIs.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392705 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392703 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk #1214 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1214/])     HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392707) Revert an errant commit of HADOOP-8616. (Revision 1392705) HADOOP-8616. ViewFS configuration requires a trailing slash. Contributed by Sandy Ryza. (Revision 1392703)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392707 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsURIs.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392705 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java  atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1392703 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java"], "tasks": {"summarization": "ViewFS configuration requires a trailing slash - If the viewfs config doesn't have a trailing slash commands like the following fail:  {noformat} bas...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8616 about?", "answer": "ViewFS configuration requires a trailing slash"}}}
{"issue_id": "HADOOP-8615", "project": "HADOOP", "title": "EOFException in DecompressorStream.java needs to be more verbose", "status": "Patch Available", "priority": "Major", "reporter": "Jeff Lord", "assignee": null, "created": "2012-07-23T20:21:55.000+0000", "updated": "2015-05-06T03:31:26.000+0000", "labels": ["BB2015-05-TBR", "patch"], "description": "In ./src/core/org/apache/hadoop/io/compress/DecompressorStream.java  The following exception should at least pass back the file that it encounters this error in relation to:    protected void getCompressedData() throws IOException {     checkStream();      int n = in.read(buffer, 0, buffer.length);     if (n == -1) {       throw new EOFException(\"Unexpected end of input stream\");     }   This would help greatly to debug bad/corrupt files.", "comments": ["I can see how that would really help, and what a pain it is to try to guess which stream we're dealing with.  Unfortunately, DecompressorStream doesn't have any way to know the name of the file or even if the InputStream is a file. If it is, it's more than likely to be buffered such that DecompressorStream never \"sees\" the actual FileInputStream. ...and even if he did have the FileInputStream I'm not aware of any way to retrieve the filename.  We could try to make each caller that creates a DecompressorStream from a file log debug information regarding which file is being used, but how would we correlate this message with a particular DecompressionStream?  The best solution I can think of - Perhaps creators of DecompressionStream's should catch EOFException and then print out detailed debug information.", "bq. The best solution I can think of - Perhaps creators of DecompressionStream's should catch EOFException and then print out detailed debug information.  I agree with this. IMO, this would be the right way to do it. What expects it, ought to log/print it properly.  Jeff/Tim - Any chances of a patch with the above approach? For the MR side though, I've added in MAPREDUCE-3678 to at least know what file you were dealing with.", "Harsh, I'm under some pretty Draconian schedule pressure right now. I did take a look at this none the less, and it doesn't look like a quick job. Some issues to consider:   - Will all the codecs throw EOFException in this case, or will some throw IOException?  - What if a decompressor gets a consistency check? Should it hit this same case? Certainly it should throw IOException rather than EOFException.  - Clients include SequenceFile, TFile, offlineImageViewer, FSImageCompression, a few classes in mapred(uce), rumen, gridmix and various tests - no small patch.  - Soon we should address compression stream usage in general to address reducing copies. Lots of places we buffer the stream when the CompressionInputStream is already buffering, resulting in still more copies. If we need to pick through these one by one, perhaps that would be a good time to touch up error handling as well?  Sorry to wimp out on you, but it seems like a little much right now.", "As the descriptions says, we need the file name in the exception printed.I think of a simple way to pass the filename to the constructor of the DecompressorStream. For this a new overloaded constructor is to be created.  Then we can use the fileName in the exception message. As part of creating a new overloaded constructor, we would require to make a few changes also in some other files as well.  It worked for one of my implementations.", "I think I like thomastechs' idea. It has the advantage that it makes the codec figure out whether the problem is in the stream or not. It does mean patching every codec at once and then each client one by one. Really, in any scheme, we would need to be clear that the stream is the problem in any case, so touching the codecs seems unavoidable.  Are there issues for clients where they have problems knowing the stream source too? Does this ripple through a bunch of layers?", "Thanks Tim for your comments. I am going ahead creating the patch for my suggestion.", "Please perform the code review", "Created the patch based out of the latest code from git://git.apache.org/hadoop-common.git", "This patch contains the fix as follows.As per the bug says, the EOFExceptions thrown in the DecompressorStream does not provide any information about the file at which the decompression fails. I have added overloaded constructor and necessary methods, which will have the file name also added as parameter. When the user uses this method and pass the filename, it would be printed in the EOF exception thrown, if any.So I believe the test cases may not be necessary. I was able to test it locally by forcefully creating an EOF Exception and verifying the new message as \"java.io.EOFException: Unexpected end of input stream in the file = filename\"", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12543497/HADOOP-8615.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      -1 javac.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1397//console  This message is automatically generated.", "Please  do a code review", "Hi,  I have attached a new patch named:HADOOP-8615-release-0.20.2.patch This is to be tested against only release 0.20.2, not in trunk. The rest of the versions this may be incompatible.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12545288/HADOOP-8615-release-0.20.2.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1470//console  This message is automatically generated.", "The Hudson project again tests the patch against trunk. Please let me know if there is any procedure for making this tested only on hadoop common  release 0.20.2", "Thomas, Thank you for the patch!  bq. Please let me know if there is any procedure for making this tested only on hadoop common release 0.20.2  Don't worry about the robots testing against the wrong branch, you're not doing anything wrong.  It seems to me this change would be a good thing on trunk as well. Can you port the patch to trunk?  bq. When the user uses this method and pass the filename, it would be printed in the EOF exception thrown, if any. So I believe the test cases may not be necessary. I was able to test it locally by forcefully creating an EOF Exception and verifying the new message as \"java.io.EOFException: Unexpected end of input stream in the file = filename\"  I think this should be fairly easy to test -- just write a compressed stream, truncate the compressed stream, then try to read it, catch the EOFException and verify that the filename shows up in the exception text.  Or am I missing something?  I'm a little worried about the places where your {{fileName}}-using methods add new default values, for example: {code} +  public CompressionInputStream createInputStream(InputStream in,  +    Decompressor decompressor, String fileName)  +  throws IOException { +    return new DecompressorStream(in, decompressor,  +               conf.getInt(\"io.file.buffer.size\", 4*1024),fileName); +  } {code} I'll have to think about it longer, but having a default value of 4k hidden in this method seems wrong to me at a first glance.  There are a few other instances of this as well.", "Thanks Andy for your valuable comments. I will work on the patch in the trunk too.Also on the test case. The methods having default values were already existing. I used the same methods and overloaded with an added parameter of fileName.This will not affect existing functionalities or the people who already used the existing methods without a fileName.The new methods with fileName , helps the user to use it, if the user faces the issue addressed in this bug. Please let me know your comments.", "Cancelling PA status as there's some comments to address (and patch must target trunk).  You can look at http://wiki.apache.org/hadoop/QwertyManiac/BuildingHadoopTrunk for some easy commands to get trunk building.", "Given the target version as 2.0.0", "latest patch for trunk", "Please consider the HADOOP-8615 patch to be tested against the trunk.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12550195/HADOOP-8615.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:red}-1 javac{color:red}.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1656//console  This message is automatically generated.", "Hi,  Could anyone help me out, by reviewing what exactly is the reason for the build failure for my patch,named: HADOOP-8615.patch It is not producing any compile errors in my local.  Thanks,  Thomas.", "With your patch I get the following compilation error: {noformat} [INFO] --- maven-compiler-plugin:2.5.1:testCompile (default-testCompile) @ hadoop-common --- [INFO] Compiling 322 source files to /Users/suresh/Documents/workspace/hadoop.committer/hadoop-common-project/hadoop-common/target/test-classes [INFO] ------------------------------------------------------------- [ERROR] COMPILATION ERROR :  [INFO] ------------------------------------------------------------- [ERROR] /Users/suresh/Documents/workspace/hadoop.committer/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestCodecFactory.java:[32,17] org.apache.hadoop.io.compress.TestCodecFactory.BaseCodec is not abstract and does not override abstract method createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,java.lang.String) in org.apache.hadoop.io.compress.CompressionCodec {noformat}", "Thanks Suresh for taking a look. Your inputs helped me.", "Attaching the latest patch HADOOP-8615.Please review the same.This needs to be applied at the trunk", "submitting the new patch", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12550994/HADOOP-8615.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1677//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1677//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1677//console  This message is automatically generated.", "Thomas,  Thank you for working on this!  I've been annoyed by this unhelpful error message before.  The findbugs complaint seems legit: {code} Correctness Warnings Code \tWarning MF \tField BlockDecompressorStream.fileName masks field in superclass org.apache.hadoop.io.compress.DecompressorStream {code} Please fix the coding style throughout the patch: * you have leftover unused comments like \";//\" at the end of lines * always put a space after , in argument lists, for example {{decompress(buf,0,10);}} but there are many occurrences in the patch. * in {{if}} tests, always put exactly space before ( and { and around operators.  For example {{if(null !=  this.fileName){ }} has one extra space after {{!=}} and is missing spaces before ( and {. * properly indent continuation lines.  Use vim or emacs or eclipse for automatic indentation if necessary. * exactly one space around {{else}}, you have }else{ in several places. * in {{testBlockDecompress}} you want to {{fail(\"did not raise expected exception\")}} after calling {{.decompress}}. * please fill in javadoc {{\\@param}} entries, or delete them.  The patch is looking good, almost all the above is just cosmetic.  Again, thanks for the code!", "Thanks Andy for the comments.I have incorporated your comments and attaching a new patch.Please let me know any comments,", "Attaching the patch named HADOOP-8615-ver2.patch, incorporating Andy's comments.Please review and let me know.", "New patch with the review comments, named HADOOP-8615-ver2.patch", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12551056/HADOOP-8615-ver2.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1679//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1679//console  This message is automatically generated.", "Hi, Please let me know for any feedback. Requesting for further action. Thanks,  Thomas.", "bq. Please let me know for any feedback.  Sorry for the delay!  A few more whitespace fixups: - make sure {{) throws}} has a space between the ) and \"throws\". (2 examples in this patch) - still a few argument lists missing a space after \",\" for example {{return createInputStream(in, null,fileName);}}. - also a few argument lists with extra spaces before \",\" for example {{Decompressor decompressor , String fileName}} - extra space in {{protected  String fileName}} - extra space in {{this.fileName =  fileName}} - missing spaces in {{\\+\"file = \"\\+this.fileName}}, always put spaces on both sides of \"\\+\" and other operators. also we generally put the \"+\" on the previous line for a string continuation like this one. - missing space in {{if ((b1 | b2 | b3 | b4) < 0\\)\\{}} before \"{\" - missing space in {{String fileName =\"fileName\";}} after \"=\"  Thanks again for working on this enhancement!", "Thanks Andy for the review. To incorporate these fixes, should I take the latest from the trunk again, since it is 1 week past now.? Or shall I edit these space fixes in the patch itself.? I am new to this JIRA process.So, please let me know any thoughts. Thanks, Thomas.", "bq. To incorporate these fixes, should I take the latest from the trunk again,  Good question.  Your patch will be applied against trunk when it's committed, so in some cases you will need to make changes to merge with trunk, and there's never a downside to refreshing your patch against trunk.  From the patch file formatting it looks like you're using svn, so you can probably just \"svn up\" and resolve any merge conflicts.  I use git, and use \"git pull --rebase\" to get a similar effect on my working branches.", "BTW, the wiki page http://wiki.apache.org/hadoop/HowToContribute is supposed to answer these questions, but it doesn't currently answer them very well I think.  If you're willing to contribute to the wiki, some improvements there would be helpful to everybody! :)", "Thanks Andy for the review. I am incorporating your review comments and attching the new patch.", "New patch incorporated with Andy's fix", "Resolved coding standard issues mentioned in the Andy's review", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12552853/HADOOP-8615-ver3.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 2 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1726//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1726//console  This message is automatically generated.", "Please let me know for any updates", "The latest version of the patch looks great. +1.", "Hi,  Please treat this as a gentle reminder on further procedures. Thanks,  Thomas.", "Hi,  I noted that this changes the CompressionCodec interface, which would make it an incompatible change for its users (as older code, downstream, would fail to compile as they now may be missing a few method implementations).  Is it absolutely necessary to break compatibility to have just some information over this exception?", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12552853/HADOOP-8615-ver3.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6267/console |   This message was automatically generated."], "tasks": {"summarization": "EOFException in DecompressorStream.java needs to be more verbose - In ./src/core/org/apache/hadoop/io/compress/DecompressorStream.java  The following exception should ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8615 about?", "answer": "EOFException in DecompressorStream.java needs to be more verbose"}}}
{"issue_id": "HADOOP-8614", "project": "HADOOP", "title": "IOUtils#skipFully hangs forever on EOF", "status": "Closed", "priority": "Minor", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-23T18:30:47.000+0000", "updated": "2012-10-11T17:45:04.000+0000", "labels": [], "description": "IOUtils#skipFully contains this code:  {code}   public static void skipFully(InputStream in, long len) throws IOException {     while (len > 0) {       long ret = in.skip(len);       if (ret < 0) {         throw new IOException( \"Premature EOF from inputStream\");       }       len -= ret;     }   } {code}  The Java documentation is silent about what exactly skip is supposed to do in the event of EOF.  However, I looked at both InputStream#skip and ByteArrayInputStream#skip, and they both simply return 0 on EOF (no exception).  So it seems safe to assume that this is the standard Java way of doing things in an InputStream.  Currently IOUtils#skipFully will loop forever if you ask it to skip past EOF!", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537591/HADOOP-8614.001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1210//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1210//console  This message is automatically generated.", "+1 looks good, test failure is unrelated.", "Integrated in Hadoop-Common-trunk-Commit #2607 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2607/])     HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375216)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375216 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2671 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2671/])     HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375216)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375216 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2636 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2636/])     HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375216)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375216 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java", "Integrated in Hadoop-Hdfs-trunk #1141 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1141/])     HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375216)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375216 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java", "Integrated in Hadoop-Mapreduce-trunk #1173 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1173/])     HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375216)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375216 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java", "I pulled this into 0.23.3 too.", "Integrated in Hadoop-Hdfs-0.23-Build #351 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/351/])     svn merge -c 1375216 FIXES: HADOOP-8614. IOUtils#skipFully hangs forever on EOF. Contributed by Colin Patrick McCabe (Revision 1375572)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375572 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestIOUtils.java"], "tasks": {"summarization": "IOUtils#skipFully hangs forever on EOF - IOUtils#skipFully contains this code:  {code}   public static void skipFully(InputStream in, long le...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8614 about?", "answer": "IOUtils#skipFully hangs forever on EOF"}}}
{"issue_id": "HADOOP-8613", "project": "HADOOP", "title": "AbstractDelegationTokenIdentifier#getUser() should set token auth type", "status": "Closed", "priority": "Critical", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-23T18:02:49.000+0000", "updated": "2016-05-12T18:23:06.000+0000", "labels": [], "description": "{{AbstractDelegationTokenIdentifier#getUser()}} returns the UGI associated with a token.  The UGI's auth type will either be SIMPLE for non-proxy tokens, or PROXY (effective user) and SIMPLE (real user).  Instead of SIMPLE, it needs to be TOKEN.", "comments": ["+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537606/HADOOP-8613.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1212//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1212//console  This message is automatically generated.", "Don't you need to pull the code out of JspHelper.getUGI that also sets the authentication method to TOKEN?", "I did that in the linked HDFS-3553 but I can pull it back into this patch too if you'd like.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537900/HADOOP-8613-2.branch-1.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1219//console  This message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537895/HADOOP-8613-2.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:                    org.apache.hadoop.hdfs.TestDFSClientRetries      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1218//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1218//console  This message is automatically generated.", "Test failure is unrelated.", "+1", "Integrated in Hadoop-Hdfs-trunk-Commit #2592 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2592/])     HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366440)       Result = SUCCESS daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366440 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "Integrated in Hadoop-Common-trunk-Commit #2528 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2528/])     HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366440)       Result = SUCCESS daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366440 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "Thanks Owen!  I've committed it.", "Integrated in Hadoop-Mapreduce-trunk-Commit #2548 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2548/])     HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366440)       Result = FAILURE daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366440 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "Integrated in Hadoop-Hdfs-0.23-Build #327 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/327/])     svn merge -c 1366440 FIXES: HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366462)       Result = SUCCESS daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366462 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/branches/branch-0.23/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "Integrated in Hadoop-Hdfs-trunk #1118 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1118/])     HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366440)       Result = FAILURE daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366440 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "Integrated in Hadoop-Mapreduce-trunk #1150 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1150/])     HADOOP-8613. AbstractDelegationTokenIdentifier#getUser() should set token auth type. (daryn) (Revision 1366440)       Result = FAILURE daryn : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1366440 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java", "added 1.2.0 to fixVersion, per CHANGES.txt and above comments"], "tasks": {"summarization": "AbstractDelegationTokenIdentifier#getUser() should set token auth type - {{AbstractDelegationTokenIdentifier#getUser()}} returns the UGI associated with a token.  The UGI's ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8613 about?", "answer": "AbstractDelegationTokenIdentifier#getUser() should set token auth type"}}}
{"issue_id": "HADOOP-8612", "project": "HADOOP", "title": "Backport HADOOP-8599 to branch-1 (Non empty response when read beyond eof)", "status": "Closed", "priority": "Major", "reporter": "Matthew Foley", "assignee": "Eli Collins", "created": "2012-07-23T07:51:37.000+0000", "updated": "2013-05-15T05:16:04.000+0000", "labels": [], "description": "When FileSystem.getFileBlockLocations(file,start,len) is called with \"start\" argument equal to the file size, the response is not empty. See HADOOP-8599 for details and tiny patch.", "comments": ["Patch attached. Same as the trunk version.", "+1", "I've committed this, thanks for the review Todd.", "Closed upon release of Hadoop 1.2.0."], "tasks": {"summarization": "Backport HADOOP-8599 to branch-1 (Non empty response when read beyond eof) - When FileSystem.getFileBlockLocations(file,start,len) is called with \"start\" argument equal to the f...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8612 about?", "answer": "Backport HADOOP-8599 to branch-1 (Non empty response when read beyond eof)"}}}
{"issue_id": "HADOOP-8611", "project": "HADOOP", "title": "Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails", "status": "Closed", "priority": "Major", "reporter": "Kihwal Lee", "assignee": "Robert Parker", "created": "2012-07-20T13:41:53.000+0000", "updated": "2013-05-06T03:30:07.000+0000", "labels": [], "description": "When the JNI-based users-group mapping is enabled, the process/command will fail if the native library, libhadoop.so, cannot be found. This mostly happens at client-side where users may use hadoop programatically. Instead of failing, falling back to the shell-based implementation will be desirable. Depending on how cluster is configured, use of the native netgroup mapping cannot be subsituted by the shell-based default. For this reason, this behavior must be configurable with the default being \"disabled\".", "comments": ["This patch applies to trunk.  Once reviewed I will post the 1.0 patch", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541046/HADOOP-8611.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1304//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1304//console  This message is automatically generated.", "core-tests failures were time out issues, verified locally on trunk and my branch that TestZKFailoverController tests pass.", "TestZKFailoverController timing out is a known issue, see HADOOP-8591.", "The code looks good, but I have a few comments.   # There are tabs in the patch, please update it to use spaces instead.  # The way the patch is getting the impl instance is very repetative.  I think it would be simpler to do the following {code} Class<GroupMappingServiceProvider> clazz = conf.getClass(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,                                   ShellBasedUnixGroupsMapping.class,                                   GroupMappingServiceProvider.class); if (conf.getBoolean(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING_ALLOW_FALLBACK, false) &&   !NativeCodeLoader.isNativeCodeLoaded()) {   LOG.info(\"Falling back to Shell Based Groups\");   clazz = ShellBasedUnixGroupsMapping.class; } impl = ReflectionUtils.newInstance(clazz, conf); {code}  # It would be good to update ./hadoop-hdfs-project/hadoop-hdfs/src/main/docs/src/documentation/content/xdocs/hdfs_permissions_guide.xml with the new config option  # Do we want to check explicitly for JniBasedUnixGroupsMapping and JniBasedUnixGroupsNetgroupMapping? or perhaps move some of this code over into those classes explicitly instead?  It seems like configuring LdapGroupsMapping with fallback enabled and non-native code would never work.  Also there would be issues with JniBasedUnixGroupsNetgroupMapping and fallback.  Is this the reason for having the fallback enable?", "Uploaded new patch, isolated the fallback functionality to 2 new specific classes which can be specified within the current configuration scheme.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541230/HADOOP-8611.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1319//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1319//console  This message is automatically generated.", "Patch for branch-1.0.2", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541277/HADOOP-8611-branch1.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in .      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1321//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1321//console  This message is automatically generated.", "I have two comments.   # {code}if (LOG.isDebugEnabled())       LOG.debug(\"Group mapping impl=\" + impl.getClass().getName()); {code} needs curly braces around the if body.    # The 1.0 branch refers to running mvn test, but 1.0 does not support mvn.  Please either update them or remove them.", "removed mvn references", "added curly braces", "added a new patch for branch 1 to remove mvn references and added curly braces to if statement added a new patch for trunk to add curly braces to if statement", "+1 the changes look good I'll check it in.", "It might be good to consider filing a separate JIRA to make the default groups implementation the JNI shell with fallback.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541645/HADOOP-8611.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1330//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1330//console  This message is automatically generated.", "Thanks Rob,  I checked this into trunk, branch-2, branch-2.1.0-alpha and branch-0.23.", "Integrated in Hadoop-Common-trunk-Commit #2607 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2607/])     HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375221)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375221 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2671 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2671/])     HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375221)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375221 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2636 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2636/])     HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375221)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375221 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "Integrated in Hadoop-Hdfs-0.23-Build #350 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/350/])     svn merge -c 1375221 FIXES: HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375224)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375224 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "Integrated in Hadoop-Hdfs-trunk #1141 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1141/])     HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375221)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375221 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "Integrated in Hadoop-Mapreduce-trunk #1173 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1173/])     HADOOP-8611. Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails (Robert Parker via bobby) (Revision 1375221)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1375221 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestGroupFallback.java", "added 1.2.0 to fixVersion, per CHANGES.txt and above comments."], "tasks": {"summarization": "Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails - When the JNI-based users-group mapping is enabled, the process/command will fail if the native libra...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8611 about?", "answer": "Allow fall-back to the shell-based implementation when JNI-based users-group mapping fails"}}}
{"issue_id": "HADOOP-8610", "project": "HADOOP", "title": "test-patch should run tests in the root repo", "status": "Resolved", "priority": "Blocker", "reporter": "Eli Collins", "assignee": null, "created": "2012-07-19T22:33:36.000+0000", "updated": "2014-07-31T21:15:09.000+0000", "labels": [], "description": "The test patch target should run tests from root projets, eg hadoop-tools. Otherwise we miss patches that introduce test failures, eg HDFS-3690.", "comments": ["Likely stale."], "tasks": {"summarization": "test-patch should run tests in the root repo - The test patch target should run tests from root projets, eg hadoop-tools. Otherwise we miss patches...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8610 about?", "answer": "test-patch should run tests in the root repo"}}}
{"issue_id": "HADOOP-8609", "project": "HADOOP", "title": "IPC server logs a useless message when shutting down socket", "status": "Closed", "priority": "Major", "reporter": "Todd Lipcon", "assignee": "Jon Zuanich", "created": "2012-07-19T17:44:43.000+0000", "updated": "2012-10-11T17:45:10.000+0000", "labels": ["newbie"], "description": "I occasionally see this WARN message out of the NameNode: {code} 12/07/19 10:37:37 WARN ipc.Server: Ignoring socket shutdown exception {code} with no further details. This message isn't useful - it either needs to have more contextual information (eg what client it's talking about and the exception ignored), or should be reduced to debug level.", "comments": ["Here's a patch Todd. Hugs", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537232/hadoop-8609.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1209//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1209//console  This message is automatically generated.", "The test failure is unrelated. No tests needed since this is just a simple logging change.  +1, I've just committed committed this to trunk and branch-2.", "Integrated in Hadoop-Common-trunk-Commit #2509 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2509/])     HADOOP-8609. IPC server logs a useless message when shutting down socket. Contributed by Jon Zuanich. (Revision 1363950)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363950 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2574 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2574/])     HADOOP-8609. IPC server logs a useless message when shutting down socket. Contributed by Jon Zuanich. (Revision 1363950)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363950 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2530 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2530/])     HADOOP-8609. IPC server logs a useless message when shutting down socket. Contributed by Jon Zuanich. (Revision 1363950)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363950 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java", "Integrated in Hadoop-Hdfs-trunk #1111 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1111/])     HADOOP-8609. IPC server logs a useless message when shutting down socket. Contributed by Jon Zuanich. (Revision 1363950)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363950 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java", "Integrated in Hadoop-Mapreduce-trunk #1143 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1143/])     HADOOP-8609. IPC server logs a useless message when shutting down socket. Contributed by Jon Zuanich. (Revision 1363950)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1363950 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java"], "tasks": {"summarization": "IPC server logs a useless message when shutting down socket - I occasionally see this WARN message out of the NameNode: {code} 12/07/19 10:37:37 WARN ipc.Server: ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8609 about?", "answer": "IPC server logs a useless message when shutting down socket"}}}
{"issue_id": "HADOOP-8608", "project": "HADOOP", "title": "Add Configuration API for parsing time durations", "status": "Closed", "priority": "Minor", "reporter": "Todd Lipcon", "assignee": "Christopher Douglas", "created": "2012-07-19T00:58:27.000+0000", "updated": "2016-02-23T23:49:39.000+0000", "labels": [], "description": "Hadoop has a lot of configurations which specify durations or intervals of time. Unfortunately these different configurations have little consistency in units - eg some are in milliseconds, some in seconds, and some in minutes. This makes it difficult for users to configure, since they have to always refer back to docs to remember the unit for each property.  The proposed solution is to add an API like {{Configuration.getTimeDuration}} which allows the user to specify the units with a postfix. For example, \"10ms\", \"10s\", \"10m\", \"10h\", or even \"10d\". For backwards-compatibility, if the user does not specify a unit, the API can specify the default unit, and warn the user that they should specify an explicit unit instead.", "comments": ["That would be a great API to add in. +1 for this idea.    In the yarn code we standardized to put the units in the name of the config property i.e. yarn.am.liveness-monitor.expiry-interval-ms and yarn.resourcemanager.application-tokens.master-key-rolling-interval-secs.  We probably want to be able to supply a default unit when getting a config so we can deprecate/replace an old property and have both still be valid.", "+1", "I am thinking the API can be  {code} // return the property value in millisecond.  The property value can be specified as \"10ms\", \"10s\", \"10m\", \"10h\", or \"10d\" public long getTimeDurationInMillisecond(String name, long defaultValue); {code}  After that developers can use methods like {code} java.util.concurrent.TimeUnit.MILLISECONDS.toSeconds(long duration) {code} to convert to duration in second or other time units.  However, the conversion from TimeUnit may lost precision, e.g., 70 seconds will be converted to 1 minute.  Any thoughts?", "{code} public void setTimeDuration(String name, long value, TimeUnit unit); public long getTimeDuration(String name, long defaultValue, TimeUnit unit); {code}  It warns when the unit is unspecified and assumes whatever the caller requested.", "Comments as follows:  * I am kind of concerned about the \"lose precision\" from the conversion.  That is why I propose to give that control back to the caller to make the precision lose explicitly.  If we go this approach, at least I think we should document this precision lose.  Comments? * The parsing of value is a bit loose.  For example, it cannot handle \"10S\" or \"10 s\".  A strict format can reduce errors but may be inflexible and the exception can be little harsh.  Or we need to document the expecting format is \"10s\" not \"10 s\" nor \"10sec\". * Also the parsing part relies on the enum order implicitly.  It works now.  But it may bite us later.  A Pattern instead? * It would be better to add the value of the timeduration property into LOG. * Can you please change test from \"testTime\" to \"testTimeDuration\" for consistency?  {code}     conf.setStrings(\"test.time.str\", new String[]{\"10S\"});     assertEquals(10000L, conf.getTimeDuration(\"test.time.str\", 30, MILLISECONDS)); {code}  It logs  2012-10-04 00:29:37,172 WARN  conf.Configuration (Configuration.java:getTimeDuration(1212)) - No unit for test.time.str assuming MILLISECONDS  Better as \"No unit for test.time.str (value: 10S), assuming MILLISECONDS\" or something like that with property value", "bq. I am kind of concerned about the \"lose precision\" from the conversion. That is why I propose to give that control back to the caller to make the precision lose explicitly. If we go this approach, at least I think we should document this precision lose. Comments?  The caller has exactly the same control in both models; the difference is whether the user should be notified because they misinterpreted the knob. A warning when a conversion loses some significant fraction of the value could be helpful, but is this common? When someone makes a typo and sets a timeout as 10d instead of 10s, that seems more worthy of a warning, but the caller knows better than the config if the value is in range.  It's a matter of taste, but I'd prefer to leave it with the caller. That OK?  bq. The parsing of value is a bit loose. For example, it cannot handle \"10S\" or \"10 s\". A strict format can reduce errors but may be inflexible and the exception can be little harsh. Or we need to document the expecting format is \"10s\" not \"10 s\" nor \"10sec\"  The format is documented and I prefer strict units, but feel free to make the matching fuzzier.  bq. Also the parsing part relies on the enum order implicitly. It works now. But it may bite us later. A Pattern instead?  Bite how? The enum order is part of the spec. The parsing is dead-simple. This is a trivial feature; a complex implementation is unlikely to justify itself...  bq. It would be better to add the value of the timeduration property into LOG. bq. Can you please change test from \"testTime\" to \"testTimeDuration\" for consistency?  Sure.", "I am looking for some document like in TimeUnit.convert that includes examples and warning. Not warning in the code.  {code} /**   * Return time duration in the given time unit. Valid units are encoded in   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds   * (ms), seconds (s), minutes (m), hours (h), and days (d).     * For example, the value can be 10ns, 10us, 10ms, 10s, and etc.    * Note getting time duration set in finer granularity as coarser granularity can lose precision.     * For example, if property \"example.duration\" is set to 999ms,    * <tt>getTimeDuration(\"example.duration\", 1L, TimeUnit.SECONDS)</tt> returns 0. {code}  Some other minor points: * In {{ParsedTimeDuration.unitFor}} the {{return null;}} is never executed. * In the same method, you used \"pdt\". I guess you mean \"ptd\".", "The caller requests a unit and the function returns {{long}}. Seems pretty straightforward.  bq. In ParsedTimeDuration.unitFor the return null; is never executed.  It's required.  bq. In the same method, you used \"pdt\". I guess you mean \"ptd\".  ...", "I like the new API.  I do have a few minor comments, none of which I see as blocking this from going in.  The first is to not use StringBuilder for the warning.  It seems cleaner and more compact to have   {code} LOG.warn(\"No unit for \" + name + \" (\" + vStr +\") assuming \"    + unit); {code} over {code} StringBuilder sb = new StringBuilder(\"No unit for \")   .append(name).append(\" (\").append(vStr).append(\") assuming \")   .append(unit.toString()); LOG.warn(sb.toString()); {code}  I would also like to see more lenient parsing, but that is something that can come later as the need shows up.  I can see people wanting to write \"1.5 days\" over \"36h\".  But like I said before this is not a big deal because \"36h\" is a whole lot easier to understand and less error prone compared to 129600000.", "Updated patch.  More complex parsing- units, fractions, composites like 1d3h2s, etc.- seems like overkill for the use cases we have.", "But again: it's a matter of taste, so if someone wants to be more thorough, they should feel free take point. It may change the return type and we don't need multiple time-oriented APIs to configuration vars.", "I like the latest patch and I am +1 on it assuming that test-patch comes back OK with it.  I don't really see much of a use case where the API would need to change. If I specify 1d3h2s or 1.5 days in general with java I am probably going to be requesting the result in ms because that is what most time oriented APIs in java take.  Perhaps seconds for a few configs because that is the default units for the config.  Yes, the newer APIs take a TimeUnit as well but why request it in days if I know it is going to lose precision and make it potentially harder to write unit tests for? no one wants to wait around for 1 day in a unit test for something to happen.  If someone really wants a more thorough format we can add it in in a separate JIRA so long as it maintains backwards compatibility with the current format.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12548457/8608-2.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1597//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1597//console  This message is automatically generated.", "I committed this.", "Integrated in Hadoop-trunk-Commit #3703 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3703/])     HADOOP-8608. Add Configuration API for parsing time durations. (Revision 1477869)       Result = SUCCESS cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1477869 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Yarn-trunk #200 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/200/])     HADOOP-8608. Add Configuration API for parsing time durations. (Revision 1477869)       Result = SUCCESS cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1477869 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-trunk #1389 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1389/])     HADOOP-8608. Add Configuration API for parsing time durations. (Revision 1477869)       Result = FAILURE cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1477869 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Mapreduce-trunk #1416 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1416/])     HADOOP-8608. Add Configuration API for parsing time durations. (Revision 1477869)       Result = FAILURE cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1477869 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "[~chris.douglas] This would be a good change to merge to branch-2 and branch-2.1.0-beta.", "bq. This would be a good change to merge to branch-2 and branch-2.1.0-beta.  Soright; merged back", "Integrated in Hadoop-trunk-Commit #3986 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3986/])     Move HADOOP-8608 to branch-2.1 (Revision 1494824)       Result = SUCCESS cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1494824 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Yarn-trunk #246 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/246/])     Move HADOOP-8608 to branch-2.1 (Revision 1494824)       Result = FAILURE cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1494824 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1436 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1436/])     Move HADOOP-8608 to branch-2.1 (Revision 1494824)       Result = FAILURE cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1494824 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1463 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1463/])     Move HADOOP-8608 to branch-2.1 (Revision 1494824)       Result = SUCCESS cdouglas : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1494824 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt"], "tasks": {"summarization": "Add Configuration API for parsing time durations - Hadoop has a lot of configurations which specify durations or intervals of time. Unfortunately these...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8608 about?", "answer": "Add Configuration API for parsing time durations"}}}
{"issue_id": "HADOOP-8607", "project": "HADOOP", "title": "Replace references to \"Dr Who\" in codebase with @BigDataBorat", "status": "Resolved", "priority": "Minor", "reporter": "Steve Loughran", "assignee": "Sanjay Radia", "created": "2012-07-18T23:17:06.000+0000", "updated": "2021-01-21T13:39:25.000+0000", "labels": [], "description": "People complain that having \"Dr Who\" in the code causes confusion and isn't appropriate in Hadoop now that it has matured.  I propose that we replace this anonymous user ID with {{@BigDataBorat}}. This will # Increase brand awareness of @BigDataBorat and their central role in the Big Data ecosystem. # Drive traffic to twitter, and increase their revenue. As contributors to the Hadoop platform, this will fund further Hadoop development.  Patching the code is straightforward; no easy tests, though we could monitor twitter followers to determine rollout of the patch in the field.", "comments": ["Assigning to Sanjay", "Steve, I have reason to believe your system clock may have substantially deviated from UTC, perhaps due to the recent leap second bug. My estimates put your deviation at approximately 9347387 seconds (~108 days). Could you please verify your NTP setup? Thanks.", "@Sanjay you should put it in your TODO list - https://twitter.com/DEVOPS_BORAT/status/223556198866235393", "We could make it configurable with a property {code} <property>  <name>borat.id</name>  <value>@BigDataBorat</value> </property> {code}  This would let people reassign the borat references to @Devops_Borat without rebuilding anything", "Small suggestion - key name should be {{hadoop.borat.id}}", "An enthusiastic +1 for making it configurable. I am totally for configurability, so an enterprise-ready Hadoop distro should be able to do <value></value>.", "@Milind, \"\" is an invalid borat configuration. Every technology needs Borat. All we are proposing here is make choice of Borat pluggable, so you can replace with your own version.", "@Steve, that works too (Need to confirm with product management;-). The Enterprise distributions can have a managed @<EnterpriseName>Borat twitter account substituted there. In fact, this gives such distros an ability to brand their distros for specific customers, such as @AcmeCorpBorat, and charge extra for it. (The *Borat accounts will essentially be retweeting @BigDataBorat, and charging premium customers for this value-ad, thus increasing profit margins. Love it!)", "@Milind -how are you going to test that? Will the property be read and then resolved to a URL under http://twitter.com/ ? How will that work offline, or when there is a proxy in the way.  Please supply test code for review", "We'd like to backport this to all known Hadoop versions in production, which is back to 0.15. Shall we attach as separate patches?", "Andrew, I've marked it against the latest releases of all the branches.  Shall I create JIRA release versions 0.15.4, 0.16.5, 0.18.4 and 0.19.2 for this patch?  Also: who is still running 0.15 that can verify the fix took?", "Can we make this a blocker?", "bq. Shall I create JIRA release versions 0.15.4, 0.16.5, 0.18.4 and 0.19.2 for this patch?  That won't be necessary until Hadoop QA succeeds.  bq. Also: who is still running 0.15 that can verify the fix took?   I believe this information is classified.", "Sorry this is too late to make 1.1.0.  Please consider contributing to 3.14159.", "Nobody has submitted a patch; this issue is 9 years old. And everyone should use kerberos. closing as WONTFIX"], "tasks": {"summarization": "Replace references to \"Dr Who\" in codebase with @BigDataBorat - People complain that having \"Dr Who\" in the code causes confusion and isn't appropriate in Hadoop no...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8607 about?", "answer": "Replace references to \"Dr Who\" in codebase with @BigDataBorat"}}}
{"issue_id": "HADOOP-8606", "project": "HADOOP", "title": "FileSystem.get may return the wrong filesystem", "status": "Closed", "priority": "Major", "reporter": "Daryn Sharp", "assignee": "Daryn Sharp", "created": "2012-07-18T17:19:31.000+0000", "updated": "2016-05-12T18:21:48.000+0000", "labels": [], "description": "{{FileSystem.get(URI, conf)}} will return the default fs if the scheme is null, regardless of whether the authority is null too.  This causes URIs of \"//authority/path\" to _always_ refer to \"/path\" on the default fs.  To the user, this appears to \"work\" if the authority in the null-scheme URI matches the authority of the default fs.  When the authorities don't match, the user is very surprised that the default fs is used.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537026/HADOOP-8606.branch-1.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1203//console  This message is automatically generated.", "Resubmitting patch because jenkins only tried to apply the branch-1 patch to trunk...", "This happened to be discovered while testing federation (but not related to federation).  Old tests using the bad syntax failed when the conf's default fs was changed.  Lots of confusion ensued as commands seem to run but the files weren't on the right NN, etc.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537032/HADOOP-8606.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1204//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1204//console  This message is automatically generated.", "+1 for the patch.", "I am +1 n this patch too, Thanks Daryn, I'll check this in for you.", "Thanks Daryn,  I put this into trunk, branch-2, branch-1, and branch-0.23", "Integrated in Hadoop-Hdfs-trunk-Commit #2581 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2581/])     HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365224)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365224 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Common-trunk-Commit #2516 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2516/])     HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365224)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365224 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2537 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2537/])     HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365224)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365224 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Hdfs-trunk #1115 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1115/])     HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365224)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365224 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Hdfs-0.23-Build #324 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/324/])     svn merge -c 1365224 FIXES: HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365228)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365228 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "Integrated in Hadoop-Mapreduce-trunk #1147 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1147/])     HADOOP-8606. FileSystem.get may return the wrong filesystem (Daryn Sharp via bobby) (Revision 1365224)       Result = FAILURE bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1365224 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileSystemCaching.java", "added 1.2.0 to fixVersion, per CHANGES.txt."], "tasks": {"summarization": "FileSystem.get may return the wrong filesystem - {{FileSystem.get(URI, conf)}} will return the default fs if the scheme is null, regardless of whethe...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8606 about?", "answer": "FileSystem.get may return the wrong filesystem"}}}
{"issue_id": "HADOOP-8605", "project": "HADOOP", "title": "TestReflectionUtils.testCacheDoesntLeak() can't illustrate ReflectionUtils don't generate memory leak", "status": "Open", "priority": "Minor", "reporter": "Jiandan Yang ", "assignee": null, "created": "2012-07-18T03:44:58.000+0000", "updated": "2012-07-18T14:21:10.000+0000", "labels": [], "description": "TestReflectionUtils.testCacheDoesntLeak() uses different urlClassLoader to load TestReflectionUtils$LoadedInChild in a for cycle: {code}     int iterations=9999;     for (int i=0; i<iterations; i++) {        URLClassLoader loader = new URLClassLoader(new URL[0], getClass().getClassLoader());        Class cl = Class.forName(\"org.apache.hadoop.util.TestReflectionUtils$LoadedInChild\", false, loader);        Object o = ReflectionUtils.newInstance(cl, null);        assertEquals(cl, o.getClass());      } {code} but every time it generate the same class\uff0cso in ReflectionUtils.CONSTRUCTOR_CACHE only include one class.", "comments": [], "tasks": {"summarization": "TestReflectionUtils.testCacheDoesntLeak() can't illustrate ReflectionUtils don't generate memory leak - TestReflectionUtils.testCacheDoesntLeak() uses different urlClassLoader to load TestReflectionUtils$...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8605 about?", "answer": "TestReflectionUtils.testCacheDoesntLeak() can't illustrate ReflectionUtils don't generate memory leak"}}}
{"issue_id": "HADOOP-8604", "project": "HADOOP", "title": "conf/* files overwritten at Hadoop compilation", "status": "Resolved", "priority": "Minor", "reporter": "Robert Grandl", "assignee": null, "created": "2012-07-18T03:31:38.000+0000", "updated": "2015-03-09T20:04:42.000+0000", "labels": [], "description": "Whenever I compile hadoop from terminal as: ant compile jar run  all the conf/* files are overwritten. I am not sure if some of them should not be like that but at least hadoop-env.sh, mapred-site.ml, core-site.xml, hdfs-site.xml, masters, slaves should remains. Otherwise I am forced to backup and replace content again after compilation.", "comments": ["The conf/ directory within the Hadoop tree is intended to hold the sample/default configuration.  During development, custom configuration changes are not intended to be placed in that directory, as those changes will be overwritten during builds.  Usually custom configuration is placed in a separate directory, outside the Hadoop tree, and conveyed to Hadoop via the HADOOP_CONF_DIR environment variable.  You can also specify a config directory via the --config option to the hadoop front-end script.  With a separate config directory, your changes won't be overwritten during compilation.", "Closing as won't fix."], "tasks": {"summarization": "conf/* files overwritten at Hadoop compilation - Whenever I compile hadoop from terminal as: ant compile jar run  all the conf/* files are overwritte...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8604 about?", "answer": "conf/* files overwritten at Hadoop compilation"}}}
{"issue_id": "HADOOP-8603", "project": "HADOOP", "title": "Test failures with \"Container .. is running beyond virtual memory limits\"", "status": "Resolved", "priority": "Major", "reporter": "Ilya Katsov", "assignee": null, "created": "2012-07-17T12:35:32.000+0000", "updated": "2012-08-09T06:38:50.000+0000", "labels": ["test"], "description": "Tests org.apache.hadoop.tools.TestHadoopArchives.{testRelativePath,testPathWithSpaces} fail with the following message:  {code} Container [pid=7785,containerID=container_1342495768864_0001_01_000001] is running beyond virtual memory limits. Current usage: 143.6mb of 1.5gb physical memory used; 3.4gb of 3.1gb virtual memory used. Killing container. Dump of the process-tree for container_1342495768864_0001_01_000001 : \t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE \t|- 7797 7785 7785 7785 (java) 573 38 3517018112 36421 /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster  \t|- 7785 7101 7785 7785 (bash) 1 1 108605440 332 /bin/bash -c /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stdout 2>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stderr    {code}  Is it related to https://issues.apache.org/jira/browse/MAPREDUCE-3933 ? This is not a stably reproducible problem, but it seems that adding MALLOC_ARENA_MAX resolves the problem.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12536971/HADOOP-8603-branch-0.23.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-tools/hadoop-archives.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1202//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1202//console  This message is automatically generated.", "Adding more units that are subjects of failed tests", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12537133/HADOOP-8603-branch-0.23_002.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-tools/hadoop-archives hadoop-tools/hadoop-streaming:                    org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1205//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1205//console  This message is automatically generated.", "This is a platform-specific issue, no additional tests can be added.", "Moved to https://issues.apache.org/jira/browse/MAPREDUCE-4535", "Moved to https://issues.apache.org/jira/browse/MAPREDUCE-4535"], "tasks": {"summarization": "Test failures with \"Container .. is running beyond virtual memory limits\" - Tests org.apache.hadoop.tools.TestHadoopArchives.{testRelativePath,testPathWithSpaces} fail with the...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8603 about?", "answer": "Test failures with \"Container .. is running beyond virtual memory limits\""}}}
{"issue_id": "HADOOP-8602", "project": "HADOOP", "title": "Passive mode support for FTPFileSystem", "status": "Patch Available", "priority": "Minor", "reporter": "Nemon Lou", "assignee": null, "created": "2012-07-17T03:06:10.000+0000", "updated": "2017-09-11T05:38:57.000+0000", "labels": ["BB2015-05-TBR"], "description": "FTPFileSystem uses active mode for default data connection mode.We shall be able to choose passive mode when active mode doesn't work (firewall for example).  My thoughts is to add an option \"fs.ftp.data.connection.mode\" in core-site.xml.Since FTPClient(in org.apache.commons.net.ftp package) already supports passive mode, we just need to add a few code in FTPFileSystem .connect() method.", "comments": ["I havn't found any test case for FTPFileSystem.java. So no test case added for passive mode option. I will add one if necessary.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12580259/HADOOP-8602.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2470//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2470//console  This message is automatically generated.", "Case test needs to use {{toLower()}} with a locale; the {{equalsIgnoreCase()}} fails in countries where {{\"I\".toLower() != \"i\"}}", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12580259/HADOOP-8602.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3711//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3711//console  This message is automatically generated.", "Updated patch which uses toLowerCase(Locale.ENGLISH) for fs.ftp.data.connection.mode to address localization concern.", "Q: Is it actually the case that many/most hadoop configuration options are case-insensitive?", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12637012/HADOOP-8602.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/3722//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/3722//console  This message is automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12637012/HADOOP-8602.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6327/console |   This message was automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12637012/HADOOP-8602.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6339/console |   This message was automatically generated.", "I also encounter this issue when i try to use ftp server as filesystem. Unfortunately, it's still unresolved. Therefor, I want to fix the rest based on the previous version. Please review and give some advice. Thank you.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | pre-patch |  16m 42s | Pre-patch trunk compilation is healthy. | | {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. | | {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. | | {color:green}+1{color} | javac |   7m 37s | There were no new javac warning messages. | | {color:green}+1{color} | javadoc |   9m 37s | There were no new javadoc warning messages. | | {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. | | {color:red}-1{color} | checkstyle |   1m  5s | The applied patch generated  1 new checkstyle issues (total was 6, now 7). | | {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. | | {color:green}+1{color} | install |   1m 21s | mvn install still works. | | {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. | | {color:green}+1{color} | findbugs |   1m 53s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. | | {color:red}-1{color} | common tests |  21m 46s | Tests failed in hadoop-common. | | | |  61m  1s | | \\\\ \\\\ || Reason || Tests || | Failed unit tests | hadoop.fs.ftp.TestFTPFileSystem | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12748634/HADOOP-8602.004.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / 9a08999 | | checkstyle |  https://builds.apache.org/job/PreCommit-HADOOP-Build/7406/artifact/patchprocess/diffcheckstylehadoop-common.txt | | whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/7406/artifact/patchprocess/whitespace.txt | | hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7406/artifact/patchprocess/testrun_hadoop-common.txt | | Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7406/testReport/ | | Java | 1.7.0_55 | | uname | Linux asf906.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7406/console |   This message was automatically generated.", "test triggers an NPE  {code} java.lang.NullPointerException: null \tat org.apache.hadoop.fs.ftp.TestFTPFileSystem.testFTPClientConnectionMode(TestFTPFileSystem.java:37) {code}", "I'm not sure whether the test method is appropriate. Can give some advice if possible?", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12748716/HADOOP-8602.005.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / 9a08999 | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7410/console |   This message was automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | pre-patch |  17m 12s | Pre-patch trunk compilation is healthy. | | {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. | | {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. | | {color:green}+1{color} | javac |   8m  3s | There were no new javac warning messages. | | {color:green}+1{color} | javadoc |  10m 11s | There were no new javadoc warning messages. | | {color:green}+1{color} | release audit |   0m 22s | The applied patch does not increase the total number of release audit warnings. | | {color:green}+1{color} | checkstyle |   1m 11s | There were no new checkstyle issues. | | {color:red}-1{color} | whitespace |   0m  0s | The patch has 1  line(s) that end in whitespace. Use git apply --whitespace=fix. | | {color:green}+1{color} | install |   1m 24s | mvn install still works. | | {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. | | {color:green}+1{color} | findbugs |   1m 58s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. | | {color:red}-1{color} | common tests |  22m 13s | Tests failed in hadoop-common. | | | |  63m 21s | | \\\\ \\\\ || Reason || Tests || | Failed unit tests | hadoop.fs.ftp.TestFTPFileSystem | |   | hadoop.ipc.TestRPCWaitForProxy | |   | hadoop.ipc.TestCallQueueManager | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12748800/HADOOP-8602.006.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / d540374 | | whitespace | https://builds.apache.org/job/PreCommit-HADOOP-Build/7413/artifact/patchprocess/whitespace.txt | | hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7413/artifact/patchprocess/testrun_hadoop-common.txt | | Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7413/testReport/ | | Java | 1.7.0_55 | | uname | Linux asf905.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7413/console |   This message was automatically generated.", "Like you note, the amount of testing for FTP is ~= 0.  there's some under {{org/apache/hadoop/fs/contract/ftp}};  they use instances of {{FTPContract}} to decide whether to run the tests or not. Essentially, if the configuration in the (git/svn ignored) file {{test/resources/\"contract-test-options.xml}} contains the key {{test.fs.ftp}}, the ftp URL so defined is used as the test target. If the key is missing, the tests are skipped.   There's some detail [In the online docs|http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/filesystem/testing.html]  I'd recommend that # you get your setup into a state where those tests are running # you extend them/add a new contract test to test new features.  The FTP fs doesn't get much attention; it's primary use is a read-only server of data. And if you search for \"hadoop ftp open\" there's a short list of outstanding issues. Its a non-critical component & the risk of major breakages is low: it just needs someone to sit down & fix things. Thank you for volunteering :)", "Oh~ it sounds like a good news to me. I shall try me best to fix those ftp problems.  By the way, i was wondering if it is feasible to offer method getFTPClient(), and let the user use setXXX() provided by the FTPClient, it may help reduce some problems.", "having get/set operations would be a bit of an ugly workaround: actually having the passive mode would be the ideal one.", "Excuse me, why there isn't Hadoop QA these days. 007.patch doesn't get any response.", "you have to hit the \"submit patch\" button", "test bugs fixed.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | pre-patch |  16m 30s | Pre-patch trunk compilation is healthy. | | {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. | | {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. | | {color:green}+1{color} | javac |   7m 41s | There were no new javac warning messages. | | {color:green}+1{color} | javadoc |   9m 42s | There were no new javadoc warning messages. | | {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. | | {color:red}-1{color} | checkstyle |   1m  5s | The applied patch generated  2 new checkstyle issues (total was 6, now 8). | | {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. | | {color:green}+1{color} | install |   1m 21s | mvn install still works. | | {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. | | {color:green}+1{color} | findbugs |   1m 52s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. | | {color:red}-1{color} | common tests |  22m 24s | Tests failed in hadoop-common. | | | |  61m 34s | | \\\\ \\\\ || Reason || Tests || | Failed unit tests | hadoop.ha.TestZKFailoverController | |   | hadoop.net.TestNetUtils | |   | hadoop.fs.ftp.TestFTPFileSystem | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12749772/HADOOP-8602.007.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / fa1d84a | | checkstyle |  https://builds.apache.org/job/PreCommit-HADOOP-Build/7436/artifact/patchprocess/diffcheckstylehadoop-common.txt | | hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7436/artifact/patchprocess/testrun_hadoop-common.txt | | Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7436/testReport/ | | Java | 1.7.0_55 | | uname | Linux asf909.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7436/console |   This message was automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | pre-patch |  17m 40s | Pre-patch trunk compilation is healthy. | | {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. | | {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. | | {color:green}+1{color} | javac |   8m 17s | There were no new javac warning messages. | | {color:green}+1{color} | javadoc |  10m 19s | There were no new javadoc warning messages. | | {color:green}+1{color} | release audit |   0m 23s | The applied patch does not increase the total number of release audit warnings. | | {color:red}-1{color} | checkstyle |   1m  9s | The applied patch generated  1 new checkstyle issues (total was 6, now 7). | | {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. | | {color:green}+1{color} | install |   1m 27s | mvn install still works. | | {color:green}+1{color} | eclipse:eclipse |   0m 34s | The patch built with eclipse:eclipse. | | {color:green}+1{color} | findbugs |   2m  0s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. | | {color:red}-1{color} | common tests |  23m  7s | Tests failed in hadoop-common. | | | |  64m 59s | | \\\\ \\\\ || Reason || Tests || | Failed unit tests | hadoop.net.TestNetUtils | |   | hadoop.ha.TestZKFailoverController | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12749824/HADOOP-8602.008.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / fa1d84a | | checkstyle |  https://builds.apache.org/job/PreCommit-HADOOP-Build/7437/artifact/patchprocess/diffcheckstylehadoop-common.txt | | hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7437/artifact/patchprocess/testrun_hadoop-common.txt | | Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7437/testReport/ | | Java | 1.7.0_55 | | uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7437/console |   This message was automatically generated.", "Those failed unit tests \"hadoop.net.TestNetUtils, hadoop.ha.TestZKFailoverController\" are not part of this issue? I don't know how to deal with them.", "you are right: not your patche's problem. Some regression you don't need to worry about", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | pre-patch |  17m 58s | Pre-patch trunk compilation is healthy. | | {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. | | {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. | | {color:green}+1{color} | javac |   8m  1s | There were no new javac warning messages. | | {color:green}+1{color} | javadoc |  10m 25s | There were no new javadoc warning messages. | | {color:green}+1{color} | release audit |   0m 24s | The applied patch does not increase the total number of release audit warnings. | | {color:green}+1{color} | checkstyle |   1m 17s | There were no new checkstyle issues. | | {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. | | {color:green}+1{color} | install |   1m 26s | mvn install still works. | | {color:green}+1{color} | eclipse:eclipse |   0m 35s | The patch built with eclipse:eclipse. | | {color:green}+1{color} | findbugs |   1m 58s | The patch does not introduce any new Findbugs (version 3.0.0) warnings. | | {color:red}-1{color} | common tests |  23m  7s | Tests failed in hadoop-common. | | | |  65m 14s | | \\\\ \\\\ || Reason || Tests || | Failed unit tests | hadoop.net.TestNetUtils | |   | hadoop.ha.TestZKFailoverController | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12749855/HADOOP-8602.009.patch | | Optional Tests | javadoc javac unit findbugs checkstyle | | git revision | trunk / fa1d84a | | hadoop-common test log | https://builds.apache.org/job/PreCommit-HADOOP-Build/7438/artifact/patchprocess/testrun_hadoop-common.txt | | Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7438/testReport/ | | Java | 1.7.0_55 | | uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7438/console |   This message was automatically generated.", "! :) Pass! My patch can be submitted?", "Hi~ If there're some unreasonable codes in patch,(always in unit test), please let me know, i will fix it.  If it works, i may try to solve other ftp-related issues. Thank you.", "Setting target-version instead of fix-version.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 8s {color} | {color:blue} docker + precommit patch detected. {color} | | {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} | | {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 3m 8s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 4m 51s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 4m 38s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 50s {color} | {color:green} trunk passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 2s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 13s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} | | {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 32s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 5m 8s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 5m 8s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} compile {color} | {color:green} 4m 31s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} | | {color:green}+1{color} | {color:green} javac {color} | {color:green} 4m 31s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} | | {color:green}+1{color} | {color:green} xml {color} | {color:green} 0m 0s {color} | {color:green} The patch has no ill-formed XML file. {color} | | {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 57s {color} | {color:green} the patch passed {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 59s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} | | {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 9s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} | | {color:green}+1{color} | {color:green} unit {color} | {color:green} 7m 45s {color} | {color:green} hadoop-common in the patch passed with JDK v1.8.0_60. {color} | | {color:red}-1{color} | {color:red} unit {color} | {color:red} 7m 31s {color} | {color:red} hadoop-common in the patch failed with JDK v1.7.0_79. {color} | | {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 23s {color} | {color:green} Patch does not generate ASF License warnings. {color} | | {color:black}{color} | {color:black} {color} | {color:black} 49m 46s {color} | {color:black} {color} | \\\\ \\\\ || Reason || Tests || | JDK v1.7.0_79 Failed junit tests | hadoop.fs.shell.TestCopyPreserveFlag | |   | hadoop.ipc.TestDecayRpcScheduler | \\\\ \\\\ || Subsystem || Report/Notes || | Docker | Client=1.7.1 Server=1.7.1 Image:test-patch-base-hadoop-date2015-10-26 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12749855/HADOOP-8602.009.patch | | JIRA Issue | HADOOP-8602 | | Optional Tests |  asflicense  javac  javadoc  mvninstall  unit  findbugs  checkstyle  compile  xml  | | uname | Linux 47dd7181962a 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux | | Build tool | maven | | Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/patchprocess/apache-yetus-b9c369f/dev-support/personality/hadoop.sh | | git revision | trunk / 2f1eb2b | | Default Java | 1.7.0_79 | | Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_60 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_79 | | findbugs | v3.0.0 | | unit | https://builds.apache.org/job/PreCommit-HADOOP-Build/7938/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_79.txt | | unit test logs |  https://builds.apache.org/job/PreCommit-HADOOP-Build/7938/artifact/patchprocess/patch-unit-hadoop-common-project_hadoop-common-jdk1.7.0_79.txt | | JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-HADOOP-Build/7938/testReport/ | | Max memory used | 222MB | | Powered by | Apache Yetus   http://yetus.apache.org | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/7938/console |   This message was automatically generated.", "Moving this improvement out of 2.7.2 and from future maintenance lines.  [~steve_l], bump on behalf of the contributor if you are still looking at this.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  8s{color} | {color:red} HADOOP-8602 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Issue | HADOOP-8602 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12749855/HADOOP-8602.009.patch | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/11972/console | | Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |   This message was automatically generated.", "| (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} | | {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  5s{color} | {color:red} HADOOP-8602 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} | \\\\ \\\\ || Subsystem || Report/Notes || | JIRA Issue | HADOOP-8602 | | JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12749855/HADOOP-8602.009.patch | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/13249/console | | Powered by | Apache Yetus 0.6.0-SNAPSHOT   http://yetus.apache.org |   This message was automatically generated."], "tasks": {"summarization": "Passive mode support for FTPFileSystem - FTPFileSystem uses active mode for default data connection mode.We shall be able to choose passive m...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8602 about?", "answer": "Passive mode support for FTPFileSystem"}}}
{"issue_id": "HADOOP-8601", "project": "HADOOP", "title": "Extend TestShell to cover Windows shell commands", "status": "Resolved", "priority": "Major", "reporter": "Chuan Liu", "assignee": "Chuan Liu", "created": "2012-07-16T23:43:16.000+0000", "updated": "2012-11-06T00:02:02.000+0000", "labels": [], "description": "The existing unit test only covers Linux shell commands. Since we begin to support Windows and use completely different commands on Windows, it make sense to extend TestShell to cover Windows use cases.", "comments": ["With HADOOP-8972, we will test Windows shell commands with a separate test: TestWinUtils, and this JIRA is no longer needed.", "Changing the resolution to Invalid."], "tasks": {"summarization": "Extend TestShell to cover Windows shell commands - The existing unit test only covers Linux shell commands. Since we begin to support Windows and use c...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8601 about?", "answer": "Extend TestShell to cover Windows shell commands"}}}
{"issue_id": "HADOOP-8599", "project": "HADOOP", "title": "Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file", "status": "Closed", "priority": "Major", "reporter": "Andrey Klochkov", "assignee": "Andrey Klochkov", "created": "2012-07-16T19:48:21.000+0000", "updated": "2012-10-11T22:55:02.000+0000", "labels": [], "description": "When FileSystem.getFileBlockLocations(file,start,len) is called with \"start\" argument equal to the file size, the response is not empty. There is a test TestGetFileBlockLocations.testGetFileBlockLocations2 which uses randomly generated \"start\" and \"len\" arguments when calling FileSystem.getFileBlockLocations and the test fails randomly (when the generated start value equals to the file size).", "comments": ["The build https://builds.apache.org/job/Hadoop-Common-0.23-Build/312/ failed due to this bug, this is the failure log:  {code} org.apache.hadoop.fs.TestGetFileBlockLocations.testGetFileBlockLocations2  Failing for the past 1 build (Since #312 ) Took 0.29 sec.  Stacktrace  junit.framework.AssertionFailedError: null \tat junit.framework.Assert.fail(Assert.java:47) \tat junit.framework.Assert.assertTrue(Assert.java:20) \tat junit.framework.Assert.assertTrue(Assert.java:27) \tat org.apache.hadoop.fs.TestGetFileBlockLocations.oneTest(TestGetFileBlockLocations.java:90) \tat org.apache.hadoop.fs.TestGetFileBlockLocations.__CLR3_0_2h1r7rc12wg(TestGetFileBlockLocations.java:136) \tat org.apache.hadoop.fs.TestGetFileBlockLocations.testGetFileBlockLocations2(TestGetFileBlockLocations.java:131) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) \tat java.lang.reflect.Method.invoke(Method.java:597) \tat junit.framework.TestCase.runTest(TestCase.java:168) \tat junit.framework.TestCase.runBare(TestCase.java:134) \tat junit.framework.TestResult$1.protect(TestResult.java:110) \tat junit.framework.TestResult.runProtected(TestResult.java:128) \tat junit.framework.TestResult.run(TestResult.java:113) \tat junit.framework.TestCase.run(TestCase.java:124) \tat junit.framework.TestSuite.runTest(TestSuite.java:243) \tat junit.framework.TestSuite.run(TestSuite.java:238) \tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) \tat org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53) \tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123) \tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) \tat java.lang.reflect.Method.invoke(Method.java:597) \tat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164) \tat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110) \tat org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175) \tat org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:81) \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68) Standard Output  2012-07-15 09:13:27,734 INFO  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(50)) - Loaded the native-hadoop library {code}", "The patch fixes the condition of going beyond EOF in the FileSystem.getFileBlockLocations by changing   {code}     if (file.getLen() < start) {       return new BlockLocation[0]; {code}  to   {code}     if (file.getLen() <= start) {       return new BlockLocation[0]; {code}   An additional assert is added to the unit tests to verify the case.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12536733/HADOOP-8859-branch-0.23.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1198//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1198//console  This message is automatically generated.", "+1, looks good to me. Thanks Andrey.", "Integrated in Hadoop-Hdfs-trunk-Commit #2551 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2551/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362295)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362295 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Integrated in Hadoop-Common-trunk-Commit #2486 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2486/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362295)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362295 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2507 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2507/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362295)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362295 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Integrated in Hadoop-Hdfs-trunk #1106 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1106/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362295)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362295 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Integrated in Hadoop-Hdfs-0.23-Build #316 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/316/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362297)       Result = SUCCESS todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362297 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Integrated in Hadoop-Mapreduce-trunk #1139 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1139/])     HADOOP-8599. Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file. Contributed by Andrey Klochkov. (Revision 1362295)       Result = FAILURE todd : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362295 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestGetFileBlockLocations.java", "Fixed Target Versions to be consistent with Fix Versions. Removed 1.1.0 target and opened HADOOP-8612 for backport.", "I think TestCombineFileInputFormat.testForEmptyFile started failing after this. The split size on an empty input file used to be 1, but it's now 0.", "MAPREDUCE-4470 has been filed.", "Can someone take a look at MAPREDUCE-4470 and propose a proper fix or retract HADOOP-8599 from the trunk?  This is causing the trunk build to fail for more than a week.  I think returning a 0 split for an empty input is not a good idea.  The original behavior of 1 split with size 0 was good.  Can others jump in and comment on this? Thanks.", "Thank you for your attention not sending so many junk mails to the mailing list. It's infliction.  Sent from iPhone."], "tasks": {"summarization": "Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file  - When FileSystem.getFileBlockLocations(file,start,len) is called with \"start\" argument equal to the f...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8599 about?", "answer": "Non empty response from FileSystem.getFileBlockLocations when asking for data beyond the end of file"}}}
{"issue_id": "HADOOP-8598", "project": "HADOOP", "title": "Server-side Trash", "status": "Open", "priority": "Critical", "reporter": "Eli Collins", "assignee": "Eli Collins", "created": "2012-07-15T22:12:08.000+0000", "updated": "2012-08-13T18:53:07.000+0000", "labels": [], "description": "There are a number of problems with Trash that continue to result in permanent data loss for users. The primary reasons trash is not used:  - Trash is configured client-side and not enabled by default. - Trash is shell-only. FileSystem, WebHDFS, HttpFs, etc never use trash. - If trash fails, for example, because we can't create the trash directory or the move itself fails, trash is bypassed and the data is deleted.  Trash was designed as a feature to help end users via the shell, however in my experience the primary use of trash is to help administrators implement data retention policies (this was also the motivation for HADOOP-7460).  One could argue that (periodic read-only) snapshots are a better solution to this problem, however snapshots are not slated for Hadoop 2.x and trash is complimentary to snapshots (and backup) - eg you may create and delete data within your snapshot or backup window - so it makes sense to revisit trash's design. I think it's worth bringing trash's functionality in line with what users need.  I propose we enable trash on a per-filesystem basis and implement it server-side. Ie trash becomes an HDFS feature enabled by administrators. Because the trash emptier lives in HDFS and users already have a per-filesystem trash directory we're mostly there already. The design preference from HADOOP-2514 was for trash to be implemented in \"user code\" however (a) in light of these problems, (b) we have a lot more user-facing APIs than the shell and (c) clients increasingly span file systems (via federation and symlinks) this design choice makes less sense. This is why we already use a per-filesystem trash/home directory instead of the user's client-configured one - otherwise trash would not work because renames can't span file systems.  In short, HDFS trash would work similarly to how it does today, the difference is that client delete APIs would result in a rename into trash (ala TrashPolicyDefault#moveToTrash) if trash is enabled. Like today it would be renamed to the trash directory on the file system where the file being removed resides. The primary difference is that enablement and policy are configured server-side by adminstrators and is used regardless of the API used to access the filesytem. The one execption to this is that I think we should continue to support the explict skipTrash shell option. The rationale for skipTrash (HADOOP-6080) is that a move to trash may fail in cases where a rm may not, if a user has a home directory quota and does a rmr /tonsOfData, for example. Without a way to bypass this the user has no way (unless we revisit quotas, permissions or trash paths) to remove a directory they have permissions to remove without getting their quota adjusted by an admin. The skip trash API can be implemented by adding an explicit FileSystem API that bypasses trash and modifying the shell to use it when skipTrash is enabled. Given that users must explicitly specify skipTrash the API is less error prone. We could have the shell ask confirmation and annotate the API private to FsShell to discourage programatic use. This is not ideal but can be done compatibly (unlike redefining quotas, permissions or trash paths).  In terms of compatibility, while this proposal is technically an incompatible change (client side configuration that disables trash and uses skipTrash with a previous FsShell release will now both be ignored if server-side trash is enabled, and non-HDFS file systems would need to make similar changes) I think it's worth targeting for Hadoop 2.x given that the new semantics preserve the current semantics. In 2.x I think we should preserve FsShell based trash and support both it and server-side trash (defaults to disabled). For trunk/3.x I think we should remove the FsShell based trash entirely and enable server-side trash by default.", "comments": ["Linking in some related issues.", "Forgot to mention that the pluggae trash policy makes less sense server side, should probably be replaced with a delete hook in FsShell since reasonable policies might want to do things that ant run in the NN.", "I think a quick and easy option for implementing \"server-side trash\" would be to just not require any configuration client-side. This could be done by adding an \"isTrashEnabled\" field to FsServerDefaults and changing the TrashPolicyDefault#isEnabled method to call Filesystem#getServerDefaults and return the value of isTrashEnabled. This won't result in a ton of extra RPCs to the NN since the HDFS implementation of getServerDefaults is cached in the DFSClient and rate-limited to one call per hour. Though it obviously wouldn't cover the issue of APIs besides the FsShell, I think this would cover a very large portion of the user problems we currently see with trash.", "I haven't digested this jira, but be sure to keep viewfs in mind.  Ie. {{FileSystem#getServerDefaults(Path)}} would be required to get the info for the right mount point.", "@ATM, like your idea, though it will only work with new clients (modified to check the server config) and FsShell it seems like a reasonable approach for branch-2.", "Filed HADOOP-8689 for v2 per ATM's suggestion so re-targeting this change for trunk/v3."], "tasks": {"summarization": "Server-side Trash - There are a number of problems with Trash that continue to result in permanent data loss for users. ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8598 about?", "answer": "Server-side Trash"}}}
{"issue_id": "HADOOP-8597", "project": "HADOOP", "title": "FsShell's Text command should be able to read avro data files", "status": "Closed", "priority": "Major", "reporter": "Harsh J", "assignee": "Ivan Vladimirov Ivanov", "created": "2012-07-14T07:37:02.000+0000", "updated": "2013-02-15T13:12:35.000+0000", "labels": ["newbie"], "description": "Similar to SequenceFiles are Apache Avro's DataFiles. Since these are getting popular as a data format, perhaps it would be useful if {{fs -text}} were to add some support for reading it, like it reads SequenceFiles. Should be easy since Avro is already a dependency and provides the required classes.  Of discussion is the output we ought to emit. Avro DataFiles aren't simple as text, nor have they the singular Key-Value pair structure of SequenceFiles. They usually contain a set of fields defined as a record, and the usual text emit, as available from avro-tools via http://avro.apache.org/docs/current/api/java/org/apache/avro/tool/DataFileReadTool.html, is in proper JSON format.  I think we should use the JSON format as the output, rather than a delimited form, for there are many complex structures in Avro and JSON is the easiest and least-work-to-do way to display it (Avro supports json dumping by itself).", "comments": ["The proposed patch adds the logic to output the content of Avro data files in JSON format.  The implementation does not use the DataFileReadTool class since, as it turned out, the org.apache.avro.tool package is not currently part of the project's dependencies. As a consequence this allowed a more memory efficient implementation, which keeps only a constant number of Avro records in memory.", "This looks like a useful addition.  Can you please add a unit test for it?", "Done - a unit test is added.", "Not sure why, but your patch file didn't apply cleanly for me.  Here's the same patch, but a version that applies cleanly.", "+1 Patch looks good to me.  Let's see what Jenkins says.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12544521/HADOOP-8597.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1427//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1427//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1427//console  This message is automatically generated.", "Jenkins says that org.apache.hadoop.fs.shell.Display$AvroFileInputStream should be static.", "New version with AvroFileInputStream made static.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12544535/HADOOP-8597.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1429//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1429//console  This message is automatically generated.", "Sorry for the inconvenience that applying my patch caused. Since I am new to the project I was unsure against which version (or branch) to create the patch - so I chose \"release-2.0.0-alpha\". It seemed to most closely match the \"Affects Version/s\" field. In retrospect the choice was probably a mistake. To avoid such problems in the future, I would like to ask the following question - Should patches be created against the first branch with a version number greater or equal to that in the \"Affects Version/s\" field (\"branch-2.0.1-alpha\" in the current case) or if the version is new enough to directly use the trunk.  Thank you for taking the time to review my patch. I hope that it will be useful and would be very happy if it gets committed.", "Ivan, patches are normally against trunk.  After they're committed to trunk they may be backported to a branch.  http://wiki.apache.org/hadoop/HowToContribute  This patch should probably be committed to trunk and to branch-2 with fix-version 2.0.3-alpha.", "Integrated in Hadoop-Hdfs-trunk-Commit #2782 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2782/])     HADOOP-8597. Permit FsShell's text command to read Avro files.  Contributed by Ivan Vladimirov. (Revision 1383607)       Result = SUCCESS cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1383607 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java", "I just committed this.  Thanks, Ivan!", "Integrated in Hadoop-Common-trunk-Commit #2719 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2719/])     HADOOP-8597. Permit FsShell's text command to read Avro files.  Contributed by Ivan Vladimirov. (Revision 1383607)       Result = SUCCESS cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1383607 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2743 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2743/])     HADOOP-8597. Permit FsShell's text command to read Avro files.  Contributed by Ivan Vladimirov. (Revision 1383607)       Result = FAILURE cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1383607 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java", "Integrated in Hadoop-Hdfs-trunk #1163 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1163/])     HADOOP-8597. Permit FsShell's text command to read Avro files.  Contributed by Ivan Vladimirov. (Revision 1383607)       Result = FAILURE cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1383607 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java", "Integrated in Hadoop-Mapreduce-trunk #1194 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1194/])     HADOOP-8597. Permit FsShell's text command to read Avro files.  Contributed by Ivan Vladimirov. (Revision 1383607)       Result = SUCCESS cutting : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1383607 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Display.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/shell/TestTextCommand.java"], "tasks": {"summarization": "FsShell's Text command should be able to read avro data files - Similar to SequenceFiles are Apache Avro's DataFiles. Since these are getting popular as a data form...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8597 about?", "answer": "FsShell's Text command should be able to read avro data files"}}}
{"issue_id": "HADOOP-8596", "project": "HADOOP", "title": "TestFileAppend4#testCompleteOtherLeaseHoldersFile times out", "status": "Open", "priority": "Major", "reporter": "Eli Collins", "assignee": null, "created": "2012-07-14T01:04:10.000+0000", "updated": "2012-07-14T01:04:10.000+0000", "labels": [], "description": "Saw TestFileAppend4#testCompleteOtherLeaseHoldersFile on a recent jenkins run. Perhaps we need to bump the timeout?   {noformat} Error Message  test timed out after 60000 milliseconds Stacktrace  java.lang.Exception: test timed out after 60000 milliseconds \tat java.lang.Object.wait(Native Method) \tat java.lang.Thread.join(Thread.java:1186) \tat java.lang.Thread.join(Thread.java:1239) \tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.join(BPServiceActor.java:477) \tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.join(BPOfferService.java:259) \tat org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.shutDownAll(BlockPoolManager.java:117) \tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1101) \tat org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1343) \tat org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1323) \tat org.apache.hadoop.hdfs.TestFileAppend4.testCompleteOtherLeaseHoldersFile(TestFileAppend4.java:289) {noformat}", "comments": [], "tasks": {"summarization": "TestFileAppend4#testCompleteOtherLeaseHoldersFile times out - Saw TestFileAppend4#testCompleteOtherLeaseHoldersFile on a recent jenkins run. Perhaps we need to bu...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8596 about?", "answer": "TestFileAppend4#testCompleteOtherLeaseHoldersFile times out"}}}
{"issue_id": "HADOOP-8595", "project": "HADOOP", "title": "Create security page in the docs and update the ASF page to link to it", "status": "Open", "priority": "Major", "reporter": "Eli Collins", "assignee": null, "created": "2012-07-14T00:59:46.000+0000", "updated": "2012-07-14T00:59:46.000+0000", "labels": [], "description": "We should (1) create a http://hadoop.apache.org/security.html with info on how to report a security problem, pointer to security@hadoop.apache.org, and (2) get it linked off the main ASF page: http://www.apache.org/security/projects.html.", "comments": [], "tasks": {"summarization": "Create security page in the docs and update the ASF page to link to it - We should (1) create a http://hadoop.apache.org/security.html with info on how to report a security ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8595 about?", "answer": "Create security page in the docs and update the ASF page to link to it"}}}
{"issue_id": "HADOOP-8594", "project": "HADOOP", "title": "Fix issues identified by findsbugs2", "status": "Resolved", "priority": "Major", "reporter": "Eli Collins", "assignee": null, "created": "2012-07-13T23:34:20.000+0000", "updated": "2015-02-05T01:33:46.000+0000", "labels": [], "description": "Harsh recently ran findbugs 2 (instead of 1.3.9 which is what jenkins runs) and it showed thousands of warnings (they've made a lot of progress in findbugs releases). We should upgrade to findbugs 2 and fix these.", "comments": ["We currently use this version of findbugs-maven-plugin across hadoop trunk: http://mojo.codehaus.org/findbugs-maven-plugin-2.3.2/  Latest is 2.5.1 that uses 2.0.0 findbugs, at http://mojo.codehaus.org/findbugs-maven-plugin/  Making some changes to see if merely switching over works.", "Here's a findbugs converted to HTML report over HDFS generated with the 2.5.1 version. Reports 53 new warnings.", "Here's the patch that bumps the version. Had to do it in two places.", "Here's a list of all findbug reports across the hadoop project, in HTML and preserved in their own dirs, archived.", "How should we address these individual projects though? Via sub-tasks or one global patch? I can do most of them, and compile a list we'd have to discuss about.", "Thanks Harsh, let's have a jira per project to switch over to the new plugin version and address the warnings in the patch that bumps the version.", "I agree with Eli on this.  Switching to a new findbugs is a great thing, but I don't want to break the pre-commit builds in the process.  If we do that new findbugs issues will get past it simply because 54 looks a lot like 53 and it is simpler to say I didn't break it then to look at all 54 of the errors to see what the new ones might be.", "Filed HDFS-4014 for HDFS.", "Can you pls file one jira rather than tonnes to fix this? Thanks.", "Attaching the 10 findbugs reports for common and the various hadoop-tools projects and hadoop-auth. Common has 100 or so warnings and there's a couple dozen spread out around the others. Probably makes sense to do these like HDFS-4014, sensitive changes get their own patch and the general new classes of warning can be handled in bulk in a small set of patches.", "Closing this issue as all sub-tasks has been closed. Now hadoop use findbugs 3.0.0 and we are fixing new warnings in HADOOP-10477."], "tasks": {"summarization": "Fix issues identified by findsbugs2 - Harsh recently ran findbugs 2 (instead of 1.3.9 which is what jenkins runs) and it showed thousands ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8594 about?", "answer": "Fix issues identified by findsbugs2"}}}
{"issue_id": "HADOOP-8593", "project": "HADOOP", "title": "add  the missed @Override to methods in Metric/Metric2 package", "status": "Resolved", "priority": "Minor", "reporter": "Brandon Li", "assignee": "Brandon Li", "created": "2012-07-13T22:29:39.000+0000", "updated": "2016-05-12T18:25:43.000+0000", "labels": [], "description": "Adding @Override to the proper methods to take advantage of the compiler checking and make the code more readable.", "comments": ["+1. I also think some of the code does not follow coding guidelines. We should fix that too.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12536470/HADOOP-8593.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1196//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1196//console  This message is automatically generated.", "{quote}some of the code does not follow coding guidelines. We should fix that too{quote} Will do more code cleanup in the future.", "The test failures are not introduced by this patch. All the failed tests passed in my local test.", "I committed the patch. Thank you Brandon.", "Integrated in Hadoop-Hdfs-trunk-Commit #2551 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2551/])     HADOOP-8593. Add missed @Override annotations in Metric/Metrics2 package. Contributed by Brandon Li. (Revision 1362294)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362294 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsUtil.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/file/FileContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext31.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/CompositeContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsIntValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsLongValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java", "Integrated in Hadoop-Common-trunk-Commit #2486 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2486/])     HADOOP-8593. Add missed @Override annotations in Metric/Metrics2 package. Contributed by Brandon Li. (Revision 1362294)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362294 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsUtil.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/file/FileContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext31.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/CompositeContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsIntValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsLongValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2506 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2506/])     HADOOP-8593. Add missed @Override annotations in Metric/Metrics2 package. Contributed by Brandon Li. (Revision 1362294)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362294 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsUtil.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/file/FileContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext31.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/CompositeContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsIntValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsLongValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java", "Hi Suresh, Can you also backport this into branch-2? Helps avoid backport pains, since this touches and adds a lot of lines. Thanks!", "Integrated in Hadoop-Hdfs-trunk #1106 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1106/])     HADOOP-8593. Add missed @Override annotations in Metric/Metrics2 package. Contributed by Brandon Li. (Revision 1362294)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362294 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsUtil.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/file/FileContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext31.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/CompositeContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsIntValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsLongValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java", "Integrated in Hadoop-Mapreduce-trunk #1139 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1139/])     HADOOP-8593. Add missed @Override annotations in Metric/Metrics2 package. Contributed by Brandon Li. (Revision 1362294)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1362294 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsServlet.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/MetricsUtil.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/file/FileContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/ganglia/GangliaContext31.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/CompositeContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContext.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsIntValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsLongValue.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MethodMetric.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/lib/MutableStat.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/FileSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java"], "tasks": {"summarization": "add  the missed @Override to methods in Metric/Metric2 package - Adding @Override to the proper methods to take advantage of the compiler checking and make the code ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8593 about?", "answer": "add  the missed @Override to methods in Metric/Metric2 package"}}}
{"issue_id": "HADOOP-8592", "project": "HADOOP", "title": "Hadoop-auth should use o.a.h.util.Time methods instead of System#currentTimeMillis", "status": "Open", "priority": "Minor", "reporter": "Eli Collins", "assignee": null, "created": "2012-07-12T19:35:02.000+0000", "updated": "2012-08-15T16:29:58.000+0000", "labels": [], "description": "HDFS-3641 moved HDFS' Time methods to common so they can be used by MR (and eventually others). We should replace used of System#currentTimeMillis in MR with Time#now (or Time#monotonicNow when computing intervals, eg to sleep).", "comments": ["hadoop-auth does not depend on hadoop-common, but the other way around."], "tasks": {"summarization": "Hadoop-auth should use o.a.h.util.Time methods instead of System#currentTimeMillis - HDFS-3641 moved HDFS' Time methods to common so they can be used by MR (and eventually others). We s...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8592 about?", "answer": "Hadoop-auth should use o.a.h.util.Time methods instead of System#currentTimeMillis"}}}
{"issue_id": "HADOOP-8591", "project": "HADOOP", "title": "TestZKFailoverController tests time out", "status": "Resolved", "priority": "Major", "reporter": "Eli Collins", "assignee": "Aaron Myers", "created": "2012-07-11T01:45:50.000+0000", "updated": "2013-05-02T02:29:54.000+0000", "labels": ["test-fail"], "description": "Looks like the TestZKFailoverController timeout needs to be bumped.  {noformat} java.lang.Exception: test timed out after 30000 milliseconds \tat java.lang.Object.wait(Native Method) \tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:460) \tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:648) \tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:58) \tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:593) \tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:590) \tat java.security.AccessController.doPrivileged(Native Method) \tat javax.security.auth.Subject.doAs(Subject.java:396) \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1334) \tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:590) \tat org.apache.hadoop.ha.TestZKFailoverController.testOneOfEverything(TestZKFailoverController.java:575) {noformat}", "comments": ["I think this JIRA should be moved to Common. Eli, do you agree?", "Yup, thanks.", "I looked into this today and realized that it was a problem with a particular Jenkins slave. Whenever a pre-commit test or nightly build was run on hadoop1, it would fail. Whenever it was run anywhere else, it would pass. When I logged in to hadoop1, I noticed that there were a bunch of pre-commit processes and even a nightly build that had been running for weeks or months. After killing these zombie processes, TestZKFailoverController now passes reliably on hadoop1."], "tasks": {"summarization": "TestZKFailoverController tests time out - Looks like the TestZKFailoverController timeout needs to be bumped.  {noformat} java.lang.Exception:...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8591 about?", "answer": "TestZKFailoverController tests time out"}}}
{"issue_id": "HADOOP-8590", "project": "HADOOP", "title": "Backport HADOOP-7318 (MD5Hash factory should reset the digester it returns) to branch-1", "status": "Resolved", "priority": "Major", "reporter": "Todd Lipcon", "assignee": null, "created": "2012-07-12T03:25:40.000+0000", "updated": "2017-11-24T11:22:39.000+0000", "labels": [], "description": "I ran into this bug on branch-1 today, it seems like we should backport it.", "comments": ["Is it still a valid request? Do we still support branch-1?"], "tasks": {"summarization": "Backport HADOOP-7318 (MD5Hash factory should reset the digester it returns) to branch-1 - I ran into this bug on branch-1 today, it seems like we should backport it....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8590 about?", "answer": "Backport HADOOP-7318 (MD5Hash factory should reset the digester it returns) to branch-1"}}}
{"issue_id": "HADOOP-8589", "project": "HADOOP", "title": "ViewFs tests fail when tests and home dirs are nested", "status": "Closed", "priority": "Major", "reporter": "Andrey Klochkov", "assignee": "Sanjay Radia", "created": "2012-07-11T23:27:00.000+0000", "updated": "2014-09-03T23:11:07.000+0000", "labels": [], "description": "TestFSMainOperationsLocalFileSystem fails in case when the test root directory is under the user's home directory, and the user's home dir is deeper than 2 levels from /. This happens with the default 1-node installation of Jenkins.   This is the failure log:  {code} org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here \tat org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244) \tat org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334) \tat org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167) \tat org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167) \tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094) \tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79) \tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128) \tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110) \tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290) \tat org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76) \tat org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)  ...  Standard Output 2012-07-11 22:07:20,239 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /var/lib {code}  The reason for the failure is that the code tries to mount links for both \"/var\" and \"/var/lib\", and it fails for the 2nd one as the \"/var\" is mounted already.  The fix was provided in HADOOP-8036 but later it was reverted in HADOOP-8129.", "comments": ["I'm proposing a simple fix: mount the top level components of both test root dir (which is being done already) and home dir. In this case it would work fine in the described case. I tested it on a default Jenkins installation (1-node), on a Jenkins with slaves, on a MacOS laptop and on a Linux machine to verify it's addressing all kinds of home directories correctly.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12536143/HADOOP-8859.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1195//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1195//console  This message is automatically generated.", "HADOOP-8129 just brought back the test link, since HADOOP-8036 implicitly assumed that the tests are run under home and replaced the test link with the home link. So after HADOOP-8129 we have the two links.  Your patch basically changes how HADOOP-8036 adds the home link:  {code} -    ConfigUtil.addLink(conf, homeDirRoot, -        fsTarget.makeQualified(new Path(homeDirRoot)).toUri()); +    String homeDirFirstComponent = getDirFirstComponent(homeDirRoot); +    ConfigUtil.addLink(conf, homeDirFirstComponent, +        fsTarget.makeQualified(new Path(homeDirFirstComponent)).toUri()); {code}  Versus what was done in HADOOP-8036:  {code} +    String homeDirRoot = fsTarget.getHomeDirectory() +        .getParent().toUri().getPath(); +    ConfigUtil.addLink(conf, homeDirRoot, +        fsTarget.makeQualified(new Path(homeDirRoot)).toUri()); {code}  So, the statement in your description: \"The fix was provided in HADOOP-8036 but later it was reverted in HADOOP-8129\" looks confusing, since your patch is basically changing how HADOOP-8036 adds the home link. Can you please clarify?", "The problem with the current code is that those 2 links can point to nested directories. In case of tests being run on a Jenkins master in it's default configuration one of the links the code will try to create will point to \"/var\" and another to \"/var/lib\", and the creation of the second link will fail with \"Path /var already exists as dir; cannot create link\". I'm proposing to use just the first component of the path for both links, i.e. cut off everything after \"/var\" in my example. It's already being done for the test link, and my patch is adding the same logic for the home link.", "bq. I'm proposing to use just the first component of the path for both links, i.e. cut off everything after \"/var\" in my example. It's already being done for the test link, and my patch is adding the same logic for the home link.  * Can you clarify that in the ticket description since it is misleading? * Would it be better to check if (homeDirFirstComponent == testDirFirstComponent), and just add a single link, instead of adding the same link twice?", "I'd like to see viewfs tests only accessing directories mounted under the build.test.dir.  There's been a number of recurring bugs centered around the mounting of the user's home dir.  Are you aware the trash test tries to blow away your home directory to verify it won't delete an ancestor of the trash dir?  Shining illustration of why everything should be under build.test.dir.", "I agree Daryn. Yes, TestViewFsTrash tries to delete the user's home directory!! Which raise the question of what if this testcase fails and it successfully deletes the user's home directory. I think this specific issue need to be prioritized and addressed in a separate ticket.", "What I'm trying to say is if the viewfs tests are confined to build.test.dir, then the mount path problems _and_ the trash problem are both solved.  They are only orthogonal if you believe that viewfs should mount your home dir, and trust it to not munge the contents of you home directory, and/or your ~/.Trash if you are on OS X.  This is at least the third jira to \"fix\" mounting home dirs.  It always fixes for some & breaks for others.  Are there drawbacks to confining the tests to mounts within build.test.dir?", "Providing an alternative patch: 1. An overloaded implementation of LocalFileSystem is used to a) override the tests root dir as the home dir b) validate that there are no \"delete\" operations executed out of the tests root dir 2. Just one link is created in the ViewFS mount table, the one for the tests root dir", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12536856/HADOOP-8589.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 3 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1200//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1200//console  This message is automatically generated.", "This is indeed still a problem on trunk:  /home/foo/bar  {code} Running org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 49, Skipped: 0, Time elapsed: 1.28 sec <<< FAILURE! {code}  /home/harsh  {code} Running org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.574 sec {code}  /foo  {code} Running org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 49, Skipped: 0, Time elapsed: 1.29 sec <<< FAILURE! {code}", "Is that even with Andrey's latest patch applied?", "No I didn't apply it, let me try today since I have the user env. anyway.", "Confirming - Applying Andrey's patch does solve the issue in all 3 cases (same as above, all org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem tests passed in each count).  I've rebased his patch for trunk as it didn't apply cleanly (line offset issues). Lets have a jenkins run of the same again.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12546983/HADOOP-8589.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1540//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1540//console  This message is automatically generated.", "Daryn - Want to give this another look? Looks clean and passes on /foo, /home/harsh and /home/foo/bar as home directories.", "I like the sanity check in {{TestLFS#delete}}, but should the local fs maybe just use a chrooted fs to the build test dir?  That would help ensure that viewfs doesn't try to mount / - I think it does or used to do that.", "I had missed this jira and recently fixed this in another jira (will I will close as a duplicate). This fixes the buig by linking the first component in tests and home dir (same as what others have suggested in this jira). It also fixes the tests when run via eclipse where the test dir is relative to wd - ie it sets up wd correctly for viewfs.  I have cleaned up things so that all the set up is done in a single setup routing (one for viewFs and one viewFileSystem)  As part of this testing I found another bug for homedir and chrootedfs - also fixed this.  I have attached the patch here.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12548817/hadoop-8589-sanjay.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1615//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1615//console  This message is automatically generated.", "Andrey    in your patch why did you move TestLFS from TestViewFsTrash to ViewFileSystemTestSetup? While ViewFileSystemTestSetup is used in many tests, TestLFS is used in only in ViewFileSystemTestSetup.", "I think we should separate the trash fix into a separate jira. I have extracted it out and, andrey if you agree I can create a new jira (assigned to you) and attach the trash patch to the new jira.", "Sanjay, can you please describe the changes you're making in your patch? It's just so much bigger and seems to be fixing a part of the problem. Right now I don't see a necessity to do further work if the issue was fixed already and the patch was validated (see comments from Harsh).", "My patch does the following (well commented in the patch it self) # sets the mount points =  {code} + * We set a viewFileSystems with 3 mount points:  + * 1) /<firstComponent>\" of testdir  pointing to same in  target fs + * 2)   /<firstComponent>\" of home  pointing to same in  target fs  + * 3)  /<firstComponent>\" of wd  pointing to same in  target fs + * (note in many cases the link may be the same - viewFileSytem handles this) {code} # sets viewfs's wd  since otherwise if you runs the tests in eclipse it create the testdirs in your home dir. # teardown ensures that junk is not left in your home dir (due to previous wd fix). # the above changes are made for BOTH viewfs and viewFileSystem tests # a couple of tests now calls the common setup routines # the chroot test fails on mac - fixed that  I tried  doing some of the above in separate patches but then some tests were failing in either mac or linux.  My patch does not fix trash issue. I believe this should be separate patch (but if you prefer i can incorporate it here). However the TestLFS should be moved back to where it was as I have indicated in a previous comment: {quote} in your patch why did you move TestLFS from TestViewFsTrash to ViewFileSystemTestSetup? While ViewFileSystemTestSetup is used in many tests, TestLFS is used in only in ViewFileSystemTestSetup. {quote}  If you prefer we can let your patch through (but move the TestLFS back to TestViewFsTrash since it is used only there). I can then do the cleanup I want to do in a separate jira.", "Is it possible to just make sure everything is rooted under build.test.data?  Then we may not need to worry about how deep the home dir is in the directory structure?", "Maybe use a chroot fs over local fs to lock it down to build.test.data?", "I had planned to make chroot fs a full fledged fs but it turns out it cannot and now it is used mostly internally in viewfs as a implementation. (I can't remember the reasons for why it cannot be made a full fledged file system.) Are you suggesting this because trash test can delete your home dir? BTW  the issue of a trash test  deleting the home dir is also possible in the other trash tests (ie the non-viewfs trash tests). I suggest we move the trash issue to another jira.", "{quote} This is indeed still a problem on trunk: /home/foo/bar ... /home/harsh .. /foo .. {quote} Harsh what did you mean in your comment above - did you run the tests with  the home directory or wd set to the above 3 paths?", "Sanjay, The idea of moving TestLFS into ViewFileSystemTestSetup was to make all tests related to viewfs use the same configuration with 1) home dir located under test dir 2) preventing any delete operations outside of test dir. I agree that using chroot fs would be even better - all the changes in the local FS would be locked under test dir, which is the right thing, but I really don't know any details on whether chroot is capable of that.", "bq. Harsh what did you mean in your comment above - did you run the tests with the home directory or wd set to the above 3 paths?  I created 3 users - bar, harsh and foo with those HOME dirs and cloned the patched trunk repo into each of them to run the tests from (i.e. in an su'd environment).  Lemme know if you'd like me to repeat the tests with your patch!", "bq. The idea of moving TestLFS into ViewFileSystemTestSetup ... *Only* the trash tests use TestLFS -- TestLFS is not used in any of the other viewfs tests. ViewFileSystemTestSetup should not be setting up things that only ONE of the many tests that call it need. Indeed TestLFS should move to a more general test-helper class because TestLFS is used in non-viewfs test - the other test trash (e.g. TestTrash) has its own copy of TestLFS.   But your fix to TestLFS to ensure that there is no accidental delete during a TestTrash of home dirs is fine. I suggest we file a jira to make that change. If as part of that fix you also want to create a common TestLFS (for viewfs tests and for TestTrash) then that also makes sense.  At this stage I suggest we do the following # file a jira to fix TestLFS - extract your fix from this patch in this jira. # either  ** let a modified version of your patch without TestLFS go through and then I file another Jira to get my patch through (since it does more cleanup) OR  ** just let my patch go through in", "bq. just let my patch go through in  I'm fine with this if you're saying it does additional cleanup. I'm OK with extracting changes in TestLFS too.", "bq. Lemme know if you'd like me to repeat the tests with your patch! Harsh, please repeat your  tests against the my patch  - thanks.", "Sorry for the delay:  With Sanjay's patch, with various HOMEDIRs: {{mvn clean install -Dtest=TestChRootedFileSystem,TestChRootedFs,TestFcMainOperationsLocalFs,TestFSMainOperationsLocalFileSystem}}:  * /home/harsh [Regular home dir]  Tests run: 174, Failures: 0, Errors: 0, Skipped: 0  {code} Running org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.583 sec Running org.apache.hadoop.fs.viewfs.TestChRootedFileSystem Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.25 sec Running org.apache.hadoop.fs.viewfs.TestChRootedFs Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.085 sec Running org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.071 sec Running org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs Tests run: 54, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.279 sec {code}  * /foo [For user 'foo']  Tests run: 277, Failures: 0, Errors: 206, Skipped: 0  {code} Running org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.701 sec Running org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs Tests run: 108, Failures: 0, Errors: 108, Skipped: 0, Time elapsed: 2.171 sec <<< FAILURE! testFsStatus(org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs)  Time elapsed: 644 sec  <<< ERROR! java.lang.StringIndexOutOfBoundsException: String index out of range: -1 \tat java.lang.String.substring(String.java:1949) \tat org.apache.hadoop.fs.viewfs.ViewFsTestSetup.linkUpFirstComponents(ViewFsTestSetup.java:115) \tat org.apache.hadoop.fs.viewfs.ViewFsTestSetup.setupForViewFsLocalFs(ViewFsTestSetup.java:76) \tat org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs.setUp(TestFcMainOperationsLocalFs.java:42) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \tat java.lang.reflect.Method.invoke(Method.java:616) \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27) \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) \tat org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79) \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71) \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49) \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236) \tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) \tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) \tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \tat java.lang.reflect.Method.invoke(Method.java:616) \tat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) \tat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) \tat org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)  testFsStatus(org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs)  Time elapsed: 645 sec  <<< ERROR! java.lang.NullPointerException \tat org.apache.hadoop.fs.FileContextTestHelper.getAbsoluteTestRootPath(FileContextTestHelper.java:82) \tat org.apache.hadoop.fs.FileContextMainOperationsBaseTest.tearDown(FileContextMainOperationsBaseTest.java:105) \tat org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs.tearDown(TestFcMainOperationsLocalFs.java:49) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \tat java.lang.reflect.Method.invoke(Method.java:616) \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:37) \tat org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79) \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71) \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49) \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) \tat org.junit.runners.ParentRunner.run(ParentRunner.java:236) \tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) \tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) \tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \tat java.lang.reflect.Method.invoke(Method.java:616) \tat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) \tat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) \tat org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) \tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) \tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75) --truncated-- {code}  * /home/bar/eggs [For user 'eggs']  Tests run: 174, Failures: 0, Errors: 0, Skipped: 0  {code} Running org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.648 sec Running org.apache.hadoop.fs.viewfs.TestChRootedFileSystem Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.268 sec Running org.apache.hadoop.fs.viewfs.TestChRootedFs Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.094 sec Running org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem Tests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.967 sec Running org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs Tests run: 54, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.025 sec {code}  Edit: Truncated command so that all web page loads do not need to serve back MBs of logs now that its no longer useful.", "[~sanjay.radia], can you please reassign to yourself? Thanks.", "Updated patch that fixes the bug related to home dir being at root (e.g. /joe). I ran the tests though 3 different home dirs: /Users/foo, /foo, /x/y/foo", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12552168/Hadoop-8589.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1707//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1707//console  This message is automatically generated.", "{code} +++ hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java @@ -156,8 +156,11 @@ public Path getResolvedQualifiedPath(final Path f)        @Override    public Path getHomeDirectory() { +    return super.getHomeDirectory(); +    /*      return  new Path(\"/user/\"+System.getProperty(\"user.name\")).makeQualified(            getUri(), null); +          */    } {code} The getHomeDirectory() in ChRootedFileSystem only calls super.getHomeDirectory().  Then, we can simply remove the entire method but not overriding it.  Patch looks good other than that.", "Update patch - incorporates Nicolas's feedaback and run successfully against homedir of /joe, /x/joe, /x/y/joe", "+1 the new patch looks good.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12552511/Hadoop-8589-v2.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1718//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1718//console  This message is automatically generated.", "Integrated in Hadoop-trunk-Commit #2977 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/2977/])     HADOOP-8589 ViewFs tests fail when tests and home dirs are nested (sanjay Radia) (Revision 1406939)       Result = SUCCESS sradia : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406939 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestFcMainOperationsLocalFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemTestSetup.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFsTestSetup.java", "Integrated in Hadoop-Yarn-trunk #30 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/30/])     HADOOP-8589 ViewFs tests fail when tests and home dirs are nested (sanjay Radia) (Revision 1406939)       Result = SUCCESS sradia : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406939 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestFcMainOperationsLocalFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemTestSetup.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFsTestSetup.java", "Integrated in Hadoop-Hdfs-trunk #1220 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1220/])     HADOOP-8589 ViewFs tests fail when tests and home dirs are nested (sanjay Radia) (Revision 1406939)       Result = SUCCESS sradia : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406939 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestFcMainOperationsLocalFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemTestSetup.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFsTestSetup.java", "Integrated in Hadoop-Mapreduce-trunk #1250 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1250/])     HADOOP-8589 ViewFs tests fail when tests and home dirs are nested (sanjay Radia) (Revision 1406939)       Result = FAILURE sradia : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1406939 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/DelegateToFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/FileSystemTestHelper.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestChRootedFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/TestFcMainOperationsLocalFs.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFileSystemTestSetup.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/viewfs/ViewFsTestSetup.java", "The reported issue is still not fixed. If tests are run on a Jenkins master node, with Jenkins using it's default home (/var/lib/jenkins), view fs tests fail with the error given in the description of this Jira.", "Apparently it's fixed in trunk only, while the \"target versions\" field points to 3.x/0.23/2.x. Can it be backported to 2.x/0.23 as well?", "I merged this change to branch-2."], "tasks": {"summarization": "ViewFs tests fail when tests and home dirs are nested - TestFSMainOperationsLocalFileSystem fails in case when the test root directory is under the user's h...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8589 about?", "answer": "ViewFs tests fail when tests and home dirs are nested"}}}
{"issue_id": "HADOOP-8588", "project": "HADOOP", "title": "SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty", "status": "Resolved", "priority": "Minor", "reporter": "Harsh J", "assignee": "Sho Shimauchi", "created": "2012-07-11T14:27:41.000+0000", "updated": "2016-05-12T18:25:23.000+0000", "labels": [], "description": "The SerializationFactory throws an NPE if CommonConfigurationKeys.IO_SERIALIZATIONS_KEY is set to an empty list in the config.  It should rather print a WARN log indicating the serializations list is empty, and start up without any valid serialization classes.", "comments": ["Attached patch check the value of CommonConfigurationKeys.IO_SERIALIZATIONS_KEY before  conf.getStrings() is run. Also I refactord test methods to cover all codes in SerializationFactory class.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541518/HADOOP-8588.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1502//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1502//console  This message is automatically generated.", "Sorry for the late review Sho, I'd been busy. Some comments:  {code} +    if (conf.get(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY) == \"\") { {code}  The double equal-to seems wrong to compare strings? Can we use .isEmpty() or .equals() instead? May also be worth having a Configuration method that checks for existence and for emptiness rather than us always comparing if it has one manually like this all the time (but this is just a nit, lets do it in another JIRA if agreed, that doesn't depend on the patch here).  {code} +  public void testGetDesirializer() { {code}  s/Desirializer/Deserializer  {code} LOG.warn(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY +          + \" is empty. start up without any valid serialization classes.\"); {code}  Maybe this can be refined to: \"Serialization for various data types may not be available. Please configure X properly to have serialization support (it is currently not set).\"?", "Thanks Harsh! I updated the patch.  {quote} -1 core tests. The patch failed these unit tests in hadoop-common-project/hadoop-common: org.apache.hadoop.ha.TestZKFailoverController {quote}  This test timed out but it seems nothing to do with my patch.", "Thanks much Sho, I verified the tests locally and have committed this to trunk (r1389002).", "Sorry, meant to mark as resolved.", "Integrated in Hadoop-Common-trunk-Commit #2757 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2757/])     HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh) (Revision 1389002)       Result = SUCCESS harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1389002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2820 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2820/])     HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh) (Revision 1389002)       Result = SUCCESS harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1389002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java", "Attaching committed patch as I had also cleaned up a couple of whitespace and one relevant indentation fix while committing.", "Integrated in Hadoop-Mapreduce-trunk-Commit #2779 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2779/])     HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh) (Revision 1389002)       Result = FAILURE harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1389002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java", "Integrated in Hadoop-Hdfs-trunk #1174 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1174/])     HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh) (Revision 1389002)       Result = SUCCESS harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1389002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java", "Integrated in Hadoop-Mapreduce-trunk #1205 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1205/])     HADOOP-8588. SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty. Contributed by Sho Shimauchi. (harsh) (Revision 1389002)       Result = SUCCESS harsh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1389002 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java"], "tasks": {"summarization": "SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty - The SerializationFactory throws an NPE if CommonConfigurationKeys.IO_SERIALIZATIONS_KEY is set to an...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8588 about?", "answer": "SerializationFactory shouldn't throw a NullPointerException if the serializations list is empty"}}}
{"issue_id": "HADOOP-8587", "project": "HADOOP", "title": "HarFileSystem access of harMetaCache isn't threadsafe", "status": "Closed", "priority": "Minor", "reporter": "Eli Collins", "assignee": "Eli Collins", "created": "2012-07-11T03:38:30.000+0000", "updated": "2013-05-06T03:16:52.000+0000", "labels": [], "description": "HarFileSystem's use of the static harMetaCache map is not threadsafe. Credit to Todd for pointing this out.", "comments": ["Patch attached, use a ConcurrentHashMap instead.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535996/hadoop-8587.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1192//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1192//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1192//console  This message is automatically generated.", "Same patch for branch-1. Ran TestHarFileSystem for sanity.", "+1 Looks simple enough.  I think the findbugs and test failures are not related, and it wouldn't be easy to write a test for this.", "Thanks for the review Daryn. I've committed this to trunk and merged to branch-2 and branch-1.", "Integrated in Hadoop-Common-trunk-Commit #2453 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2453/])     HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360448)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360448 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2519 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2519/])     HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360448)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360448 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2471 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2471/])     HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360448)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360448 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Integrated in Hadoop-Hdfs-trunk #1101 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1101/])     HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360448)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360448 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Integrated in Hadoop-Mapreduce-trunk #1134 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1134/])     HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360448)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360448 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Integrated in Hadoop-Hdfs-0.23-Build #312 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/312/])     svn merge -c 1360448 FIXES: HADOOP-8587. HarFileSystem access of harMetaCache isn't threadsafe. Contributed by Eli Collins (Revision 1360738)       Result = SUCCESS bobby : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360738 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java", "Added 1.2.0 to fixVersion, per CHANGES.txt and svn log."], "tasks": {"summarization": "HarFileSystem access of harMetaCache isn't threadsafe - HarFileSystem's use of the static harMetaCache map is not threadsafe. Credit to Todd for pointing th...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8587 about?", "answer": "HarFileSystem access of harMetaCache isn't threadsafe"}}}
{"issue_id": "HADOOP-8586", "project": "HADOOP", "title": "Fixup a bunch of SPNEGO misspellings", "status": "Closed", "priority": "Major", "reporter": "Eli Collins", "assignee": "Eli Collins", "created": "2012-07-11T03:24:12.000+0000", "updated": "2012-10-11T17:45:05.000+0000", "labels": [], "description": "SPNEGO is misspelled as \"SPENGO\" a bunch of places.", "comments": ["Patch attached.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535975/hadoop-8586.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1190//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1190//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1190//console  This message is automatically generated.", "No tests because it's just spelling mistakes. Findbugs warning is unrelated (Colin is working on a fix for it). Test failures are unrelated as well.", "+1, lgtm.", "Integrated in Hadoop-Common-trunk-Commit #2446 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2446/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360048)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360048 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Thanks for the review Harsh. I've committed this and merged to branch-2.", "Integrated in Hadoop-Hdfs-trunk-Commit #2513 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2513/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360048)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360048 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Hdfs-trunk-Commit #2514 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2514/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360056)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360056 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Common-trunk-Commit #2448 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2448/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360056)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360056 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Mapreduce-trunk-Commit #2465 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2465/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360048)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360048 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Mapreduce-trunk-Commit #2466 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2466/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360056)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360056 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Hdfs-trunk #1100 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1100/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360056) HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360048)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360056 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm  eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360048 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "Integrated in Hadoop-Mapreduce-trunk #1133 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1133/])     HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360056) HADOOP-8586. Fixup a bunch of SPNEGO misspellings. Contributed by Eli Collins (Revision 1360048)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360056 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm  eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360048 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/docs/src/documentation/content/xdocs/HttpAuthentication.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/HarFileSystem.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/packages/templates/conf/hdfs-site.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/resources/httpfs-default.xml * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/UsingHttpTools.apt.vm * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/site/apt/index.apt.vm * /hadoop/common/trunk/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/WebHDFS.apt.vm", "I merged this to branch-1."], "tasks": {"summarization": "Fixup a bunch of SPNEGO misspellings - SPNEGO is misspelled as \"SPENGO\" a bunch of places....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8586 about?", "answer": "Fixup a bunch of SPNEGO misspellings"}}}
{"issue_id": "HADOOP-8585", "project": "HADOOP", "title": "Fix initialization circularity between UserGroupInformation and HadoopConfiguration", "status": "Closed", "priority": "Minor", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-10T23:42:36.000+0000", "updated": "2012-10-11T17:45:09.000+0000", "labels": [], "description": "Fix findbugs warning about initialization circularity between UserGroupInformation and UserGroupInformation#HadoopConfiguration.  From the findbugs text: {code} Initialization circularity between org.apache.hadoop.security.UserGroupInformation and org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration \t  Bug type IC_INIT_CIRCULARITY (click for details) In class org.apache.hadoop.security.UserGroupInformation In class org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration At UserGroupInformation.java:[lines 76-1395] {code}", "comments": ["The patch looks good to me. +1 pending Jenkins.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535953/HDFS-3632.001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1188//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1188//console  This message is automatically generated.", "I've just committed this to trunk and branch-2. Thanks a lot for fixing this so quickly, Colin.", "Integrated in Hadoop-Hdfs-trunk-Commit #2520 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2520/])     HADOOP-8585. Fix initialization circularity between UserGroupInformation and HadoopConfiguration. Contributed by Colin Patrick McCabe. (Revision 1360498)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360498 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "Integrated in Hadoop-Mapreduce-trunk-Commit #2473 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2473/])     HADOOP-8585. Fix initialization circularity between UserGroupInformation and HadoopConfiguration. Contributed by Colin Patrick McCabe. (Revision 1360498)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360498 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "Integrated in Hadoop-Common-trunk-Commit #2455 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2455/])     HADOOP-8585. Fix initialization circularity between UserGroupInformation and HadoopConfiguration. Contributed by Colin Patrick McCabe. (Revision 1360498)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360498 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "Wouldn't it have been easier to just suppress the warning as a false alarm?", "Integrated in Hadoop-Hdfs-trunk #1101 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1101/])     HADOOP-8585. Fix initialization circularity between UserGroupInformation and HadoopConfiguration. Contributed by Colin Patrick McCabe. (Revision 1360498)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360498 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "Integrated in Hadoop-Mapreduce-trunk #1134 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1134/])     HADOOP-8585. Fix initialization circularity between UserGroupInformation and HadoopConfiguration. Contributed by Colin Patrick McCabe. (Revision 1360498)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1360498 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "bq. Wouldn't it have been easier to just suppress the warning as a false alarm?  That would certainly work, but it doesn't seem much easier to me. I also don't see how the method used by this patch could possibly cause any problems, as it changes the behavior back to what it was before the recently-committed HDFS-3568, i.e. each invocation of newLoginContext will create a new HadoopConfiguration object.", "Clearly they have the same effect, because the configuration doesn't have any state. The new approach just creates a throw away object for each invocation. In the future, you should probably just revert the previous fix and update it rather than create a new jira the next day.", "bq. Clearly they have the same effect, because the configuration doesn't have any state. The new approach just creates a throw away object for each invocation.  This is not really a \"new\" approach. This is going back to the way it was done before HDFS-3568.  bq. In the future, you should probably just revert the previous fix and update it rather than create a new jira the next day.  We routinely file follow-up JIRAs to fix trivial problems that were introduced by a recent commit. I don't see how this is any different."], "tasks": {"summarization": "Fix initialization circularity between UserGroupInformation and HadoopConfiguration - Fix findbugs warning about initialization circularity between UserGroupInformation and UserGroupInfo...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8585 about?", "answer": "Fix initialization circularity between UserGroupInformation and HadoopConfiguration"}}}
{"issue_id": "HADOOP-8584", "project": "HADOOP", "title": "test-patch.sh should not immediately exit when no tests are added or modified", "status": "Resolved", "priority": "Major", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-10T19:24:20.000+0000", "updated": "2016-05-12T18:23:51.000+0000", "labels": [], "description": "test-patch.sh should not immediately exit when no tests are added or modified.  Although it's good to note whether or not a patch introduces or modifies tests, it's not good to abort the Jenkins patch process if it did not.", "comments": ["Btw, was this change made to demand that all patches, unless doc or log changes, be provided with a patch?", "bq. Btw, was this change made to demand that all patches, unless doc or log changes, be provided with a patch?  No.  It seems to have been an unintented issue introduced by the patch for HADOOP-8523.", "+1", "I've committed this to trunk.", "Integrated in Hadoop-Hdfs-trunk #1100 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1100/])     HADOOP-8584. test-patch.sh should not immediately exit when no tests are added or modified. Contributed by Colin Patrick McCabe (Revision 1359902)       Result = FAILURE eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359902 Files :  * /hadoop/common/trunk/dev-support/test-patch.sh * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1133 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1133/])     HADOOP-8584. test-patch.sh should not immediately exit when no tests are added or modified. Contributed by Colin Patrick McCabe (Revision 1359902)       Result = SUCCESS eli : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359902 Files :  * /hadoop/common/trunk/dev-support/test-patch.sh * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt"], "tasks": {"summarization": "test-patch.sh should not immediately exit when no tests are added or modified - test-patch.sh should not immediately exit when no tests are added or modified.  Although it's good t...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8584 about?", "answer": "test-patch.sh should not immediately exit when no tests are added or modified"}}}
{"issue_id": "HADOOP-8583", "project": "HADOOP", "title": "Globbing is not correctly handled in a few cases on Windows", "status": "Resolved", "priority": "Major", "reporter": "Ramya Sunil", "assignee": null, "created": "2012-07-10T18:10:12.000+0000", "updated": "2012-09-06T16:13:17.000+0000", "labels": [], "description": "Glob handling fails in a few cases on a Windows environment.  For example: {noformat} c:\\> hadoop dfs -ls / Found 2 items drwxrwxrwx   - Administrator supergroup          0 2012-07-06 15:00 /tmp drwxr-xr-x   - Administrator supergroup          0 2012-07-06 18:52 /user   c:\\> hadoop dfs -ls /tmpInvalid* Found 2 items drwxr-xr-x   - Administrator supergroup          0 2012-07-10 18:50 /user/Administrator/sortInputDir drwxr-xr-x   - Administrator supergroup          0 2012-07-10 18:50 /user/Administrator/sortOutputDir  c:\\> hadoop dfs -rmr /tmp/* Usage: java FsShell [-rmr [-skipTrash] <src> ] {noformat}", "comments": ["Basically, batch script expansion caused the problem.  Here is how the expansion happens on Windows: 1. cmd.exe passed \"/tmp/*\" to %HADOOP_HOME%\\bin\\hadoop script 2. hadoop script also passes \"/tmp/*\" to %HADOOP_HOME%\\bin\\hdfs.cmd 3. the expansion is done by the shell function make_command_arguments in hdfs.cmd. If it has problem expanding the name, it just simply drop it. 4. when FsShell gets the request, either it gets a list of expended name or no name. FsShell doesn't know the expended names are from outside HDFS. When there is no name with some command, FsShell complains and prints the usage information. {noformat} :make_command_arguments   if \"%2\" == \"\" goto :eof   set _count=0   if defined service_entry (set _shift=2) else (set _shift=1)   if defined config_override (set /a _shift=!_shift! + 2)   for %%i in (%*) do (                        <== expansion happens here!!!     set /a _count=!_count!+1     if !_count! GTR %_shift% (         if not defined _hdfsarguments (           set _hdfsarguments=%%i         ) else (           set _hdfsarguments=!_hdfsarguments! %%i         )     )   )   set hdfs-command-arguments=%_hdfsarguments%   goto :eof {noformat} Using single quotation marks around the pathname may result in getting the pathname dropped by the above function. Therefore it is not a good workaround. Looks like the above function needs to be fixed.", "I don't think the batch script should be attempting any expansion.  {{FsShell}} and other commands are going to expand both local and/or remote paths via {{FileSystem}} so no pre-expansion is necessary.  Any pre-expansion is going to cause problems if option args contain glob chars, and it may also cause unintended expansion of relative paths intended for a remote fs that happen to match to local paths (ie. /tmp/*.txt will expand to local fs paths even if the default fs is hdfs).", "correct. The above mentioned function is to process the rest of the arguments from user input. The expansion is just a bad side effect.", "This should have been fixed with HADOOP-8739, right?", "HADOOP-8739 should fix the same problem. Resolve this as dup.", "dup of HADOOP-8739"], "tasks": {"summarization": "Globbing is not correctly handled in a few cases on Windows - Glob handling fails in a few cases on a Windows environment.  For example: {noformat} c:\\> hadoop df...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8583 about?", "answer": "Globbing is not correctly handled in a few cases on Windows"}}}
{"issue_id": "HADOOP-8582", "project": "HADOOP", "title": "Improve error reporting for GZIP-compressed SequenceFiles with missing native libraries.", "status": "Open", "priority": "Minor", "reporter": "Paul Wilkinson", "assignee": null, "created": "2012-07-10T12:42:54.000+0000", "updated": "2012-07-13T14:26:07.000+0000", "labels": [], "description": "At present it is not possible to write or read block-compressed SequenceFiles using the GZIP codec without the native libraries being available.  The SequenceFile.Writer code checks for the availability of native libraries and throws a useful exception, but the SequenceFile.Reader doesn't do the same:  {noformat} Exception in thread \"main\" java.io.EOFException \tat java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:249) \tat java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:239) \tat java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:142) \tat java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58) \tat java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:67) \tat org.apache.hadoop.io.compress.GzipCodec$GzipInputStream$ResetableGZIPInputStream.<init>(GzipCodec.java:95) \tat org.apache.hadoop.io.compress.GzipCodec$GzipInputStream.<init>(GzipCodec.java:104) \tat org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:173) \tat org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:183) \tat org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1591) \tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1493) \tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1480) \tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475) \tat test.SequenceReader.read(SequenceReader.java:23) {noformat}", "comments": ["This should give a more helpful error message.", "Hi Paul, thanks for filing this and the patch! I've run into this as well.  Patch looks good but can you also add in a test (you can selectively disable loading of native libs via configuration if need be), so we don't regress from this test, if possible?  Also, perhaps we can instead be more specific in the error message (saying \"SequenceFile.Reader can't read Gzip compressed files without native-hadoop libraries\" or so? Feel free to improve the Writer when you're at it too, if needed)", "I think it would make more sense to avoid coupling {{SequenceFile}} against a specific codec.  Shouldn't {{GzipCodec}} generate the exception?  Perhaps in the ctor?", "Daryn,  There's a reason that check exists specifically in SequenceFile I think. See https://issues.apache.org/jira/browse/HADOOP-538?focusedCommentId=12444771&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12444771  In other places, GzipCodec works either ways I think (native or Java provided fallback), so we can't push back the thrower to it, as that'd break other apps.", "I skimmed the jira, but I'm not sure I groked it.  It sounds like the problem is zlib vs gzip headers?  Or that some of the header bytes are gobbled before the non-native gzip stream is created?", "Daryn,  Its sorta the latter. To be clearer, the reason is this, from HADOOP-538:  {quote} Arun:  Context: gzip is just zlib algo + extra headers. java.util.zip.GZIP{Input|Output}Stream and hence existing GzipCodec won't work with SequenceFile due the fact that java.util.zip.GZIP{Input|Output}Streams will try to read/write gzip headers in the constructors which won't work in SequenceFiles since we typically read data from disk onto buffers, these buffers are empty on startup/after-reset and cause the java.util.zip.GZIP{Input|Output}Streams to fail. {quote}", "Daryn, are you good with the patch's general approach, given the above?  Paul, will you be sending an updated patch soon? If not, let me know, and I'm happy to tweak on your behalf and add in those changes.", "Harsh, it sounds like you are happy with the approach, so I'll add in those changes and resubmit later today - thanks.", "+1 Sure, I don't particularly like, but I guess it's inevitable due to the design..."], "tasks": {"summarization": "Improve error reporting for GZIP-compressed SequenceFiles with missing native libraries. - At present it is not possible to write or read block-compressed SequenceFiles using the GZIP codec w...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8582 about?", "answer": "Improve error reporting for GZIP-compressed SequenceFiles with missing native libraries."}}}
{"issue_id": "HADOOP-8581", "project": "HADOOP", "title": "add support for HTTPS to the web UIs", "status": "Closed", "priority": "Major", "reporter": "Alejandro Abdelnur", "assignee": "Alejandro Abdelnur", "created": "2012-07-10T00:18:28.000+0000", "updated": "2013-10-06T00:05:35.000+0000", "labels": [], "description": "HDFS/MR web UIs don't work over HTTPS, there are places where 'http://' is hardcoded.", "comments": ["preview patch, documentation is missing. tested manually that web UI for MR/YARN & HDFS work over HTTPS. This patch requires HADOOP-8644.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538965/HADOOP-8581.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      -1 javac.  The patch appears to cause the build to fail.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1245//console  This message is automatically generated.", "applying the patch to the latest trunk from the GIT mirror it applies just fine (SVN seems down at the moment so I cannot try there)", "+1 looks good to me.", "Early comments.  I prefer splitting this into separate patches instead of one single patch that Jenkins cannot use. # There are unnecessary white space changes (e.g: WebAppProxyServlet.java). Indentation in some places is incorrect as well (4 spaces instead of two spaces). # core-site.xml - typo \"SSL for for the HTTP\". Can you please add more/better description for the new parameter added. # HttpServer. java - please do not turn checked exception GeneralSecurityException into RTE. Perhaps you could throw it as IOException # Add brief comments to TestSSLHttpServer.java # Not sure you needed to make getTaskLogsUrl() non-static", "bq. I prefer splitting this into separate patches instead of one single patch that Jenkins cannot use.  Why can't Jenkins use it? Cross-project patches should work now.", "bq. Why can't Jenkins use it? Cross-project patches should work now. That is good! I was not aware of it.", "@suresh, thanks for your detailed review. Attached is a patch incorporating all your feedback.  * splitting into different patches. As ATM noted, cross-projects work, the patch was nto applying because HADOOP-8644 was not yet committed.  I've also reverted using ${hadoop.ssl.enabled} as the default value for the mapreduce {{mapreduce.shuffle.ssl.enabled}}. With this change there is not pre-assumption that enabling SSL for the webui enables SSL for encrypted shuffle.", "re-uploading to see if jenkins takes notice.", "jenkins test-patch for some weird reason keeps ignoring this patch. Just run test-patch locally, following the result:  {code}     +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      -1 findbugs.  The patch appears to introduce 4 new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings. {code}  The 4 findbugs warnings are unrelated.", "canceling patch as I have to rebased it due to the yarn move.", "patch rebased to trunk (after yarn move)", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12539884/HADOOP-8581.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy:                    org.apache.hadoop.hdfs.TestDatanodeBlockScanner                   org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1267//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1267//console  This message is automatically generated.", "Patch looks pretty good to me, Tucu. Just a few small comments:  # Per our coding conventions, I don't think that HttpConfig#SSL_ENABLED should be all caps. # In the HttpServer constructor, move the .setHost and .setPort to after the if/else: {code} if (...) { ...   sslListener.setHost(bindAddress);   sslListener.setPort(port);   listener = sslListener; } else {   listener = createBaseListener(conf);   listener.setHost(bindAddress);   listener.setPort(port); } {code} # In the core-default.xml description, take out the word \"it\" and change \"webuis\" to \"web UIs\": {code} +    Whether to use SSL for the HTTP endpoints. If set to true, it the +    NameNode, DataNode, ResourceManager, NodeManager, HistoryServer and +    MapReduceAppMaster webuis will be served over HTTPS instead HTTP. {code} # Rather than go through the headache of writing out a core-default.xml containing the appropriate SSL config, how about just adding a setSslEnabledForTesting static function to HttpConfig? # Considering that every place you call HttpConfig#getScheme you immediately append \"://\", maybe just append that in HttpConfig#getScheme? Or perhaps have a HttpConfig#getPrefix which returns HttpConfig#getScheme() + \"://\" ? # I think you inadvertently incorrectly changed the indentation in HostUtil#getTaskLogUrl to be 4 spaces instead of 2. # There are some inadvertent and unnecessary whitespace changes in RMAppAttemptImpl.", "@atm, thx for there review. new patch addresses all your comments except for the generation of the core-site.xml. The MR AM needs that info coming from the core-site.xml, not from the job.xml. And the MR AM is started in a separate VM, thus cannot set it from the testcase bootstrap of the minicluster.  build and install a pseudo cluster, configured for ssl and verified pages work over ssl for all services.", "bq. new patch addresses all your comments except for the generation of the core-site.xml. The MR AM needs that info coming from the core-site.xml, not from the job.xml. And the MR AM is started in a separate VM, thus cannot set it from the testcase bootstrap of the minicluster.  Got it. Makes sense. Maybe add a comment in the test to that effect?  The latest patch looks good to me. +1 pending Jenkins.", "adding comment following comment to the testcase regarding #4 above:   {code}      //we do this trick because the MR AppMaster is started in another VM and     //the HttpServer configuration is not loaded from the job.xml but from the     //site.xml files in the classpath {code}", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12540104/HADOOP-8581.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy:                    org.apache.hadoop.hdfs.TestDatanodeBlockScanner                   org.apache.hadoop.mapreduce.lib.input.TestCombineFileInputFormat      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1277//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1277//console  This message is automatically generated.", "test failures seem unrelated.", "Committed to trunk. Looking into branch-2 as it seems a JIRA touching HttpServer didn't make it there yet and the merge does not cleanly apply.", "Committed to branch-2 (did typo in JIRA commit messages both for trunk and branch-2, used HADOOP-8681 instead HADOOP-8581, missing git amends :) )", "Integrated in Hadoop-Mapreduce-trunk-Commit #2597 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2597/])     HADOOP-8581 Amendment to CHANGES.txt setting right JIRA number, add support for HTTPS to the web UIs. (tucu) (Revision 1372644)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372644 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Common-trunk-Commit #2575 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2575/])     HADOOP-8581 Amendment to CHANGES.txt setting right JIRA number, add support for HTTPS to the web UIs. (tucu) (Revision 1372644)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372644 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk-Commit #2640 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2640/])     HADOOP-8581 Amendment to CHANGES.txt setting right JIRA number, add support for HTTPS to the web UIs. (tucu) (Revision 1372644)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372644 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1135 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1135/])     HADOOP-8581 Amendment to CHANGES.txt setting right JIRA number, add support for HTTPS to the web UIs. (tucu) (Revision 1372644)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372644 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1167 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1167/])     HADOOP-8581 Amendment to CHANGES.txt setting right JIRA number, add support for HTTPS to the web UIs. (tucu) (Revision 1372644)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1372644 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "[~tucu00], addition of this change has made the previously existed solution unworkable.  In the past we had http and https port separately configurable. With this you are using the same port for http or https. This causes multiple ways to configure the https functionality. See the comment - https://issues.apache.org/jira/browse/HDFS-5271?focusedCommentId=13780581&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13780581  There are also YARN jiras associate with this.  Here is my proposal: - Retain the older http and https configuration - If a setup only wants to support https, http can redirect to https port  This needs to happen quickly. If I do not hear back, I will revert this change and add the older config support with redirect back.", "BTW this should have been marked incompatible, because if you set this configuration as true, the older configurations become unnecessary.", "[~sureshms], I'll look into this today, thx.", "[~tucu00], thanks for the quick response. We have some bandwidth to work on this. Lets decide on the approach and others can pitch in with help.", "We started building on this in YARN, but we can change. We need to decide on few things.  Today, there is  - a common hadoop.ssl.enabled flag as added in the patch  - a hdfs specific dfs.https.enabled flag.  - We haven't added a new config in YARN assuming that we can just use the common flag.  We could deprecate the dfs flag in favor of the common flag, but that's a decision to be made.  Irrespective of the above decision, I think that if a https specific port is configured, then NN/DN should set the same setting for HttpServer. Today, what is happening is that NN/DN explicitly call addSslListener() and start https on user configured port. HttpServer doesn't know this, depends on hadoop.ssl.enabled flag and starts https on the regular http port also.  One more choice to make is what to do with regular http if user configures a https port. I think it makes sense to redirect traffic from http to https so that the user clearly knows he is talking https AND on a different port.", "The other related problem is the use of HttpConfig APIs. It is causing unbearable pain :) Ideally, HttpServer should take in a config key that directs it whether ssl is enabled or not.", "I had a quick discussion with [~jingzhao], [~sanjay.radia], [~wheat9] and [~vinodkv]. Here is the proposal:  h3. Use cases # Admin must be able to configure both http and https port per project # Admin must be able to enable the following access: #* Only allow access to http and not https #* Only allow access to https and not http #* Allow access to both https and http #* Allow access to only https with http redirecting to https. enable http and https and must be able redirect http to https to enforce all the access is only granted over https per project. This is important for backward compatibility.   h3. Solution # Every project must have separate configuration for http and https for all the daemons that have http server support. # Every project must have default port numbers for http and https. This should be used when configuration does not specify http/https port numbers. # Every project must support a configuration for <project>.http.policy. Value 0 means support only http. Value 1 means support only https. Value 2 means support both http and https. Value 3 means support both http and https, and http redirects to https. If not specified, this value is defaulted to 0. # hadoop.ssl.enable property will be removed. The reasons for this are: #* Currently uses http port for https #* This configuration is not backward compatible and is in conflict with the existing configuration by adding multiple ways to do the same thing. #* Per project control to enforce policy is required instead of one global flag. #* We want to support both http and https. With redirect from http to https options, migration to the new setting does not require the applications to change the URL they are currently using.  h3. Backward compatibility analysis for hdfs # HDFS already supports http and https ports for namenode and datanode. TBD add config names. # HDFS uses dfs.https.enable to enable https listener. For backward compatibility, this maps to the proposed configuration as follows: #* If dfs.https.enable == false, then the dfs.https.policy will be set to 0. #* If dfs.https.enable == true, then the dfs.https.policy will be set to 1.  h3. Backward compatibility for users using hadoop.ssl.enable=true * Note that this cannot be support in a backward compatible manner because currently this flag causes incorrect behavior where http port is used for https.  h3. Changes # Remove hadoop.ssl.enable # Per project make the proposed solution changes # Change httpserver not to read configuration, instead use the arguments passed to it. The applications using http server determine how to read it's own configuration and start http server appropriately.  I propose removing hadoop.ssl.enable flag in 2.2. Rest of the changes can be done in backward compatible way and can come in 2.3. Thoughts?", "Apologies for the delay getting back on this. Overall approach seems reasonable, a few things though:  *allow access to both HTTPS and HTTP*  Serving the same content over HTTP and HTTPS seems unnecessary. And if set by mistake, could give the false sense of security to someone that intended setting https only.  If we are talking about serving webpages over HTTP and webhdfs/fsimage over HTTPS then it makes sense.   But this means we'll have to explicitly configure each servlet to be served over the correct transport only (HTTP or HTTPS). And give how servlets are added to HttpServer today this will be a careful task to ensure nothing ends up wrongfully served on both transport endpoints.  *redirecting from http to https*  while  browsers do this automatically, if i recall correctly Java does not follow redirections from HTTP to HTTPS. This may be an issue for fsimage and webhdfs.  * <project>.http.policy  Sounds good, but I would rather user http or https as value than numbers  Also, we'll have to refactor HttpServer to take as parameter the <service> prefix (I would use service rather than project)  If we remove it from 2.2, what that exactly means? what functionality we lose?", "BTW, the problem reported in HDFS-5271, what is the exact concern? that there are 2 HTTPS endpoints? or is there anything broken?", "OK, reading again things, HDFS-5271 is a documentation issue now.   What else is a problem that would justify a revert?   [~vinodkv], What problem are you facing with the HttpConfig class?", "bq. OK, reading again things, HDFS-5271 is a documentation issue now. I fail to understand. With hadoop.ssl.enabled 50070 which is configured as http port becomes https port. How is this just a documentation issue?  bq. Serving the same content over HTTP and HTTPS seems unnecessary. A lot of standard services support secure and non-secure ports. A deployment might choose to support both http and https. Depending on what an application is accessing the service for, an app can choose to use secure or insecure. The proposed solution gives flexibility of support both http and https and in cases where an admin wants to allow only https access, that is also possible.  bq. while browsers do this automatically, if i recall correctly Java does not follow redirections from HTTP to HTTPS. This may be an issue for fsimage and webhdfs. There are other tools that can handle the redirection. This gives an opportunity for such tools to continue to work even when server changes from http to https, without requiring need for URL change.  bq. Sounds good, but I would rather user http or https as value than numbers We did discuss using names instead of number. I prefer number instead of long strings that describe http, http and https, http redirect to https etc. But if others feel strings are better, I am okay.  bq. If we remove it from 2.2, what that exactly means? what functionality we lose? Most of the functionality added by this change already existed for hdfs. Only thing we lose is enforcing https only access.  bq. What else is a problem that would justify a revert? I thought I covered it earlier. Here is again for convenience: {quote} hadoop.ssl.enable property will be removed. The reasons for this are: - Currently uses http port for https - This configuration is not backward compatible and is in conflict with the existing configuration by adding multiple ways to do the same thing. - Per project control to enforce policy is required instead of one global flag. - We want to support both http and https. With redirect from http to https options, migration to the new setting does not require the applications to change the URL they are currently using. {quote}", "Suresh,  Happy to jump on the phone do chat about this. following some answers.  > A lot of standard services support secure and non-secure ports. A deployment might choose to support both http and https. Depending on what an application is accessing the service for, an app can choose to use secure or insecure.   Serving the same content over unsecure/secure endpoints without any control does not make sense and I would say we should prevent it because it would give users a false sense of security.  > Currently uses http port for https  I don't see this a reason for removing. This is exactly what the hadoop.ssl.enable property aims to do, to make sure all HTTP traffic goes over SSL (HTTPS).  > This configuration is not backward compatible and is in conflict with the existing configuration by adding multiple ways to do the same thing.  Unless I'm missing something, previously you could use SSL only for httpfs, that was the reason of the secure port.  Also, you can still set dfs.https.enable without setting hadoop.ssl enable. This is the old behavior. How is this backwards incompatible.   > Per project control to enforce policy is required instead of one global flag.  This is great improvement, however I don't see reverting this JIRA as a requirement for this.  > We want to support both http and https. With redirect from http to https options, migration to the new setting does not require the applications to change the URL they are currently using.  This is an improvement too and it can be done without reverting this JIRA.", "bq. Happy to jump on the phone do chat about this. following some answers. [~tucu00], this is important to be resolved quickly. I have few patches already in progress. Lets have a phone call; I will get in touch with you over email.", "I spoke to [~tucu00]. Here is the summary (tucu correct me if I missed any thing):  Decisions: - Add support SSL per project instead of a single global configuration - Add support HTTP_ONLY, HTTPS_ONLY, HTTP_AND_HTTPS Policies  Proposed changes: # YARN and Job history server will have <project>.http.policy. This can be set to HTTP_ONLY or HTTPS_ONLY. # HDFS will have hdfs.http.policy. This can be set to HTTP_ONLY, HTTPS_ONLY, and HTTP_AND_HTTPS # hadoop.ssl.enable will be deprecated. # When new configuration options are used, the old configurations are ignored with a warning.  Migration paths: # HDFS: for installations using hadoop.https.enable=true, the configuration will be mapped to hdfs.http.policy=HTTP_AND_HTTPS. # Installations using hadoop.ssl.enabled=true: this will map to across the project to the policy HTTPS_ONLY. One incompatibility for such installations is, configured https port will be used for namenode and datanode, instead of configured http port where currently https is started.  Future: - Add support for all the policies for YARN, Job history server, AM, and HDFS.", "bq. YARN and Job history server will have <project>.http.policy. This can be set to HTTP_ONLY or HTTPS_ONLY. yes.. yarn behavior is this way so it should be ok. HTTP_AND_HTTPS is not currently supported in yarn.  AM should never have HTTPS because today in RM we server AM's web content via proxy server and proxy server can never trust AM's certificate even if it is issued.", "[~sureshms], thanks for summarizing, LTGM. One thing you forgot to mention is the use of default ports, that diff ones will be used if HTTP or HTTPS, which after checking a couple of things I'm OK with it. Thanks again.  [~ojoshi], AMs should be able to work with secure web UIs, this is a current limitation. For example, if Yarn issues an SSL certificate for the AM on the fly (using a local certificate authority), then seeds it in the distributedcache for the AM, then Yarn can trust that certificate and the proxy server would decrypt and re-encrypt using a fix certificate which is used for the user facing endpoint.", "I suppose we've already gone past the 'why revert' reasoning.  +1 for the proposal, it simplifies things a great deal.  [~sureshms], may be add the proposal to the JIRA description?  bq. Vinod Kumar Vavilapalli, What problem are you facing with the HttpConfig class? HttpConfig depending on statics is a real pain as we saw in the previous patches in YARN.  bq. Omkar Vinit Joshi, AMs should be able to work with secure web UIs, this is a current limitation. For example, if Yarn issues an SSL certificate for the AM on the fly (using a local certificate authority), then seeds it in the distributedcache for the AM, then Yarn can trust that certificate and the proxy server would decrypt and re-encrypt using a fix certificate which is used for the user facing endpoint.  It could work, but will have to think more. IAC please file a ticket under YARN-1280 for tracking."], "tasks": {"summarization": "add support for HTTPS to the web UIs - HDFS/MR web UIs don't work over HTTPS, there are places where 'http://' is hardcoded....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8581 about?", "answer": "add support for HTTPS to the web UIs"}}}
{"issue_id": "HADOOP-8580", "project": "HADOOP", "title": "ant compile-native fails with automake version 1.11.3", "status": "Closed", "priority": "Major", "reporter": "Eugene Joseph Koontz", "assignee": null, "created": "2012-07-09T21:25:30.000+0000", "updated": "2013-05-15T05:16:00.000+0000", "labels": [], "description": "The following:  {code} ant -d -v -DskipTests -Dcompile.native=true clean compile-native {code}  works with GNU automake version 1.11.1, but fails with automake version 1.11.3.   Relevant lines of failure seem to be these:  {code} [exec] make[1]: Leaving directory `/tmp/hadoop-common/build/native/Linux-amd64-64'      [exec] Current OS is Linux      [exec] Executing 'sh' with arguments:      [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/libtool'      [exec] '--mode=install'      [exec] 'cp'      [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/libhadoop.la'      [exec] '/tmp/hadoop-common/build/native/Linux-amd64-64/lib'      [exec]       [exec] The ' characters around the executable and arguments are      [exec] not part of the command.      [exec] /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: 3212: /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: install_prog+=cp: not found      [exec] /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: 3232: /tmp/hadoop-common/build/native/Linux-amd64-64/libtool: files+= /tmp/hadoop-common/build/native/Linux-amd64-64/libhadoop.la: not found      [exec] libtool: install: you must specify an install program      [exec] libtool: install: Try `libtool --help --mode=install' for more information.   [antcall] Exiting /tmp/hadoop-common/build.xml.  BUILD FAILED {code}   Created transcript showing entire output of the above ant command for both automake 1.11.1 (successful) versus 1.11.3 (failed) here:  https://gist.github.com/3078988", "comments": ["Actually there are many other configure tools involved besides automake. From my /usr/local/ here are a set of working versions:  https://gist.github.com/3079069  and this is a corresponding set of failing versions from my /usr/:  https://gist.github.com/3079113", "libtool is supposed to only work with 'bash' as its header defines: {code} #! /bin/bash {code}  In Hadoop build.xml, we use 'sh' to execute 'libtool'. On many systems, 'sh' is linked to 'bash'. On Ubuntu, 'sh' is linked to 'dash'. Using 'dash' to run 'libtool' results this build error. The patch modifies build.xml to involve 'bash' directly.", "libtool already references bash, so it can be used as exec, and no need to worry about the shell.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535942/HADOOP-8580-branch-1.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1235//console  This message is automatically generated.", "slightly different patch: exec libtool directly.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12538528/hadoop-8580-branch-1.gs.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1236//console  This message is automatically generated.", "Gera, I think this is a 1.0 problem. \"Submit patch\" is only used for trunk.", "Chuan, I would like to clarify whether you request a patch for the trunk.", "No. I just want to point out this problem does not affect trunk because trunk has moved to use Maven. \"Submit patch\" triggered Hadoop QA to apply the patch to trunk, and hence all the -1's.", "+1 (non-committer) for the second patch.  Applying this patch allowed me to build native on Ubuntu 12.04. The workaround I had been using was to symlink /bin/sh to /bin/bash.", "+1 as non-committer as well.", "patch looks good, verified on ubuntu and centos. +1", "I committed the patch to branch-1.  Thank you Gera for the patch.", "Closed upon release of Hadoop 1.2.0."], "tasks": {"summarization": "ant compile-native fails with automake version 1.11.3 - The following:  {code} ant -d -v -DskipTests -Dcompile.native=true clean compile-native {code}  work...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8580 about?", "answer": "ant compile-native fails with automake version 1.11.3"}}}
{"issue_id": "HADOOP-8579", "project": "HADOOP", "title": "Websites for HDFS and MapReduce both send users to video training resource which is non-public", "status": "Resolved", "priority": "Minor", "reporter": "David L. Willson", "assignee": null, "created": "2012-07-09T21:25:06.000+0000", "updated": "2016-10-26T07:17:27.000+0000", "labels": [], "description": "Main pages for HDFS and MapReduce send new user to unavailable training resource.  These two pages:  http://hadoop.apache.org/mapreduce/ http://hadoop.apache.org/hdfs/  Link to this page:  http://vimeo.com/3584536  That page is not public, and not shared to all registered Vimeo users, and I see nothing indicating how to ask for access to the resource.  Please make the vids public, or remove the link of disappointment.", "comments": ["This came up once before too: http://search-hadoop.com/m/gxF7x1OtGET1/http%253A%252F%252Fvimeo.com%252F3584536&subj=Permission+to+view+the+Video+http+vimeo+com+3584536+  I have little clue to whats happened with the video, but there's other material available if you google for simple terms such as 'hadoop video' on google or on other search engines.", "I appreciate the tremendous amount of other training resources available, including lots of good material from Cloudera. However, that doesn't fix the problem that these two broken links on prominent pages are frustrating new users.", "We'll get it fixed soon, sorry for the inconvenience. I just wanted to point you (and others who may land here) to alternative resources meanwhile :)", "Harsh, please ensure the link (if you are retaining it) follows the directions given in HADOOP-5754.", "BTW my vote is to remove that link altogether, since it is hard to make sure that the video adheres to the guidelines from HADOOP-5754.", "Ok I did some asking and turns out that video was originally hosted by Cloudera itself. They updated the video with Linden on it so the URL's changed, and the new version of the same is viewable at http://www.cloudera.com/resource/introduction-to-apache-mapreduce-and-hdfs/ (Seems to require registering, hrm).  I think we can rather replace the direct video link to an Apache Hadoop Wiki page that compiles a list of resources folks can go to, rather than a singular one. How does that sound Suresh?", "Sounds good. Do we have such a page?", "Not yet, creating one. Will update it here and once it looks good, we can make the site changes.", "Suresh - I started something off on http://wiki.apache.org/hadoop/HadoopTutorialVideos with what free resources of video I could find. Feel free to edit/expand.  If this is good, lets swap the link/text in the site to direct to this page.", "Suresh - I didn't hear back from you. Are you good with the above link? Can we make the swap?", "This does not appear to be a problem after the project re-merge."], "tasks": {"summarization": "Websites for HDFS and MapReduce both send users to video training resource which is non-public - Main pages for HDFS and MapReduce send new user to unavailable training resource.  These two pages: ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8579 about?", "answer": "Websites for HDFS and MapReduce both send users to video training resource which is non-public"}}}
{"issue_id": "HADOOP-8578", "project": "HADOOP", "title": "Provide a mechanism for cleaning config items from LocalDirAllocator which will not be used anymore", "status": "Open", "priority": "Major", "reporter": "Devaraj Kavali", "assignee": null, "created": "2012-07-09T05:13:02.000+0000", "updated": "2016-05-12T18:22:45.000+0000", "labels": [], "description": "If we use DefaultContainerExecutor, for every application one config item is getting added into the LocalDirAllocator.contexts and is not deleting forever and due to this nm throws oom error after some time. This has been fixed with MAPREDUCE-4379 by adding removeContext() api in LocalDirAllocator and explictly deleting after the application completion.   It would be good if we can clean the cache of config items from LocalDirAllocator when there is no use with that furthermore. https://issues.apache.org/jira/browse/MAPREDUCE-4379?focusedCommentId=13407237&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13407237", "comments": [], "tasks": {"summarization": "Provide a mechanism for cleaning config items from LocalDirAllocator which will not be used anymore - If we use DefaultContainerExecutor, for every application one config item is getting added into the ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8578 about?", "answer": "Provide a mechanism for cleaning config items from LocalDirAllocator which will not be used anymore"}}}
{"issue_id": "HADOOP-8577", "project": "HADOOP", "title": "The RPC must have failed proxyUser (auth:SIMPLE) via realUser1@HADOOP.APACHE.ORG (auth:SIMPLE)", "status": "Resolved", "priority": "Minor", "reporter": "chandrashekhar Kotekar", "assignee": null, "created": "2012-07-08T16:04:45.000+0000", "updated": "2012-07-19T14:51:06.000+0000", "labels": [], "description": "Hi,  I have downloaded maven source code today itself and tried test it. I did following steps : 1) mvn clean 2) mvn compile 3) mvn test  After 3rd step one step failed. Stack trace of failed test is as follows :  Failed tests:   testRealUserIPNotSpecified(org.apache.hadoop.security.TestDoAsEffectiveUser): The RPC must have failed proxyUser (auth:SIMPLE) via realUser1@HADOOP.APACHE.ORG (auth:SIMPLE)   testWithDirStringAndConf(org.apache.hadoop.fs.shell.TestPathData): checking exist   testPartialAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:123> but was:<myfs://host.a:123>   testFullAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<null> but was:<java.lang.IllegalArgumentException: Wrong FS: myfs://host/file, expected: myfs://host.a.b>   testShortAuthorityWithDefaultPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:123> but was:<myfs://host:123>   testPartialAuthorityWithDefaultPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:123> but was:<myfs://host.a:123>   testShortAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:123> but was:<myfs://host:123>   testIpAuthorityWithOtherPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://127.0.0.1:456> but was:<myfs://localhost:456>   testAuthorityFromDefaultFS(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:123> but was:<myfs://host:123>   testFullAuthorityWithDefaultPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<null> but was:<java.lang.IllegalArgumentException: Wrong FS: myfs://host/file, expected: myfs://host.a.b:123>   testShortAuthorityWithOtherPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:456> but was:<myfs://host:456>   testPartialAuthorityWithOtherPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://host.a.b:456> but was:<myfs://host.a:456>   testFullAuthorityWithOtherPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<null> but was:<java.lang.IllegalArgumentException: Wrong FS: myfs://host:456/file, expected: myfs://host.a.b:456>   testIpAuthority(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://127.0.0.1:123> but was:<myfs://localhost:123>   testIpAuthorityWithDefaultPort(org.apache.hadoop.fs.TestFileSystemCanonicalization): expected:<myfs://127.0.0.1:123> but was:<myfs://localhost:123>  Tests in error:    testUnqualifiedUriContents(org.apache.hadoop.fs.shell.TestPathData): `d1': No such file or directory  I am newbie in Hadoop source code world. Please help me in building hadoop source code.", "comments": ["The JIRA is to track issues with the project, not for user/dev-help. Please ask your question on common-dev[at]hadoop.apache.org mailing lists instead, and refrain from posting general questions on the JIRA. Thanks! :)  P.s. The issue is your OS. Fix your /etc/hosts to use the right format of \"IP FQDN ALIAS\", instead of \"IP ALIAS FQDN\". In any case, please mail the right user/dev group. See http://hadoop.apache.org/mailing_lists.html", "It's not his host config because this test is not using the system resolver.  The resolver for host-based tokens is overriding the call to the system resolver with one that simply returns lookups from a pre-defined map.  These failures should not be possible.  The only thing I can think of is that SecurityUtils is somehow broken and not using the resolver it's told to use.  I have no idea why this would be unique to JDK7..."], "tasks": {"summarization": "The RPC must have failed proxyUser (auth:SIMPLE) via realUser1@HADOOP.APACHE.ORG (auth:SIMPLE) - Hi,  I have downloaded maven source code today itself and tried test it. I did following steps : 1) ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8577 about?", "answer": "The RPC must have failed proxyUser (auth:SIMPLE) via realUser1@HADOOP.APACHE.ORG (auth:SIMPLE)"}}}
{"issue_id": "HADOOP-8576", "project": "HADOOP", "title": "Class util.KerberosName shouldn't print unnecessary warns when kerberos has not been configured/asked for", "status": "Resolved", "priority": "Trivial", "reporter": "Harsh J", "assignee": null, "created": "2012-07-08T14:43:35.000+0000", "updated": "2012-07-09T01:13:35.000+0000", "labels": ["newbie"], "description": "Currently, starting up a daemon prints this in the log even when no security has been configured:  {code} 12/07/08 20:00:22 WARN util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty {code}  I think this should be a DEBUG log, or an INFO log at best. I do not see why printing it as a warning is useful, for it is common to not have Kerberos configs.", "comments": ["Sorry for the noise. HADOOP-8400 already fixed this, but it didn't appear on my search, must've been cause of weak terms, my bad. Resolving as dupe."], "tasks": {"summarization": "Class util.KerberosName shouldn't print unnecessary warns when kerberos has not been configured/asked for - Currently, starting up a daemon prints this in the log even when no security has been configured:  {...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8576 about?", "answer": "Class util.KerberosName shouldn't print unnecessary warns when kerberos has not been configured/asked for"}}}
{"issue_id": "HADOOP-8574", "project": "HADOOP", "title": "Enable starting hadoop services from inside OSGi", "status": "Resolved", "priority": "Major", "reporter": "Guillaume Nodet", "assignee": null, "created": "2012-07-06T15:55:19.000+0000", "updated": "2021-01-21T13:33:52.000+0000", "labels": [], "description": "This JIRA captures the needed things in order to start hadoop services in OSGi.  The main idea I used so far consists in:   * using the OSGi ConfigAdmin to store the hadoop configuration   * in that configuration, use a few boolean properties to determine which services should be started (nameNode, dataNode ...)   * expose a configured url handler so that the whole OSGi runtime can use urls in hdfs:/xxx   * the use of an OSGi ManagedService means that when the configuration changes, the services will be stopped and restarted with the new configuration", "comments": ["There is already some Hadoop Jiras related to that (I will link to this one just for tracking).", "Possible patch to kick the discussion: https://github.com/gnodet/hadoop-common/commit/742ab08aa068424fc2292cf1cd2d64a345053173 Though the OSGi metadata are not yet there, so this is not really testable yet (will upload a patch for that soon or JB).  There is one possibly controversial change which is the one in the Configuration (see https://github.com/gnodet/hadoop-common/commit/742ab08aa068424fc2292cf1cd2d64a345053173#L3R207). The idea is that in OSGi, the whole configuration is controlled (at least the default) by ConfigAdmin.  The benefit is that clients don't really have to deal with configuration.   One thing I haven't really understood is why the configuration isn't a global singleton (at least the defaults), as the configuration files are being read multiple times (each time a new configuration is created).", "Yep, I take the OSGi metadata and merge back the Karaf feature descriptor. It will be done over the week end."], "tasks": {"summarization": "Enable starting hadoop services from inside OSGi - This JIRA captures the needed things in order to start hadoop services in OSGi.  The main idea I use...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8574 about?", "answer": "Enable starting hadoop services from inside OSGi"}}}
{"issue_id": "HADOOP-8573", "project": "HADOOP", "title": "Configuration tries to read from an inputstream resource multiple times.", "status": "Closed", "priority": "Major", "reporter": "Robert Joseph Evans", "assignee": "Robert Joseph Evans", "created": "2012-07-06T15:05:20.000+0000", "updated": "2016-05-12T18:23:24.000+0000", "labels": [], "description": "If someone calls Configuration.addResource(InputStream) and then reloadConfiguration is called for any reason, Configruation will try to reread the contents of the InputStream, after it has already closed it.  This never showed up in 1.0 because the framework itself does not call addResource with an InputStream, and typically by the time user code starts running that might call this, all of the default and site resources have already been loaded.  In 0.23 mapreduce is now a client library, and mapred-site.xml and mapred-default.xml are loaded much later in the process.", "comments": ["The only real way to fix this is to cache the contents of the InputStream, or a parsed version of it.  The question is how do we reduce the potential memory impact of this?  We could change configuration so that it is a list of properties instead of a single combined properties object, but this is a very large change to fix this issue. Because of this I am inclined to not really worry about the memory impact right now.  This feature is not that commonly used, and most configuration objects tend to be shared a lot so I don't see the impact being that large.", "This patch just does basic caching on the parsed properties for an InputStream.  It replaces the InputStream in the resources array with the properties themselves.", "I forgot to add in that this patch feels like a hack to me, with loadResource returning the properties to be added in, but I could not think of a cleaner way to do it without completely refactoring a lot of that code.  If someone else has a better way to do this I would be very happy to switch.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535381/HADOOP-8573.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1178//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1178//console  This message is automatically generated.", "Mostly looks good, a couple minor things.  nits: - add a space in Configuration.java line 1772 and 1905 after if before (  and 1926 between for and (  suggestions: - could add a comment describing new behavior when using InputStreams - perhaps a warning about memory usage - we might change org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.loadConfFile to use addResource(Path file) instead of the inputStream. Only used in one place right now but might be better to change it in case it gets used more.", "I have addressed your comments with this latest patch.  I did not change the behavior of reading in the configuration form HDFS, because Configuration.addResource(Path) only works for the local file system file://.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535868/HADOOP-8573.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1181//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1181//console  This message is automatically generated.", "Upmerged the changes to deal with another patch that was just merged in. Oh and the test failures are unrelated.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535885/HADOOP-8573.txt   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1182//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1182//console  This message is automatically generated.", "+1. Thanks Bobby!", "Integrated in Hadoop-Hdfs-trunk #1100 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1100/])     HADOOP-8573. Configuration tries to read from an inputstream resource multiple times (Robert Evans via tgraves) (Revision 1359891)       Result = FAILURE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359891 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Hdfs-0.23-Build #310 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/310/])     merge -r 1359891:1359892 from branch-2. FIXES: HADOOP-8573 (Revision 1359896)       Result = UNSTABLE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359896 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java", "Integrated in Hadoop-Mapreduce-trunk #1133 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1133/])     HADOOP-8573. Configuration tries to read from an inputstream resource multiple times (Robert Evans via tgraves) (Revision 1359891)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1359891 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java"], "tasks": {"summarization": "Configuration tries to read from an inputstream resource multiple times.  - If someone calls Configuration.addResource(InputStream) and then reloadConfiguration is called for a...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8573 about?", "answer": "Configuration tries to read from an inputstream resource multiple times."}}}
{"issue_id": "HADOOP-8572", "project": "HADOOP", "title": "Have the ability to force the use of the login user", "status": "Open", "priority": "Major", "reporter": "Guillaume Nodet", "assignee": null, "created": "2012-07-06T14:04:27.000+0000", "updated": "2014-05-22T08:32:32.000+0000", "labels": [], "description": "In Karaf, most of the code is run under the \"karaf\" user. When a user ssh into Karaf, commands will be executed under that user. Deploying hadoop inside Karaf requires that the authenticated Subject has the required hadoop principals set, which forces the reconfiguration of the whole security layer, even at dev time.  My patch proposes the introduction of a new configuration property {{hadoop.security.force.login.user}} which if set to true (it would default to false to keep the current behavior), would force the use of the login user instead of using the authenticated subject (which is what happen when there's no authenticated subject at all).  This greatly simplifies the use of hadoop in such environments where security isn't really needed (at dev time).", "comments": ["Patch available for review at https://github.com/gnodet/hadoop-common/commit/02af662eecd79aa4fd09afd47249e6d025de985e", "You need to also upload the patch to Apache's jira.  Can you explain more of the context? If Karaf invoking Hadoop via the command line or in the same jvm?", "I'm working on deploying Hadoop in OSGi (and Karaf in particular).  Karaf has a console which is similar has a unix shell where you can run commands (deploying new osgi bundles, starting, stopping bundles and much more).  This console is also available remotely using an ssh client (Karaf embed a java sshd server). In Karaf, we don't use any security manager, but we still have a user authenticated and associated to the thread.  The user is the one logged into the console.   When you install the hadoop osgi bundle (which I'm working on), the karaf security layer and the hadoop security mechanism do not work well together.  The patch allows hadoop to just ignore the currently associated Subject and always use the same mechanism as if no Subject was associated to the thread (i.e. defaulting to use the OS login user).  I'll attach a patch asap.", "Well, looking at trunk, it seems this patch isn't really needed as the getCurrentUser() looks like: {code}   public synchronized   static UserGroupInformation getCurrentUser() throws IOException {     AccessControlContext context = AccessController.getContext();     Subject subject = Subject.getSubject(context);     if (subject == null || subject.getPrincipals(User.class).isEmpty()) {       return getLoginUser();     } else {       return new UserGroupInformation(subject);     }   } {code} which means the current Subject is ignored if it does not contain the User principal. I guess this also solves my problem though I'm slightly more worried about the current trunk, as if people want to integrate with the JAAS layer and deny access to hadooop, I'm not sure how that can be done anymore, given hadoop will always fallback to the login user.", "Could this issue be addressed by {{UserGroupInformation.getLoginUser().doAs(...)}}?", "Definitely.  The question comes down to: is that something you want to force the user into ? I usually try to make things as easy to use as possible when possible, especially when you can change the config to suit more complex needs (for security or any other matter).", "I'd rather the complexity in the code than having another config option to turn it off, as that leads to another config option to play with when trying to get security to work. Once you start trying to talk to secure clusters or just run code in YARN app masters (as user \"yarn\") while impersonating the user submitting the job, you'll discover there's already enough to worry about. Getting the developers to care about this sooner rather than later is, while painful, the best way to make sure things run in production"], "tasks": {"summarization": "Have the ability to force the use of the login user  - In Karaf, most of the code is run under the \"karaf\" user. When a user ssh into Karaf, commands will ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8572 about?", "answer": "Have the ability to force the use of the login user"}}}
{"issue_id": "HADOOP-8571", "project": "HADOOP", "title": "Improve resource cleaning when shutting down", "status": "Open", "priority": "Major", "reporter": "Guillaume Nodet", "assignee": null, "created": "2012-07-06T10:03:24.000+0000", "updated": "2016-05-12T18:21:59.000+0000", "labels": [], "description": "", "comments": ["Clean shutdown is required because of the dynamism of OSGi, where thread leakage would be a problem.", "I've committed a patch in a github fork at    https://github.com/gnodet/hadoop-common/commit/d7a6738429716000376df344ae68ee1a1a630223", "Your patch seems to be for just the branch-1 code. Any chance we can get a trunk patch as well, with some more detail on what you've changed and why? (Some changes are obvious, but others can do with some more info, would help acceptability).  Thanks!  P.s. Also upload your patch here and grant ASF permissions to use it, otherwise reviewers may be hesitant in reviewing cause we can't integrate it unless you manually give ASF permission to do so.", "Yes, I want to first fix the 1.0 branch to be ready for OSGi and then backport everything to trunk. I'll upload real patches and attach them the usual way (btw, I'm an ASF committer already, so I don't think such a grant is really needed in that case anyway).  Related to the changes, I think the most problematic problem is related to CleanupQueue which is a singleton with a thread wich is never stopped.  I think most of the other threads are controlled somehow, but I need to include the following code when stopping the OSGi bundles to correctly stop all the threads:  {code}         FileSystem.closeAll();         CleanupQueue.getInstance().stop();         DefaultMetricsSystem.INSTANCE.shutdown(); {code}", "Moved to 1.2.0 upon release of 1.1.0.", "Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.", "Guillaume, can you look at what it takes to do this for the Hadoop trunk? Any OSGI support will go in there", "I don't really have much time to work on that atm. This specific jira issue is not OSGi specific, it's just about being able to embed hadoop, which requires that the shutdown does not leak threads or other resources."], "tasks": {"summarization": "Improve resource cleaning when shutting down - ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8571 about?", "answer": "Improve resource cleaning when shutting down"}}}
{"issue_id": "HADOOP-8570", "project": "HADOOP", "title": "Bzip2Codec should accept .bz files too", "status": "Open", "priority": "Major", "reporter": "Harsh J", "assignee": null, "created": "2012-07-06T07:12:02.000+0000", "updated": "2013-07-10T06:41:44.000+0000", "labels": ["bzip", "newbie"], "description": "The default extension reported for Bzip2Codec today is \".bz2\". This causes it not to pick up .bz files as Bzip2Codec files. Although the extension is not very popular today, it is still mentioned as a valid extension in the bunzip manual and we should support it.  We should change the Bzip2Codec default extension to \"bz\", or we should add in a new extension list support to allow for better detection across various aliases.", "comments": ["Hey guys  I'd be willing to do this, which of the approaches though? I think just changing the extension to .bz wouldn't solve this bug 'cause it wouldn't take .bz2 files, which, apart from being this same bug with another extension, would be incompatible.  From what I see, the changes are contained to {{CompressionCodecFactory}} (and 2 logging statements in {{CodecPool}}). The idea then is to change the {{CompressionCodec}} interface, specifically {{getDefaultExtension()}} and the mechanism pertaining to it to return a set of extensions the codec supports. Also, looks like at the moment there are no checks for extension overlays, so if there are two codecs that report the same extension, one would overwrite another as they get loaded by the {{ServiceLoader}}. I think at the very least this calls for a loud warning in the log, especially if we add this extension aliasing thing.  Your thoughts? Should I go ahead & make a patch for this?", "Hi, I attach the patch file which added a method to define the additional extensions to CompressionCodec. This method getOtherExtensions() defines the additional extensions in comma-separated value, and defines null when there is not additional extension."], "tasks": {"summarization": "Bzip2Codec should accept .bz files too - The default extension reported for Bzip2Codec today is \".bz2\". This causes it not to pick up .bz fil...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8570 about?", "answer": "Bzip2Codec should accept .bz files too"}}}
{"issue_id": "HADOOP-8569", "project": "HADOOP", "title": "CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE", "status": "Closed", "priority": "Minor", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-06T06:28:44.000+0000", "updated": "2013-08-27T22:06:31.000+0000", "labels": [], "description": "In the native code, we should define _GNU_SOURCE and _LARGEFILE_SOURCE so that all of the functions on Linux are available.  _LARGEFILE enables fseeko and ftello; _GNU_SOURCE enables a variety of Linux-specific functions from glibc, including sync_file_range.", "comments": ["-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535323/HADOOP-8569.001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.hdfs.TestDatanodeBlockScanner                   org.apache.hadoop.hdfs.TestHDFSTrash      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1175//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1175//console  This message is automatically generated.", "How about doing this after HDFS-3537 and enabling _GNU_SOURCE for only the fuse-dfs build? I  think it's the only native src that requires _GNU_SOURCE.", "What I'm afraid of is issues where some function or feature is not detected as present because _GNU_SOURCE was not defined.  Unfortunately, when you artificially hide functions, these kind of issues are all too common, and they can prove difficult to diagnose.  There's really no disadvantage to defining _GNU_SOURCE that I'm aware of, so most projects just define it everywhere.  I think we should too.", "The disadvantage is that libhdfs currently compiles on non-gnu systems and this breaks that. What functions are we using that are currently being hidden that are only declared via _gnu_source? If the above is true we should be able to use these via including std headers.", "bq. The disadvantage is that libhdfs currently compiles on non-gnu systems and this breaks that.  Defining _GNU_SOURCE doesn't break the compile on any systems.  Nobody should be checking for this macro except on Linux.  If they are, then that's a bug on their part, which we can work around like this:  {code} IF (${CMAKE_SYSTEM_NAME} MATCHES \"Linux\")     set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -D_GNU_SOURCE\") ENDIF() {code}  I guess maybe we should do it like that.  Although it shouldn't matter.  bq. What functions are we using that are currently being hidden that are only declared via _gnu_source? If the above is true we should be able to use these via including std headers.  man sync_file_range {code} NAME        sync_file_range - sync a file segment with disk  SYNOPSIS        #define _GNU_SOURCE         /* See feature_test_macros(7) */        #include <fcntl.h>         int sync_file_range(int fd, off64_t offset, off64_t nbytes,                            unsigned int flags); {code}  As you can see, the man page tells you to define _GNU_SOURCE in order to make this function visible.", "The downside to defining {{_GNU_SOURCE}} everywhere -- in both code that is known to need it, and in code that is intended to be portable -- is that it makes it very easy to accidentally break compilation on non-GNU platforms by unintentionally changing code from \"portable\" to \"gnu-specific\".  Suppose we have a project with {{foo-linux.c}} containing nonportable code, and {{generic.c}} containing POSIX portable code.  If I define {{_GNU_SOURCE}} in {{CFLAGS}} then unintentionally adding a call to {{sync_file_range}} to {{generic.c}} will silently work on Linux, and won't break the build until you try building on Darwin or Solaris or whatever.  If instead the project puts {{#define _GNU_SOURCE}} at the top of files intended to be platform-specific, then such portability breakage will be noticed immediately.  The argument doesn't extend to LFS support -- it's entirely reasonable to use 64-bit-{{off_t}} everywhere including Linux-32.  {code} +# note: can't enable -D_FILE_OFFSET_BITS=64: see MAPREDUCE-4258 +set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -D_REENTRANT -D_GNU_SOURCE\") +set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -D_LARGEFILE_SOURCE\") {code} If we don't have FILE_OFFSET_BITS=64, then shouldn't we also leave out LARGEFILE_SOURCE?  (This is a serious question, I don't know how those two defines interact, I just vaguely remember that there's a complicated rule about how they should be used.)  In summary -- I'd slightly prefer to limit the GNU_SOURCE to a {{#define}} in the files that we intend to be Linux-specific.  The rest of this patch is good though, cleaning up the LFS defines and adding that comment about MAPREDUCE-4258.", "_GNU_SOURCE was defined previously in most (if not all) of our native projects.  After I did the CMake conversion, the fact that it wasn't defined in the CMakeLists.txt was a bug, not a feature.  That's what I'm trying to fix here.  I realize that it's tempting to assume that code that you write without _GNU_SOURCE defined will automatically be portable.  However, this is *NOT TRUE*.  For example, even without _GNU_SOURCE defined, you still get the non-POSIX definition of strerror_r out of glibc.  The only valid way to make sure your code is portable is to build and test it on multiple platforms.  Any other strategy is just a waste of time.  Defining GNU_SOURCE is similar to setting the correct DOCTYPE in your HTML file.  It tells the browser (or compiler in this case) to turn off \"quirks mode\" and give you the real deal.  I don't think the CheckFunctionExists stuff in the CMakeLists.txt will work consistently without _GNU_SOURCE defined.  There are better ways to improve our portability.  For example, we should probably have some OpenBSD jenkins build slaves.  But let's not waste our time messing with macros.  It really adds nothing but inconvenience.  bq. If we don't have FILE_OFFSET_BITS=64, then shouldn't we also leave out LARGEFILE_SOURCE?  _LARGEFILE_SOURCE exposes fseeko and ftello.   _FILE_OFFSET_BITS changes the default off_t type to be 64 bits.  Basically _LARGEFILE_SOURCE is something you need to define in addition to _FILE_OFFSET_BITS, but the two things do different things.", "Looks like the patch no longer applies to trunk. Colin, mind posting an updated patch?  I personally find Colin's arguments compelling - there doesn't seem to be much if any downside, and adding this really just serves to restore the previous situation before introduction of cmake. If there are no further objections, I'll commit this once Colin posts an updated patch.", "Updated version.  This version leaves out the changes to the YARN CMakeLists.txt, because {{_GNU_SOURCE}} was already added there.", "On a related note, I have a bunch of patches to get native compilation working under OpenBSD that I really should submit at some point.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12570546/HADOOP-8569.003.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2219//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2219//console  This message is automatically generated.", "+1, I'm going to commit this momentarily.", "I've just committed this to trunk and branch-2.  Thanks a lot for the contribution, Colin.", "Integrated in Hadoop-trunk-Commit #3379 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3379/])     HADOOP-8569. CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE. Contributed by Colin Patrick McCabe. (Revision 1449922)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1449922 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt", "Integrated in Hadoop-Yarn-trunk #139 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/139/])     HADOOP-8569. CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE. Contributed by Colin Patrick McCabe. (Revision 1449922)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1449922 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt", "Integrated in Hadoop-Hdfs-trunk #1328 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1328/])     HADOOP-8569. CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE. Contributed by Colin Patrick McCabe. (Revision 1449922)       Result = FAILURE atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1449922 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt", "Integrated in Hadoop-Mapreduce-trunk #1356 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1356/])     HADOOP-8569. CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE. Contributed by Colin Patrick McCabe. (Revision 1449922)       Result = SUCCESS atm : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1449922 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/CMakeLists.txt * /hadoop/common/trunk/hadoop-tools/hadoop-pipes/src/CMakeLists.txt"], "tasks": {"summarization": "CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE - In the native code, we should define _GNU_SOURCE and _LARGEFILE_SOURCE so that all of the functions ...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8569 about?", "answer": "CMakeLists.txt: define _GNU_SOURCE and _LARGEFILE_SOURCE"}}}
{"issue_id": "HADOOP-8568", "project": "HADOOP", "title": "DNS#reverseDns fails on IPv6 addresses", "status": "Resolved", "priority": "Major", "reporter": "Eli Collins", "assignee": "Tony Kew", "created": "2012-07-06T00:39:36.000+0000", "updated": "2014-07-18T17:03:27.000+0000", "labels": ["newbie"], "description": "DNS#reverseDns assumes hostIp is a v4 address (4 parts separated by dots), blows up if given a v6 address:  {noformat} Caused by: java.lang.ArrayIndexOutOfBoundsException: 3         at org.apache.hadoop.net.DNS.reverseDns(DNS.java:79)         at org.apache.hadoop.net.DNS.getHosts(DNS.java:237)         at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:340)         at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:358)         at org.apache.hadoop.net.DNS.getDefaultHost(DNS.java:337)         at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:235)         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)         at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)         at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)         at java.lang.reflect.Constructor.newInstance(Constructor.java:513)         at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:1649) {noformat}", "comments": ["Since I see hbase in the stacktrace, it's interesting that it's using an unstable interface reserved for hdfs and mapreduce.  Should {{NetUtils}} be used instead?", "Yea, it's worth getting HBase off the DNS class.", "The existing DNS.java tests fail for IPv6 addresses. This only fixes IPv6 reverse resolution. IPv6 tests still fail with IPv6 nameservers (at least partly due to problems in Sun's JNDI.)", "Thanks for the patch, Tony. At first glance, the patch looks like it should solve the problem.  I am marking the JIRA Unassigned. Please feel free to assign it to yourself.", "Thanks Tony. TestDNS#testRDNS now passes on an IPV6 enabled host with your change?  Mind filing a jira for the remaining ipv6 test failures you're seeing?", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12541638/HADOOP-8568.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      -1 findbugs.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1331//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1331//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1331//console  This message is automatically generated.", "Per findbugs let's use StringBuilder.  {noformat} SBSC\tMethod org.apache.hadoop.net.DNS.reverseDns(InetAddress, String) concatenates strings using + in a loop {noformat}", "Revised patch, uses StringBuffer to build the reverseIP String", "{code} +        int rawintaddr = rawaddr[i] < 0 ? rawaddr[i] + 256 : rawaddr[i]; {code} It's a bit simpler and faster (and has the same results) to say {code} int rawintaddr = rawaddr[i] & 0xff; {code} but I would have thought that there would be a standard idiom for treating a byte[] as a list of unsigned 8 bit values.  Maybe not?  bq. Revised patch, uses StringBuffer to build the reverseIP String  Doesn't look like an updated upload happened, the current attachment has string concatenation in a loop.", "This includes the missing StringBuffer code  & the updated byte to int conversion", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542364/HADOOP-8568.patch   against trunk revision .      -1 patch.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1356//console  This message is automatically generated.", "right patch file this time (God willing!)", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542389/HADOOP-8568.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1357//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1357//console  This message is automatically generated.", "Any objection to listing this issue as fixed?  Tony", "Tony,  no need to delete old patches when uploading a new one, it can be useful to reviewers to have old patches available to use {{interdiff}} or similar tools or simply to review advancement of a change.  I tend to use a new name (hdfs-123.patch, hdfs-123-1.patch, etc) for each upload, but that's just for my convenience since Jira keeps track of different uploads with the same name just fine.  {code} +      byte rawaddr[] = hostIp.getAddress(); ... +      String[] parts = hostaddr.split(\"\\\\.\"); +      reverseIP = parts[3] + \".\" + parts[2] + \".\" + parts[1] + \".\" +        + parts[0] + \".in-addr.arpa\"; {code} I think the {{byte[]}} version of this code, used for IPv6, is significantly superior to the regex based string version used for IPv4.  Could you rewrite the IPv4 section of the code using {{getAddress()}}?  This may also result in greater code sharing between the two branches.", "{code} +        // rawaddr bytes are of type unsigned int - this converts the given +        // byte to a (signed) int \"rawintaddr\" +        int rawintaddr = rawaddr[i] & 0xff; +        // format \"rawintaddr\" into a hex String +        String addressbyte = String.format(\"%02x\", rawintaddr); {code} This can be more simply and clearly written as {code} String addressbyte = String.format(\"%02x\", rawaddr[i] & 0xff); {code} It's actually sufficient to say {{format(\"%02x\", rawaddr[i])}} but I find that a little too magic; making the 8-bit truncation explicit seems to more clearly express the intent to me.  (The mask-free version only gives the correct two-nibble output because of the overspecified {{FormatSpecifier#print(byte, Locale)}} implementation in {{java.util.Formatter}}, and breaks if you change to a local int variable for example.)", "I'm still puzzled why hbase is using a class marked: {noformat}@InterfaceAudience.LimitedPrivate({\"HDFS\", \"MapReduce\"}) @InterfaceStability.Unstable{noformat}  Why isn't hbase using {{NetUtils}}?", "This issue is related to  https://issues.apache.org/jira/browse/HADOOP-3619  I provided a similar patch which includes also a testcase.", "Similar patches are available in both issues", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12542389/HADOOP-8568.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  There were no new javadoc warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl                   org.apache.hadoop.fs.TestSymlinkLocalFSFileContext                   org.apache.hadoop.ipc.TestIPC                   org.apache.hadoop.fs.TestSymlinkLocalFSFileSystem      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/4260//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/4260//console  This message is automatically generated.", "I'm closing this JIRA out in favor of HADOOP-3619 since it has a more modern patch associated with it."], "tasks": {"summarization": "DNS#reverseDns fails on IPv6 addresses - DNS#reverseDns assumes hostIp is a v4 address (4 parts separated by dots), blows up if given a v6 ad...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8568 about?", "answer": "DNS#reverseDns fails on IPv6 addresses"}}}
{"issue_id": "HADOOP-8567", "project": "HADOOP", "title": "Port conf servlet to dump running configuration  to branch 1.x", "status": "Closed", "priority": "Major", "reporter": "Junping Du", "assignee": "Jing Zhao", "created": "2012-07-06T00:17:37.000+0000", "updated": "2019-10-25T20:25:28.000+0000", "labels": [], "description": "HADOOP-6408 provide conf servlet that can dump running configuration which great helps admin to trouble shooting the configuration issue. However, that patch works on branch after 0.21 only and should be backport to branch 1.x.", "comments": ["+1 for backport. This will be very useful feature on stable release.", "[~junping_du], Hi Junping, if you are busy, I can work on the backport and post a patch for it. Thanks!", "My current patch for the backport.", "Sorry. I owe this patch for a long time. Thanks for delivering this patch. I will help on review it.", "I do not see these changes from HADOOP-6408. Are they not relevant? Rest of the porting looks good. {noformat} diff --git src/java/org/apache/hadoop/conf/Configuration.java src/java/org/apache/hadoop/conf/Configuration.java index e5a4523..7407d8b 100644 --- src/java/org/apache/hadoop/conf/Configuration.java +++ src/java/org/apache/hadoop/conf/Configuration.java @@ -68,6 +70,7 @@ import org.apache.hadoop.util.ReflectionUtils;  +410,8 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,       if (other.overlay!=null) {         this.overlay = (Properties)other.overlay.clone();       } + +     this.updatingResource = new HashMap<String, String>(other.updatingResource);     }           this.finalParameters = new HashSet<String>(other.finalParameters); @@ -604,6 +593,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,      if (!isDeprecated(name)) {        getOverlay().setProperty(name, value);        getProps().setProperty(name, value); +      updatingResource.put(name, UNKNOWN_RESOURCE);      }      else {        DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(name); @@ -1438,9 +1426,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,        if (finalParameters.contains(oldKey)) {          finalParameters.remove(oldKey);        } -      if (storeResource) { -        updatingResource.remove(oldKey); -      } +      updatingResource.remove(oldKey);      }    }     @@ -1464,9 +1450,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,          continue;        }        properties.setProperty(key, value); -      if (storeResource) { -        updatingResource.put(key, updatingResource.get(attr)); -      } +      updatingResource.put(key, updatingResource.get(attr));        if (finalParameter) {          finalParameters.add(key);        } {noformat}", "Thanks for the review Suresh!  {noformat} @@ -68,6 +70,7 @@ import org.apache.hadoop.util.ReflectionUtils;  +410,8 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,       if (other.overlay!=null) {         this.overlay = (Properties)other.overlay.clone();       } + +     this.updatingResource = new HashMap<String, String>(other.updatingResource);     } {noformat} This has been addressed in the 002 patch.  {noformat} @@ -604,6 +593,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,      if (!isDeprecated(name)) {        getOverlay().setProperty(name, value);        getProps().setProperty(name, value); +      updatingResource.put(name, UNKNOWN_RESOURCE);      }      else {        DeprecatedKeyInfo keyInfo = deprecatedKeyMap.get(name); {noformat} For this, I think I should add \"updatingResource.put(name, UNKNOWN_RESOURCE);\" to Configuration#set(). Will address that in the new patch.  {noformat} @@ -1438,9 +1426,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,        if (finalParameters.contains(oldKey)) {          finalParameters.remove(oldKey);        } -      if (storeResource) { -        updatingResource.remove(oldKey); -      } +      updatingResource.remove(oldKey);      }    } {noformat} For this part, I did not find similar code (should be some code corresponding to remove) in both branch-1 and current trunk.  {noformat} @@ -1464,9 +1450,7 @@ public class Configuration implements Iterable<Map.Entry<String,String>>,          continue;        }        properties.setProperty(key, value); -      if (storeResource) { -        updatingResource.put(key, updatingResource.get(attr)); -      } +      updatingResource.put(key, updatingResource.get(attr));        if (finalParameter) {          finalParameters.add(key);        } {noformat} The similar change happened in Configuration#loadResource in 002 patch.", "New patch addressing Suresh's comments.", "+1 for the latest patch. I committed it to branch-1.", "Previous patch did not add the servlet to http server. I reverted the change. The new patch looks good.  Jing can you please make sure servlet is added by manually testing it as well?", "Yes. I've manually tested the new patch and could get the configuration through the conf servlet.", "Reopening to commit the issue.", "Committed the patch to branch-1. Thank you Jing.", "Merged to branch-1.1.", "Closed upon successful release of 1.1.2."], "tasks": {"summarization": "Port conf servlet to dump running configuration  to branch 1.x - HADOOP-6408 provide conf servlet that can dump running configuration which great helps admin to trou...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8567 about?", "answer": "Port conf servlet to dump running configuration  to branch 1.x"}}}
{"issue_id": "HADOOP-8566", "project": "HADOOP", "title": "AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays)", "status": "Closed", "priority": "Major", "reporter": "Alejandro Abdelnur", "assignee": "Alejandro Abdelnur", "created": "2012-07-06T00:04:52.000+0000", "updated": "2012-10-11T17:45:10.000+0000", "labels": [], "description": "the accept() method should consider the case where the class getPackage() returns NULL.", "comments": ["+1 pending Jenkins (which I just kicked off).", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535300/HADOOP-8566.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1172//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1172//console  This message is automatically generated.", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535300/HADOOP-8566.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      -1 core tests.  The patch failed these unit tests in hadoop-common-project/hadoop-common:                    org.apache.hadoop.ha.TestZKFailoverController                   org.apache.hadoop.io.file.tfile.TestTFileByteArrays                   org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1173//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1173//console  This message is automatically generated.", "Run the failed tests locally without and with the patch and they pass. The failures seem unrelated. Reattaching patch to force a rerun.", "+1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535414/HADOOP-8566.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      +1 tests included.  The patch appears to include 1 new or modified test files.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-common-project/hadoop-common.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1176//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1176//console  This message is automatically generated.", "committed to trunk and branch-2", "Integrated in Hadoop-Mapreduce-trunk-Commit #2447 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2447/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Common-trunk-Commit #2429 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2429/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2497 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2497/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Hdfs-trunk #1096 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1096/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = FAILURE tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Mapreduce-trunk #1129 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1129/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Hdfs-trunk-Commit #2508 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2508/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java", "Integrated in Hadoop-Common-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2441/])     HADOOP-8566. AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays). (tucu) (Revision 1358454)       Result = SUCCESS tucu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1358454 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro/TestAvroSerialization.java"], "tasks": {"summarization": "AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays) - the accept() method should consider the case where the class getPackage() returns NULL....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8566 about?", "answer": "AvroReflectSerializer.accept(Class) throws a NPE if the class has no package (primitive types and arrays)"}}}
{"issue_id": "HADOOP-8565", "project": "HADOOP", "title": "AuthenticationFilter#doFilter warns unconditionally when using SPNEGO", "status": "Resolved", "priority": "Major", "reporter": "Eli Collins", "assignee": "Alejandro Abdelnur", "created": "2012-07-05T22:13:19.000+0000", "updated": "2012-07-11T22:42:25.000+0000", "labels": [], "description": "The following code in AuthenticationFilter#doFilter throws AuthenticationException (and warns) unconditionally because KerberosAuthenticator#authenticate returns null if SPNEGO is used.  {code}   token = authHandler.authenticate(httpRequest, httpResponse);   ...   if (token != null) { ... } else {     throw new AuthenticationException   } {code}", "comments": ["Only affects branch-1.", "This is a dup of HADOOP-8355 (which I've missed to backport to branch-1, doing that now)"], "tasks": {"summarization": "AuthenticationFilter#doFilter warns unconditionally when using SPNEGO  - The following code in AuthenticationFilter#doFilter throws AuthenticationException (and warns) uncon...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8565 about?", "answer": "AuthenticationFilter#doFilter warns unconditionally when using SPNEGO"}}}
{"issue_id": "HADOOP-8564", "project": "HADOOP", "title": "Port and extend Hadoop native libraries for Windows to address datanode concurrent reading and writing issue", "status": "Resolved", "priority": "Major", "reporter": "Chuan Liu", "assignee": "Chuan Liu", "created": "2012-07-05T18:48:38.000+0000", "updated": "2013-05-02T02:29:57.000+0000", "labels": [], "description": "HDFS files are made up of blocks. First, let\u2019s look at writing. When the data is written to datanode, an active or temporary file is created to receive packets. After the last packet for the block is received, we will finalize the block. One step during finalization is to rename the block file to a new directory. The relevant code can be found via the call sequence: FSDataSet.finalizeBlockInternal -> FSDir.addBlock. {code}          if ( ! metaData.renameTo( newmeta ) ||             ! src.renameTo( dest ) ) {           throw new IOException( \"could not move files for \" + b +                                  \" from tmp to \" +                                   dest.getAbsolutePath() );         } {code}  Let\u2019s then switch to reading. On HDFS, it is expected the client can also read these unfinished blocks. So when the read calls from client reach datanode, the datanode will open an input stream on the unfinished block file.  The problem comes in when the file is opened for reading while the datanode receives last packet from client and try to rename the finished block file. This operation will succeed on Linux, but not on Windows .  The behavior can be modified on Windows to open the file with FILE_SHARE_DELETE flag on, i.e. sharing the delete (including renaming) permission with other processes while opening the file. There is also a Java bug ([id 6357433|http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6357433]) reported a while back on this. However, since this behavior exists for Java on Windows since JDK 1.0, the Java developers do not want to break the backward compatibility on this behavior. Instead, a new file system API is proposed in JDK 7.   As outlined in the [Java forum|http://www.java.net/node/645421] by the Java developer (kbr), there are three ways to fix the problem: # Use different mechanism in the application in dealing with files. # Create a new implementation of InputStream abstract class using Windows native code. # Patch JDK with a private patch that alters FileInputStream behavior.  For the third option, it cannot fix the problem for users using Oracle JDK.  We discussed some options for the first approach. For example one option is to use two phase renaming, i.e. first hardlink; then remove the old hardlink when read is finished. This option was thought to be rather pervasive.  Another option discussed is to change the HDFS behavior on Windows by not allowing client reading unfinished blocks. However this behavior change is thought to be problematic and may affect other application build on top of HDFS.  For all the reasons discussed above, we will use the second approach to address the problem.  If there are better options to fix the problem, we would also like to hear about them.", "comments": ["+1 for the second option. This will also allow adding future optimization at the stream level on Windows, similar to the ones done for Linux.", "Can this be merged into the existing NativeIO JNI library? Or are the number of {{#ifdef WINDOWS}} macros required so numerous that we should just have two entirely separate libhadoops?", "{quote} Can this be merged into the existing NativeIO JNI library? Or are the number of #ifdef WINDOWS macros required so numerous that we should just have two entirely separate libhadoops? {quote} NativeIO JNI library is only available on Linux while this class is only needed on Windows. I think it make sense to create a separate native lib file. We don't necessary need to name it libhadoop. For example, if the class is called 'WindowsFileInputStream', the new lib could be 'WindowsFileInputStream.dll'. Is there any concern over this? E.g. you want to reduce native library files exposed in Hadoop in general?", "bq. NativeIO JNI library is only available on Linux while this class is only needed on Windows. I think it make sense to create a separate native lib file. We don't necessary need to name it libhadoop. For example, if the class is called 'WindowsFileInputStream', the new lib could be 'WindowsFileInputStream.dll'. Is there any concern over this? E.g. you want to reduce native library files exposed in Hadoop in general?  Currently NativeIO JNI is Linux-only, but I think all of the stuff found in there is useful on Windows as well. For example: - Native CRC32 computation: the SSE instructions probably need slightly different syntax for the Windows C++ compiler, but are necessary for good performance - Various other flags to open() needed for race-condition free security support: probably needs different APIs in Windows but likely there are equivalents available - Compression: Windows equally needs fast compression libraries, etc  So, I think it makes sense to get libhadoop generally compiling on Windows and making it the central place for native dependency code.", "Hi Todd, thanks for the clarification. I see you point now. However I think there are three things here.  # Make existing NativeIO works on Windows. # Create new Windows native IO functionality that solves the above issue. # Build and organize the code/lib so that we have a central place for the native code.  For this Jira, we only intend to solve 2. I agree with you on 1. For 3, I can see both pros and cons. But once 1 is done, there should be only modest work to create a common lib for all native code. Does this make sense to you?", "Yes, that makes sense. I was thinking that it makes sense to tackle #1 first -- even if it means that many of the native pieces are disabled for now on Windows. That way we only have to fix the build in one place, rather than adding a new build component and later merging the two builds.", "Hi Todd, I did some further investigation and coding. A basic porting of NativeIO is not working on Windows. However I did see some issues for a cross platform native library. For example, the open() method and fstat() method have some flags that are mostly specific to Linux or POSIX world. It may be difficult to find an exact mapping of those flags to Windows equivalent. Does it make sense to create sub classes to separate Unix/POSIX and Windows functions? E.g. we can have NativeIO.POSIX.open() and NativeIO.Windows.createFile(). However this will break existing APIs. I am not sure what is the best way to proceed.", "Hi Chuan. Thanks for taking a look into that.  I wouldn't be concerned about API compatibility here, since these are private-facing (internal) APIs. We can change them between versions without breaking any contracts with downstream projects.  I think your idea of separating the windows calls from the POSIX ones makes sense. But, we should probably also enumerate the uses of the POSIX calls and figure out what the equivalents are on the Windows side - for example, we use fstat and open(O_EXCL) for a lot of security reasons. I don't know for sure whether Windows has equivalent APIs or we need to take another route entirely in those situations.", "Attaching a patch and update the JIRA title to reflect the change.  We port and extend Hadoop native libraries to Windows.  The POSIX native functions and flags are moved under the new nested class NativeIO.POSIX. The Windows functions and flags are created under NativeIO.Windows.  We spent some time on investigating how to map POSIX APIs, specially all the flags to Windows equivalent. However, this seems very difficult if even possible given all the IO options and error codes.  Instead, we created some special IO functions in NativeIO, i.e. getShareDeleteFileInputStream(), getCreateForWriteFileOutputStream() that abstract the IO usage pattern.  We changed the related data node functions to use the new native library functions to get the desired I/O streams.  Some new test cases are added to TestNativeIO. TestFileConcurrentReader is fixed to test concurrent reading and writing scenarios.", "Todd, please post if you have any comments. Otherwise I am going to commit this tomorrow.", "Sorry, I missed that the new patch was uploaded. Can I have a couple days to review it? It's a big patch. If you want to go ahead and commit to the branch, that's OK so long as review feedback can be addressed afterwards.", "I will commit this patch, since there are other patches that are dependent on this.   One you post your review comments, it can be addressed in another Jira.", "Attaching the patch with indentation changed to spaces and CRLF changed LF.", "+1 for the patch. I committed it to branch-1-win. Thank you Chuan.  Todd, please do post your comments. It will be addressed by a separate jira.", "I reverted the commit.   Chuan the patch fails build: {noformat}     [javac] .../src/test/org/apache/hadoop/io/nativeio/TestNativeIO.java:83: cannot find symbol     [javac] symbol  : method setOwner(java.io.File,java.lang.String,<nulltype>)     [javac] location: class org.apache.hadoop.fs.FileUtil     [javac]     FileUtil.setOwner(testFile, username, null); {noformat}  Is this patch dependent on any other jira?", "Let me try it on my machine.", "Suresh, sorry for the break.  This indeed depends on another JIRA HADOOP-8763.  FileUtil.setOwner() method was added in that JIRA.", "Reopened because I had reverted the patch earlier due to build issue.", "I committed the patch.  Thank you Chuan.", "I forgot to include two new Windows build files. Attach a new patch of the two missing files.", "I committed additional files missed in previous commit to branch-1-win."], "tasks": {"summarization": "Port and extend Hadoop native libraries for Windows to address datanode concurrent reading and writing issue - HDFS files are made up of blocks. First, let\u2019s look at writing. When the data is written to datanode...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8564 about?", "answer": "Port and extend Hadoop native libraries for Windows to address datanode concurrent reading and writing issue"}}}
{"issue_id": "HADOOP-8563", "project": "HADOOP", "title": "don't package hadoop-pipes examples/bin", "status": "Closed", "priority": "Minor", "reporter": "Colin McCabe", "assignee": "Colin McCabe", "created": "2012-07-05T18:23:04.000+0000", "updated": "2012-10-11T17:45:04.000+0000", "labels": [], "description": "Let's not package hadoop-pipes examples/bin", "comments": ["The reason for removing these from the packaged distribution is that they probably are primarily useful to developers, who will have the source files anyway, not just the stuff that's packaged.  See HADOOP-8547 for more discussion about why we should remove them (although it would be nice if new discussion happened here)", "-1 overall.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12535238/HADOOP-8563.001.patch   against trunk revision .      +1 @author.  The patch does not contain any @author tags.      -1 tests included.  The patch doesn't appear to include any new or modified tests.                         Please justify why no new tests are needed for this patch.                         Also please list what manual steps were performed to verify this patch.      +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      +1 javadoc.  The javadoc tool did not generate any warning messages.      +1 eclipse:eclipse.  The patch built with eclipse:eclipse.      +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      +1 release audit.  The applied patch does not increase the total number of release audit warnings.      +1 core tests.  The patch passed unit tests in hadoop-assemblies.      +1 contrib tests.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1171//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1171//console  This message is automatically generated.", "Thanks Colin, taking a look now.  Sorry again for coming in so late on 8547.", "+1 lgtm.", "Integrated in Hadoop-Common-trunk-Commit #2426 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2426/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk-Commit #2494 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2494/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk-Commit #2443 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/2443/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = FAILURE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1095 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1095/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = FAILURE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1128 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1128/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk-Commit #2508 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit/2508/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Common-trunk-Commit #2441 (See [https://builds.apache.org/job/Hadoop-Common-trunk-Commit/2441/])     HADOOP-8563. don't package hadoop-pipes examples/bin (Colin Patrick McCabe via tgraves) (Revision 1357811)       Result = SUCCESS tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1357811 Files :  * /hadoop/common/trunk/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt"], "tasks": {"summarization": "don't package hadoop-pipes examples/bin - Let's not package hadoop-pipes examples/bin...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8563 about?", "answer": "don't package hadoop-pipes examples/bin"}}}
{"issue_id": "HADOOP-8562", "project": "HADOOP", "title": "Enhancements to support Hadoop on Windows Server and Windows Azure environments", "status": "Closed", "priority": "Major", "reporter": "Bikas Saha", "assignee": "Bikas Saha", "created": "2012-07-05T17:57:31.000+0000", "updated": "2019-01-12T00:21:46.000+0000", "labels": [], "description": "This JIRA tracks the work that needs to be done on trunk to enable Hadoop to run on Windows Server and Azure environments. This incorporates porting relevant work from the similar effort on branch 1 tracked via HADOOP-8079.", "comments": ["Attaching a merge patch for all the changes that need to be merged from branch-trunk-win to trunk.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12560729/branch-trunk-win.patch   against trunk revision .      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1861//console  This message is automatically generated.", "Looks like this patch has unrelated changes in it as well?", "I see, let me check.", "I diffed against a wrong branch (branch-1-win) instead of trunk. Here is the correct patch.", "The patch still undoes a bunch of changes that have gone into trunk recently (look at the CHANGES.txt removals in the diff for example). I think you need to merge trunk into the windows branch before doing this diff.", "Todd, I just wanted to post a patch to get Jenkins results and to give an idea how a merge patch looks. This patch is not intended to be committed. There is still some work left in passing 100% unit tests on windows etc.", "Updated patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12560915/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 61 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM                   org.apache.hadoop.streaming.TestStreamingBadRecords                   org.apache.hadoop.streaming.TestMultipleCachefiles                   org.apache.hadoop.streaming.TestMultipleArchiveFiles                   org.apache.hadoop.streaming.TestStreamingTaskLog                   org.apache.hadoop.streaming.TestFileArgs                   org.apache.hadoop.streaming.TestSymLink                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1871//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1871//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1871//console  This message is automatically generated.", "The FindBugs warnings look legitimate, so we'll need to fix those.  {{TestMapReduceChildJVM}} will be fixed by committing the patch attached to MAPREDUCE-4869.  All of the test failures in streaming are caused by \"Queue configuration missing child queue names for root\".  I can repro this even on the trunk codebase, so it appears to be unrelated to the branch-trunk-win merge.  {{TestNodeManagerShutdown}} has the same failure for me even on trunk, so it appears to be unrelated to the branch-trunk-win merge.  For all other YARN test failures, I can't repro.  I tried trunk, branch-trunk-win, and trunk + this merge patch.  For all of these, I saw no failures in {{TestContainerManager}}, {{TestContainersMonitor}}, or {{TestContainerLaunch}}.", "Updated patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12561134/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 60 new or modified test files.        {color:red}-1 javac{color}.  The applied patch generated 2016 javac compiler warnings (more than the trunk's current 2013 warnings).      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.fs.TestFileUtil                   org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM                   org.apache.hadoop.streaming.TestStreamingBadRecords                   org.apache.hadoop.streaming.TestMultipleCachefiles                   org.apache.hadoop.streaming.TestMultipleArchiveFiles                   org.apache.hadoop.streaming.TestStreamingTaskLog                   org.apache.hadoop.streaming.TestFileArgs                   org.apache.hadoop.streaming.TestSymLink                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1883//testReport/ Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1883//artifact/trunk/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1883//console  This message is automatically generated.", "Updated patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12561221/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 26 new or modified test files.        {color:red}-1 javac{color}.  The applied patch generated 2016 javac compiler warnings (more than the trunk's current 2013 warnings).      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.fs.TestFileUtil                   org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM                   org.apache.hadoop.streaming.TestSymLink                   org.apache.hadoop.streaming.TestMultipleCachefiles                   org.apache.hadoop.streaming.TestStreamingBadRecords                   org.apache.hadoop.streaming.TestFileArgs                   org.apache.hadoop.streaming.TestMultipleArchiveFiles                   org.apache.hadoop.streaming.TestStreamingTaskLog                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1894//testReport/ Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1894//artifact/trunk/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1894//console  This message is automatically generated.", "The failure in {{TestFileUtil}} is caused by 2 missing tar files required for testing untar functionality.  Patch files don't seem to have a good way to carry over binary files, so we'll need to add them manually with the merge.  I have attached test-untar.tar and test-untar.tgz.  These must be committed to hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs when it's time.  All of the streaming failures are unrelated to the merge.  The current trunk code has the same problem.  I submitted a patch on MAPREDUCE-4884 to fix it.  For the failures in {{TestContainerManager}}, {{TestContainerLaunch}}, and {{TestContainersMonitor}}, I still don't have a repro or an explanation.  It seems to be only happening on Jenkins.  These need further investigation.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12561257/test-untar.tgz   against trunk revision .      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1897//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12561323/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 26 new or modified test files.        {color:red}-1 javac{color}.  The applied patch generated 2016 javac compiler warnings (more than the trunk's current 2013 warnings).      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.fs.TestFileUtil                   org.apache.hadoop.mapreduce.v2.app.job.impl.TestMapReduceChildJVM                   org.apache.hadoop.streaming.TestSymLink                   org.apache.hadoop.streaming.TestMultipleCachefiles                   org.apache.hadoop.streaming.TestStreamingBadRecords                   org.apache.hadoop.streaming.TestFileArgs                   org.apache.hadoop.streaming.TestMultipleArchiveFiles                   org.apache.hadoop.streaming.TestStreamingTaskLog                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1900//testReport/ Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/1900//artifact/trunk/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1900//console  This message is automatically generated.", "Updated merge patch", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12566396/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 29 new or modified test files.        {color:red}-1 javac{color}.  The applied patch generated 2017 javac compiler warnings (more than the trunk's current 2014 warnings).      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.          {color:red}-1 release audit{color}.  The applied patch generated 4 release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager:                    org.apache.hadoop.fs.TestFileUtil                   org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2088//testReport/ Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2088//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2088//artifact/trunk/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2088//console  This message is automatically generated.", "A few notes on the test failures:  TestFileUtil - This has the same root cause mentioned earlier: the test depends on a tar file to be checked in as test input, and the diffs are not carrying over binary files.  TestNMWebServices - This is a trunk bug, which will be fixed by the patch that [~kkambatl] and I wrote for HADOOP-9246.  TestContainerManager, TestNodeManagerShutdown, TestContainerLaunch, TestContainersMonitor - These need further investigation.  I cannot repro the test failures on Mac, but I can repro them on Ubuntu.  Only branch-trunk-win has the failures, not trunk.  Perhaps some of the branch-trunk-win changes in container launching caused a regression.  I filed YARN-359.", "Latest merge patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12567057/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 32 new or modified test files.        {color:red}-1 javac{color}.  The applied patch generated 2017 javac compiler warnings (more than the trunk's current 2014 warnings).      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.          {color:red}-1 release audit{color}.  The applied patch generated 4 release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.fs.TestFileUtil                   org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager                   org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown                   org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch                   org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2117//testReport/ Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2117//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt Findbugs warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2117//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html Javac warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2117//artifact/trunk/patchprocess/diffJavacWarnings.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2117//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12568491/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 34 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.          {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2174//testReport/ Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2174//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2174//console  This message is automatically generated.", "The test failure in {{TestHttpFSFWithWebhdfsFileSystem}} appears to be unrelated to branch-trunk-win changes, as other builds have been failing intermittently for the same reason.  For example, see https://builds.apache.org/job/Hadoop-Hdfs-trunk/1312/ .  The release audit warning is on /home/jenkins/jenkins-slave/workspace/PreCommit-HADOOP-Build/trunk/hadoop-common-project/hadoop-common/CHANGES.txt.orig.  That file is not in the latest patch, so I'm not sure where it came from.", "bq. CHANGES.txt.orig This must be from my dev area (due to merge issues). Next merge patch should not have this.", "Given some of the discussion on recent merge thread, I am asking for any reviewers interested in reviewing this patch to start reviewing this patch. I plan to call for merge vote in a week or so.   My +1 for the consolidated patch.", "+1 non-binding  This code has been tested successfully on Linux and Windows for the past several months.", "+1 non-binding  We have extensively tested this on both Linux and Windows.", "+1 non-binding  Tested it thoroughly", "+1 non-binding", "Here is a patch I am attaching that excludes cmd changes, winutils changes and pom changes to give an idea how much change is in core hadoop.  The number of lines goes to 1537 lines from the original file size of 15958 lines.", "Canceling the patch for now.", "Suresh, your branch-trunk-win-min.patch includes only Yarn changes. Changes for common and hdfs are not there. I guess it will be more than 1537 lines then?", "bq. Suresh, your branch-trunk-win-min.patch includes only Yarn changes. Changes for common and hdfs are not there. I guess it will be more than 1537 lines then?  yes. Sorry I removed more than necessary. I have attached two versions of the smaller patch.  https://issues.apache.org/jira/secure/attachment/12571663/branch-trunk-win.min.patch (8756 lines) https://issues.apache.org/jira/secure/attachment/12571660/branch-trunk-win.min-notest.patch (5996 lines)  It should make it easier to get an idea the changes.", "Thanks. This is better indeed. Shows significant test coverage. Few general comments on the patch # /hadoop-common-project/hadoop-common/src/main/native/native.sln and a few other new files do not have Apache license. # Do you still need files CHANGES.branch-trunk-win.txt? It will be incorporated into CHANGES.txt? # Do we still need cygwin after that patch? If not shouldn't all cygpath occurrences be removed?", "[~shv] Thanks for review. I think native.sln is added to list of files that should be ignored from Apache license text check. [~cnauroth] can .sln files support inclusion of Apache license?  bq. Do you still need files CHANGES.branch-trunk-win.txt? It will be incorporated into CHANGES.txt? Yes. After the merge I will remove that file and merge it into CHANGES.txt as done for previous feature branch changes.  bq. Do we still need cygwin after that patch? If not shouldn't all cygpath occurrences be removed? [~cnauroth] can you please answer this?", "{quote} Chris Nauroth can .sln files support inclusion of Apache license? {quote}  Yes, we have the license header in winutils.sln, but we must have forgotten to add it to native.sln.  I'll prepare a patch to add it.  {quote} Do we still need cygwin after that patch? If not shouldn't all cygpath occurrences be removed? {quote}  We do not need cygwin.  I'll prepare a patch to remove the remaining occurrences of cygpath.", "Makes sense guys, thanks.", "Submitting latest merge patch to Jenkins precommit build.", "Precommit build has timeout set to 180 minutes. Running tests for all the projects takes more than 180 minutes. Hence no results are posted from the Jenkins build.  Here are the results of the test completed so far - https://builds.apache.org/job/PreCommit-HADOOP-Build/2263/testReport/  I have asked [~gkesavan] to see if the timeout can be increased.", "Suresh, I ve increased the jenkins build abort threshold and set it to 300 minutes.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12572045/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 36 new or modified test files.    {color:red}-1 one of tests included doesn't have a timeout.{color}      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.          {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.streaming.TestStreamReduceNone                   org.apache.hadoop.streaming.TestStreamXmlRecordReader                   org.apache.hadoop.test.TestHFSTestCase                   org.apache.hadoop.fs.http.server.TestHttpFSServer                   org.apache.hadoop.fs.http.client.TestHttpFSFWithWebhdfsFileSystem                   org.apache.hadoop.fs.http.client.TestHttpFSFileSystemLocalFileSystem                   org.apache.hadoop.lib.service.hadoop.TestFileSystemAccessService                   org.apache.hadoop.fs.http.client.TestHttpFSWithHttpFSFileSystem      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2266//testReport/ Release audit warnings: https://builds.apache.org/job/PreCommit-HADOOP-Build/2266//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2266//console  This message is automatically generated.", "release audit is related to CHANGES.txt.orig file and is due to patch not cleanly applying. Some of the test failures are due to MiniDFSCluster related timeout. It would have been good to flag the tests that do not have timeout. I will dig into it.", "Updated patch.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12572231/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 36 new or modified test files.    {color:red}-1 one of tests included doesn't have a timeout.{color}      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.streaming.TestStreamReduceNone                   org.apache.hadoop.streaming.TestStreamXmlRecordReader      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2269//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2269//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12572248/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 37 new or modified test files.      {color:green}+1 tests included appear to have a timeout.{color}      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.util.TestWinUtils                   org.apache.hadoop.mapreduce.v2.TestMRJobs                   org.apache.hadoop.streaming.TestStreamReduceNone                   org.apache.hadoop.streaming.TestStreamXmlRecordReader      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2271//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2271//console  This message is automatically generated.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12572248/branch-trunk-win.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 37 new or modified test files.      {color:green}+1 tests included appear to have a timeout.{color}      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-assemblies hadoop-common-project/hadoop-common hadoop-dist hadoop-hdfs-project/hadoop-hdfs hadoop-hdfs-project/hadoop-hdfs-httpfs hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle hadoop-tools/hadoop-streaming hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests:                    org.apache.hadoop.util.TestWinUtils                   org.apache.hadoop.mapreduce.v2.TestMRJobs                   org.apache.hadoop.streaming.TestStreamReduceNone                   org.apache.hadoop.streaming.TestStreamXmlRecordReader      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/2272//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2272//console  This message is automatically generated.", "TestWinUtils and TestMRJobs failures are caused by HADOOP-9368. I'll reactivate it and add a patch.  Not sure about the other two. Will look tomorrow.", "org.apache.hadoop.streaming.TestStreamReduceNone org.apache.hadoop.streaming.TestStreamXmlRecordReader  These two failures look unrelated to the patch. They repro on current trunk.", "bq. TestWinUtils and TestMRJobs failures are caused by HADOOP-9368. I'll reactivate it and add a patch Lets create a separate Jira to address this. I am going to merge this patch to trunk shortly.", "TestStreamReduceNone and TestStreamXmlRecordReader are tracked by MAPREDUCE-5006", "I committed the patch to trunk. Thanks to van Mitic, Chuan Liu, Ramya Sunil, Bikas Saha, Kanna Karanam, John Gordon, Brandon Li, Chris Nauroth, David Lao, Sumadhur Reddy Bolli, Arpit Agarwal, Ahmed El Baz, Mike Liddell, Jing Zhao, Thejas Nair, Steve Maine, Ganeshan Iyer, Raja Aluri, Giridharan Kesavan, Ramya Bharathi Nimmagadda, Daryn Sharp, Arun Murthy, Tsz-Wo Nicholas Sze, Suresh Srinivas and Sanjay Radia. Also thanks to others who provided comments on the jiras and participated in the discussions.", "The changes from this jira and related jiras from HDFS and YARN has been in trunk for some time now. If there are no objections, I plan on merging these changes to branch-2 early next week.", "I plan on merging changes from this jira and related jiras to branch-2 by the end of the day, per my previous comment.", "+1 for merge to branch-2.  Thanks, Suresh!", "Initial patch for merge to branch-2.", "Updated merge patch to include fix for HADOOP-9372.", "So far, I see the following problems in the branch-2 merge patch:  # hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java appears to be incorrect.  In particular, there are {{@Test}} annotations on private methods that aren't annotated in trunk.  This causes JUnit to fail while trying to run the private methods as tests.  Probably the easiest way to resolve this is to just copy the current trunk version. # hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java does not include the patches required for the test to pass on Windows.  This included timeout tuning and special handling for differences in symlink behavior on Windows.  Once again, I believe the easiest solution is just to copy the trunk version.", "Chris, 1 and part of 2 should be fixed by the updated patch posted earlier.  More Windows fixes are still to be merged in.", "{quote} Chris, 1 and part of 2 should be fixed by the updated patch posted earlier. {quote}  Yes, you're right.  I had been running the older patch.  Thanks, Arpit!", "List of failures with the patch:  # TestBinaryTokenFile # TestDistributedShell # TestEncryptedShuffle # TestMRJobsWithHistoryService # TestNoDefaultsJobConf # TestResources # TestSymLink # TestWebHdfsTimeouts # TestYarnVersionInfo  Some of these are likely to be timeouts since I am running an underpowered VM. Chris just mentioned he can only reproduce two of these.", "Yes, there were only 2 failures for me.  TestEncryptedShuffle appears to be flaky, approximately 1 failure in every 5 test runs for me.  This can occur on current branch-2, so it's unrelated to the merge patch.  TestMRJobsWithHistoryService has a timeout value that is too low for a slower dev environment, like a VM.  The current timeout is 30s.  I typically see it complete in just under 30s, and sometimes it creeps over that and times out.  I think we need to increase the timeout to accommodate slow VMs.  Again, this is unrelated to the merge patch, because I can repro on branch-2 without it.", "Thanks for the verification Chris!", "Thanks guys for running the tests. I plan to merge this change with + hadoop-9372. The other related jiras I will merge one at a time. [~vinodkv], [~arpitagarwal] and [~cnauroth], let me know of you guys are okay.", "Sounds good to me.  Thanks Suresh.", "Sure, I'm holding off commits on YARN/MR. The test-failures can be fixed post merge.", "I meant timeout issues with the tests, and randomly failing tests like TestEncryptedShuffle.", "I think the merge patch is still missing MAPREDUCE-4987, which fixed handling of symlinks in the distributed cache on Windows.  HADOOP-9372 includes some of the timeout tuning, but it doesn't include the logic changes.  For example, here are links to some trunk code for ContainerLaunch and TestMRJobs.  I don't see this code in the merge patch.  https://github.com/apache/hadoop-common/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java#L610  https://github.com/apache/hadoop-common/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java#L482", "Arpit just let me know that the goal here is to merge the HADOOP-8562 changes, and since MAPREDUCE-4987 was not part of HADOOP-8562 (it was committed later), it is not included in the merge patch.  Remaining patches like this will get merged later.  Considering that, I am +1 for this merge patch.  Thanks, everyone.", "Reopening the issue for merging.", "I merge this patch into branch-2. Thank you every one for testing. Can you folks run another quick test to verify things are merged correctly?", "The merge is looking good. Just two failures on Mac.  # TestRBWBlockInvalidation (timeout) # TestBookKeeperHACheckpoints (JVM OOM)", "Integrated in Hadoop-trunk-Commit #3790 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3790/])     Update hadoop-common CHANGES.txt after merging HADOOP-8562 (Revision 1486288)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486288 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Yarn-trunk #220 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/220/])     Update hadoop-common CHANGES.txt after merging HADOOP-8562 (Revision 1486288)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486288 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1410 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1410/])     Update hadoop-common CHANGES.txt after merging HADOOP-8562 (Revision 1486288)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486288 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1436 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1436/])     Update hadoop-common CHANGES.txt after merging HADOOP-8562 (Revision 1486288)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486288 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt", "Integrated in Hadoop-trunk-Commit #3791 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3791/])     HADOOP-8562. Fix CHANGES.txt to move 8562 and related tasks to 2.0.5 section (Revision 1486334)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486334 Files :  * /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "Integrated in Hadoop-trunk-Commit #3792 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3792/])     HADOOP-8562. Moved related jiras to 2.0.5 section in CHANGES.txt (Revision 1486336)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486336 Files :  * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt * /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt", "Integrated in Hadoop-Yarn-trunk #221 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/221/])     HADOOP-8562. Moved related jiras to 2.0.5 section in CHANGES.txt (Revision 1486336) HADOOP-8562. Fix CHANGES.txt to move 8562 and related tasks to 2.0.5 section (Revision 1486334)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486336 Files :  * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt * /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt  suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486334 Files :  * /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "Integrated in Hadoop-Hdfs-trunk #1411 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1411/])     HADOOP-8562. Moved related jiras to 2.0.5 section in CHANGES.txt (Revision 1486336) HADOOP-8562. Fix CHANGES.txt to move 8562 and related tasks to 2.0.5 section (Revision 1486334)       Result = FAILURE suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486336 Files :  * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt * /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt  suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486334 Files :  * /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt", "Integrated in Hadoop-Mapreduce-trunk #1437 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1437/])     HADOOP-8562. Moved related jiras to 2.0.5 section in CHANGES.txt (Revision 1486336) HADOOP-8562. Fix CHANGES.txt to move 8562 and related tasks to 2.0.5 section (Revision 1486334)       Result = SUCCESS suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486336 Files :  * /hadoop/common/trunk/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt * /hadoop/common/trunk/hadoop-yarn-project/CHANGES.txt  suresh : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1486334 Files :  * /hadoop/common/trunk/hadoop-mapreduce-project/CHANGES.txt"], "tasks": {"summarization": "Enhancements to support Hadoop on Windows Server and Windows Azure environments - This JIRA tracks the work that needs to be done on trunk to enable Hadoop to run on Windows Server a...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8562 about?", "answer": "Enhancements to support Hadoop on Windows Server and Windows Azure environments"}}}
{"issue_id": "HADOOP-8561", "project": "HADOOP", "title": "Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes", "status": "Closed", "priority": "Major", "reporter": "Luke Lu", "assignee": "Yu Gao", "created": "2012-07-05T10:08:47.000+0000", "updated": "2014-09-03T23:22:28.000+0000", "labels": [], "description": "To solve the problem for an authenticated user to type hadoop shell commands in a web console, we can introduce an HADOOP_PROXY_USER environment variable to allow proper impersonation in the child hadoop client processes.", "comments": ["This would be really good for testing as well.  We have seen issues with HFTP tokens being broken only for proxy users, but were not testing it properly.  This should make that testing a lot simpler in the future.  +1 for the idea.", "We achieved this in Hue with a simple wrapper around FsShell:  http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/com.cloudera.hue/sudo-shell/1.2.0-cdh3u0/com/cloudera/hue/SudoFsShell.java?av=f", "I'm not against making an environment variable/property to set the user, but we might as well use the one we already have and enable         HADOOP_USER_NAME in secure mode to mean act as a proxy for the given user.", "We'd also like to use proxy user in \"semi\" secure mode as well.", "@Owen, I'm fine with repurposing HADOOP_USER_NAME and for proxy user (better auditing and access control even without kerbero), though it's an incompatible change. One of the reasons we added HADOOP_PROXY_USER is to preserve the original semantics for HADOOP_USER_NAME.", "I kind of like Todd's approach.  Maybe we should consider adding a sudo command to {{FsShell}} so it's not a separate utility.  Using an env makes me a bit squeamish since it may introduce an unexpected attack vector.", "Moved to 1.2.0 upon release of 1.1.0.", "This approach has added benefit of working with clients (like HBase shell) not written in Java.  bq. Using an env makes me a bit squeamish since it may introduce an unexpected attack vector.  It won't do anything for ordinary users. An admin web app of course needs to do a few things sanitize the input to disallow fork/exec etc.", "The patches lgtm. +1 pending jenkins.", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12551912/hadoop-8561.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1701//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1701//console  This message is automatically generated.", "Need to merge with HADOOP-9035", "{color:green}+1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12561097/hadoop-8561-v2.patch   against trunk revision .      {color:green}+1 @author{color}.  The patch does not contain any @author tags.      {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.      {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.      {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.      {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.      {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.      {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.      {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common.      {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.  Test results: https://builds.apache.org/job/PreCommit-HADOOP-Build/1878//testReport/ Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1878//console  This message is automatically generated.", "Integrated in Hadoop-trunk-Commit #3128 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/3128/])     HADOOP-8561. Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes. (Yu Gao via llu) (Revision 1422429)       Result = SUCCESS llu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1422429 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestProxyUserFromEnv.java", "Committed to trunk and branch-{2,1,1.1}. Thanks Yu!", "Integrated in Hadoop-Yarn-trunk #67 (See [https://builds.apache.org/job/Hadoop-Yarn-trunk/67/])     HADOOP-8561. Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes. (Yu Gao via llu) (Revision 1422429)       Result = SUCCESS llu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1422429 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestProxyUserFromEnv.java", "Integrated in Hadoop-Hdfs-trunk #1256 (See [https://builds.apache.org/job/Hadoop-Hdfs-trunk/1256/])     HADOOP-8561. Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes. (Yu Gao via llu) (Revision 1422429)       Result = FAILURE llu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1422429 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestProxyUserFromEnv.java", "Integrated in Hadoop-Mapreduce-trunk #1287 (See [https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1287/])     HADOOP-8561. Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes. (Yu Gao via llu) (Revision 1422429)       Result = SUCCESS llu : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1422429 Files :  * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java * /hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestProxyUserFromEnv.java", "Integrated in Hadoop-Hdfs-0.23-Build #470 (See [https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/470/])     HADOOP-8561. Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes (Yu Gao via tgraves) (Revision 1424698)       Result = UNSTABLE tgraves : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1424698 Files :  * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/CHANGES.txt * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java * /hadoop/common/branches/branch-0.23/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestProxyUserFromEnv.java"], "tasks": {"summarization": "Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes - To solve the problem for an authenticated user to type hadoop shell commands in a web console, we ca...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8561 about?", "answer": "Introduce HADOOP_PROXY_USER for secure impersonation in child hadoop client processes"}}}
{"issue_id": "HADOOP-8560", "project": "HADOOP", "title": "Change TestFTPFileSystem to use non-SNAPSHOT dependencies", "status": "Resolved", "priority": "Minor", "reporter": "Luke Lu", "assignee": "Yu Gao", "created": "2012-07-05T09:49:30.000+0000", "updated": "2015-05-08T09:22:11.000+0000", "labels": [], "description": "It would good if the stable hadoop release don't depend on SNAPSHOT ftpserver artifacts.", "comments": ["The patch lgtm. +1 pending ant test and test-patch results. Note, the issue is resolved in trunk via the mavenization.", "Attached ant test and test-patch results. Also put the overall result here:  ant test-patch:      [exec] +1 overall.        [exec]       [exec]     +1 @author.  The patch does not contain any @author tags.      [exec]       [exec]     +1 tests included.  The patch appears to include 3 new or modified tests.      [exec]       [exec]     +1 javadoc.  The javadoc tool did not generate any warning messages.      [exec]       [exec]     +1 javac.  The applied patch does not increase the total number of javac compiler warnings.      [exec]       [exec]     +1 findbugs.  The patch does not introduce any new Findbugs (version 2.0.1) warnings.  ant test -Dtestcase=TestFTPFileSystem Testsuite: org.apache.hadoop.fs.ftp.TestFTPFileSystem Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 20.243 sec", "The SNAPSHOT ftpserver and mina jars should be removed from src/test/lib as well.", "Eric, this doesn't apply to trunk. So there is no need to PA, as it'll fail hadoopqa.", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12552568/test-TestFTPFileSystem.result   against trunk revision .      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/1858//console  This message is automatically generated.", "The patch hadoop-8560-branch-1.patch is removing a required \"<dependency>\" tag from the file ivy/hadoop-test-pom-template.xml. I am attaching a new patch to fix this issue (hadoop-8560-branch-1_v2.patch).", "{color:red}-1 overall{color}.  Here are the results of testing the latest attachment    http://issues.apache.org/jira/secure/attachment/12568075/hadoop-8560-branch-1_v2.patch   against trunk revision .      {color:red}-1 patch{color}.  The patch command could not apply the patch.  Console output: https://builds.apache.org/job/PreCommit-HADOOP-Build/2152//console  This message is automatically generated.", "\\\\ \\\\ | (x) *{color:red}-1 overall{color}* | \\\\ \\\\ || Vote || Subsystem || Runtime || Comment || | {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. | \\\\ \\\\ || Subsystem || Report/Notes || | Patch URL | http://issues.apache.org/jira/secure/attachment/12568075/hadoop-8560-branch-1_v2.patch | | Optional Tests | javac unit findbugs checkstyle | | git revision | trunk / f1a152c | | Console output | https://builds.apache.org/job/PreCommit-HADOOP-Build/6268/console |   This message was automatically generated.", "Thanks to [~crystal_gaoyu] and [~vicaya] for working on this JIRA.  Resolving this as won't fix becase FTPFileSystem is no more depends on org.apache.ftpserver:ftplet-api. (trunk and branch2 is not having this dependency)  Please feel free to reopen if some one feels its valid"], "tasks": {"summarization": "Change TestFTPFileSystem to use non-SNAPSHOT dependencies - It would good if the stable hadoop release don't depend on SNAPSHOT ftpserver artifacts....", "classification": "task", "qna": {"question": "What is the issue HADOOP-8560 about?", "answer": "Change TestFTPFileSystem to use non-SNAPSHOT dependencies"}}}
{"issue_id": "HADOOP-8559", "project": "HADOOP", "title": "PMML Support in Hadoop Cluster", "status": "Resolved", "priority": "Minor", "reporter": "Duraimurugan", "assignee": null, "created": "2012-07-05T04:24:16.000+0000", "updated": "2012-07-05T20:38:54.000+0000", "labels": ["newbie"], "description": "Would like to request a support for PMML. With that once the predictive models are built and provided in PMML format, we should be able to import into hadoop cluster for scoring.", "comments": ["Duraimurugan,  This sounds like a great idea, but something that would fit much more closely with the Mahout project, then with the Hadoop project.  Please take a look at Mahout http://mahout.apache.org/ and see if my analysis is correct.  If it is then please file the JIRA there.  If not feel free to reopen this ticket.", "Hi Robert,  I agree that this request is very related to Mahout in the context of predictive analytics and machine learning process. But there are quite a few open source tools available for machine learning (R, weka,etc) but that is not scalable or fit in Hadoop architecture. PMML will give an option for models being built using different tools and still can leverage hadoop capabilities for scoring. I would like to request all your thoughts here before we close or reopen this ticket.   Thanks  Durai", "Durai,  I agree that having PMML support on top of Hadoop is a great thing, but my concern is with the maintainability and scope of this request.  Most of the people who contribute to Hadoop proper have a systems background.  Granted there are quite a few of us that also have dabbled in Machine Learning, but not nearly as many as on Mahout.  It is somewhat like asking the Linux Kernel project to also maintain a PMML library, they could probably do it, but is it the right group of people to properly maintain it and does it even fit in with the project's goals?  For Hadoop proper I would say no it does not fit with out goals, but it sounds like it would fit perfectly with Mahout's goals.", "Sure, it make sense. I will request this in Mahout."], "tasks": {"summarization": "PMML Support in Hadoop Cluster - Would like to request a support for PMML. With that once the predictive models are built and provide...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8559 about?", "answer": "PMML Support in Hadoop Cluster"}}}
{"issue_id": "HADOOP-8558", "project": "HADOOP", "title": "Hadoop RPC does not allow protocol extension with common interfaces.", "status": "Resolved", "priority": "Major", "reporter": "Konstantin Shvachko", "assignee": "Konstantin Shvachko", "created": "2012-07-05T00:46:57.000+0000", "updated": "2015-03-02T16:50:25.000+0000", "labels": [], "description": "Hadoop RPC fails if MyProtocol extends an interface, which is not a VersionedProtocol even if MyProtocol extends also VersionedProtocol. The reason is that Invocation uses Method.getDeclaringClass(), which returns the interface class rather than the class of MyProtocol. This is incompatible with former versions.", "comments": ["This was introduced in 0.23 by HADOOP-7227.", "Here is an example of what I mean. {{TestProtocol}} extends {{VersionedProtocol}} and {{ProtocolExtension}}. The latter is not versioned, and rpc fails with  {code} Tests in error: testCalls(org.apache.hadoop.ipc.TestProtocolExtension): java.lang.NoSuchFieldException: versionID {code}  I think there is no reason for this behavior, because {{TestProtocol}} is perfectly versioned. The solution is to overload in {{TestProtocol}} the method declared in {{ProtocolExtension}}. (Uncomment the lines in the test to make it pass.) This is inconvenient if {{ProtocolExtension}} has a lot of methods.  This basically prevents from using other existing interfaces to define new protocols. And this is incompatible with previous versions of hadoop, as the same test runs fine with e.g. 0.22.  Solution is simple enough - need get rid of getDeclaringClass() by passing the actual protocol into Invocation constructor. I would like to here if this is done intentionally and has some meaning I don't understand.", "Turns out that the version can be specified in annotation now. In my example above if I add {{ProtocolInfo}} annotation specifying the version like this {code} @ProtocolInfo(     protocolName = \"org.apache.hadoop.ipc.TestProtocolExtension$TestProtocol\",      protocolVersion = TestProtocol.versionID) public interface ProtocolExtention {   void logClassName(); } {code} the test will pass. {{ProtocolInfo}} was introduced by HADOOP-7524.", "Attaching version of the test that passes under current trunk.", "Resolving as not a problem."], "tasks": {"summarization": "Hadoop RPC does not allow protocol extension with common interfaces. - Hadoop RPC fails if MyProtocol extends an interface, which is not a VersionedProtocol even if MyProt...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8558 about?", "answer": "Hadoop RPC does not allow protocol extension with common interfaces."}}}
{"issue_id": "HADOOP-8557", "project": "HADOOP", "title": "Core Test failed in jekins for patch pre-commit", "status": "Resolved", "priority": "Blocker", "reporter": "Junping Du", "assignee": null, "created": "2012-07-04T23:41:17.000+0000", "updated": "2012-07-17T22:51:54.000+0000", "labels": [], "description": "In jenkins PreCommit build history (https://builds.apache.org/job/PreCommit-HADOOP-Build/), following tests are failed for all recently patches (build-1164,1166,1168,1170): org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover \t org.apache.hadoop.ha.TestZKFailoverController.testOneOfEverything  org.apache.hadoop.io.file.tfile.TestTFileByteArrays.testOneBlock  org.apache.hadoop.io.file.tfile.TestTFileByteArrays.testOneBlockPlusOneEntry \t org.apache.hadoop.io.file.tfile.TestTFileByteArrays.testThreeBlocks org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays.testOneBlock org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays.testOneBlockPlusOneEntry org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays.testThreeBlocks", "comments": ["Resolving as dup of HADOOP-8537"], "tasks": {"summarization": "Core Test failed in jekins for patch pre-commit  - In jenkins PreCommit build history (https://builds.apache.org/job/PreCommit-HADOOP-Build/), followin...", "classification": "task", "qna": {"question": "What is the issue HADOOP-8557 about?", "answer": "Core Test failed in jekins for patch pre-commit"}}}
