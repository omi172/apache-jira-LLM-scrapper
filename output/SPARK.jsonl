{"issue_id": "SPARK-53828", "project": "SPARK", "title": "Exclude `org/apache/spark/ui/static/**` from Spark operator jar", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T23:53:10.000+0000", "updated": "2025-10-08T00:13:28.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 376 [https://github.com/apache/spark-kubernetes-operator/pull/376]"], "tasks": {"summarization": "Exclude `org/apache/spark/ui/static/**` from Spark operator jar - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53828 about?", "answer": "Exclude `org/apache/spark/ui/static/**` from Spark operator jar"}}}
{"issue_id": "SPARK-53827", "project": "SPARK", "title": "Exclude `commons-(codec|compress)` and `com.ibm.icu` transitive dependencies", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T23:32:30.000+0000", "updated": "2025-10-08T00:03:39.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 375 [https://github.com/apache/spark-kubernetes-operator/pull/375]"], "tasks": {"summarization": "Exclude `commons-(codec|compress)` and `com.ibm.icu` transitive dependencies  - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53827 about?", "answer": "Exclude `commons-(codec|compress)` and `com.ibm.icu` transitive dependencies"}}}
{"issue_id": "SPARK-53826", "project": "SPARK", "title": "Use Java 25 in GitHub Action jobs consistently", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T21:28:19.000+0000", "updated": "2025-10-07T22:26:55.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 374 [https://github.com/apache/spark-kubernetes-operator/pull/374]"], "tasks": {"summarization": "Use Java 25 in GitHub Action jobs consistently - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53826 about?", "answer": "Use Java 25 in GitHub Action jobs consistently"}}}
{"issue_id": "SPARK-53825", "project": "SPARK", "title": "Use Java `MessageDigest` instead of `org.apache.commons.codec`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T21:07:50.000+0000", "updated": "2025-10-07T22:24:32.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 373 [https://github.com/apache/spark-kubernetes-operator/pull/373]"], "tasks": {"summarization": "Use Java `MessageDigest` instead of `org.apache.commons.codec` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53825 about?", "answer": "Use Java `MessageDigest` instead of `org.apache.commons.codec`"}}}
{"issue_id": "SPARK-53824", "project": "SPARK", "title": "Ban `org.apache.commons.collections4` package", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T20:32:03.000+0000", "updated": "2025-10-07T22:20:12.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 372 [https://github.com/apache/spark-kubernetes-operator/pull/372]"], "tasks": {"summarization": "Ban `org.apache.commons.collections4` package - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53824 about?", "answer": "Ban `org.apache.commons.collections4` package"}}}
{"issue_id": "SPARK-53823", "project": "SPARK", "title": "Allow list for operators", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-10-07T19:19:43.000+0000", "updated": "2025-10-24T05:11:13.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Allow list for operators - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53823 about?", "answer": "Allow list for operators"}}}
{"issue_id": "SPARK-53822", "project": "SPARK", "title": "Add python transform_with_state test case for output schemas containing composite StructType/nested StructType like ArrayType/MapType", "status": "Resolved", "priority": "Minor", "reporter": "Jason Teoh", "assignee": "Jason Teoh", "created": "2025-10-07T18:02:20.000+0000", "updated": "2025-10-10T22:13:55.000+0000", "labels": ["pull-request-available"], "description": "Existing test case `test_transform_with_state_in_pandas_composite_type` does cover composite schemas, but only within the state API.\u00a0 The output type of that test case is still strings (e.g., dicts are json dumped into a string).\r \r This ticket is to add a test case where the output of the transform_with_state operation itself contains nested types.", "comments": ["Issue resolved by pull request 52536 [https://github.com/apache/spark/pull/52536]"], "tasks": {"summarization": "Add python transform_with_state test case for output schemas containing composite StructType/nested StructType like ArrayType/MapType - Existing test case `test_transform_with_state_in_pandas_composite_type` does cover composite schemas...", "classification": "task", "qna": {"question": "What is the issue SPARK-53822 about?", "answer": "Add python transform_with_state test case for output schemas containing composite StructType/nested StructType like ArrayType/MapType"}}}
{"issue_id": "SPARK-53821", "project": "SPARK", "title": "Ban `org.apache.commons.lang3` package", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T17:38:53.000+0000", "updated": "2025-10-07T17:53:32.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 371 [https://github.com/apache/spark-kubernetes-operator/pull/371]"], "tasks": {"summarization": "Ban `org.apache.commons.lang3` package - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53821 about?", "answer": "Ban `org.apache.commons.lang3` package"}}}
{"issue_id": "SPARK-53820", "project": "SPARK", "title": "Introduce `o.a.s.k8s.operator.utils.StringUtils`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T17:01:46.000+0000", "updated": "2025-10-07T17:13:42.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 370 [https://github.com/apache/spark-kubernetes-operator/pull/370]"], "tasks": {"summarization": "Introduce `o.a.s.k8s.operator.utils.StringUtils` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53820 about?", "answer": "Introduce `o.a.s.k8s.operator.utils.StringUtils`"}}}
{"issue_id": "SPARK-53819", "project": "SPARK", "title": "Upgrade commons-lang3 to 3.19.0", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-10-07T09:58:18.000+0000", "updated": "2025-10-10T09:39:19.000+0000", "labels": ["pull-request-available"], "description": "Commons-lang3 3.12.0 contains CVE-2025-48924", "comments": [], "tasks": {"summarization": "Upgrade commons-lang3 to 3.19.0 - Commons-lang3 3.12.0 contains CVE-2025-48924...", "classification": "task", "qna": {"question": "What is the issue SPARK-53819 about?", "answer": "Upgrade commons-lang3 to 3.19.0"}}}
{"issue_id": "SPARK-53818", "project": "SPARK", "title": "Improve `SentinelManager.toString` by JEP-280 instead of `ToStringBuilder`", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T04:42:30.000+0000", "updated": "2025-10-07T06:17:22.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 369 [https://github.com/apache/spark-kubernetes-operator/pull/369]"], "tasks": {"summarization": "Improve `SentinelManager.toString` by JEP-280 instead of `ToStringBuilder` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53818 about?", "answer": "Improve `SentinelManager.toString` by JEP-280 instead of `ToStringBuilder`"}}}
{"issue_id": "SPARK-53817", "project": "SPARK", "title": "Use `o.a.s.util.Pair` instead of `o.a.commons.lang3.tuple.Pair`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T04:08:07.000+0000", "updated": "2025-10-07T06:16:45.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 368 [https://github.com/apache/spark-kubernetes-operator/pull/368]"], "tasks": {"summarization": "Use `o.a.s.util.Pair` instead of `o.a.commons.lang3.tuple.Pair` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53817 about?", "answer": "Use `o.a.s.util.Pair` instead of `o.a.commons.lang3.tuple.Pair`"}}}
{"issue_id": "SPARK-53816", "project": "SPARK", "title": "Avoid Nested ExecutionIds with Duplicate Query Plans in Streaming Queries", "status": "Open", "priority": "Major", "reporter": "Zerui Bao", "assignee": null, "created": "2025-10-07T03:28:59.000+0000", "updated": "2025-10-07T04:09:18.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Avoid Nested ExecutionIds with Duplicate Query Plans in Streaming Queries - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53816 about?", "answer": "Avoid Nested ExecutionIds with Duplicate Query Plans in Streaming Queries"}}}
{"issue_id": "SPARK-53815", "project": "SPARK", "title": "Remove `branch-0.4` from daily `publish_snapshot_*` GitHub Action jobs", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T02:10:22.000+0000", "updated": "2025-10-11T19:46:26.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 367 [https://github.com/apache/spark-kubernetes-operator/pull/367]"], "tasks": {"summarization": "Remove `branch-0.4` from daily `publish_snapshot_*` GitHub Action jobs - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53815 about?", "answer": "Remove `branch-0.4` from daily `publish_snapshot_*` GitHub Action jobs"}}}
{"issue_id": "SPARK-53814", "project": "SPARK", "title": "Use Java `List.of` instead of `Collections.(empty|singleton)List`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T01:56:25.000+0000", "updated": "2025-10-07T15:34:50.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 366 [https://github.com/apache/spark-kubernetes-operator/pull/366]"], "tasks": {"summarization": "Use Java `List.of` instead of `Collections.(empty|singleton)List` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53814 about?", "answer": "Use Java `List.of` instead of `Collections.(empty|singleton)List`"}}}
{"issue_id": "SPARK-53813", "project": "SPARK", "title": "Recover `Publish Snapshot Chart` GitHub Action Job by using GitHash", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-07T01:25:25.000+0000", "updated": "2025-10-07T01:49:34.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 365 [https://github.com/apache/spark-kubernetes-operator/pull/365]"], "tasks": {"summarization": "Recover `Publish Snapshot Chart` GitHub Action Job by using GitHash - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53813 about?", "answer": "Recover `Publish Snapshot Chart` GitHub Action Job by using GitHash"}}}
{"issue_id": "SPARK-53812", "project": "SPARK", "title": "Refactor DefineDataset and DefineFlow protos to group related properties and future-proof", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "created": "2025-10-06T23:14:22.000+0000", "updated": "2025-10-08T23:09:21.000+0000", "labels": ["pull-request-available"], "description": "The DefineDataset and DefineFlow Spark Connect protos are moshpits of properties that could be refactored into a more coherent structure:\r - In DefineDataset, there are a set of properties that are only relevant to tables (not views). They can be \r - In DefineFlow, the relation property refers to flows that write the results of a relation to a target table. In the future, we may want to introduce additional flows types that mutate the target table in different ways.", "comments": ["Issue resolved by pull request 52532 [https://github.com/apache/spark/pull/52532]"], "tasks": {"summarization": "Refactor DefineDataset and DefineFlow protos to group related properties and future-proof - The DefineDataset and DefineFlow Spark Connect protos are moshpits of properties that could be refac...", "classification": "task", "qna": {"question": "What is the issue SPARK-53812 about?", "answer": "Refactor DefineDataset and DefineFlow protos to group related properties and future-proof"}}}
{"issue_id": "SPARK-53811", "project": "SPARK", "title": "HashAggregateCodegenInterruptionSuite can be flaky", "status": "Resolved", "priority": "Major", "reporter": "Tim Armstrong", "assignee": null, "created": "2025-10-06T22:29:03.000+0000", "updated": "2025-10-06T22:31:56.000+0000", "labels": [], "description": "```- SPARK-50806: HashAggregate codegen should be interrupted on task cancellation *** FAILED *** (1 minute, 32 seconds)\r   The code passed to eventually never returned normally. Attempted 3854 times over 1.0002262633833334 minutes. Last failure message: 3 did not equal 2. (HashAggregateCodegenInterruptionSuite.scala:93)\r   org.scalatest.exceptions.TestFailedDueToTimeoutException:```", "comments": ["I can post a fix"], "tasks": {"summarization": "HashAggregateCodegenInterruptionSuite can be flaky - ```- SPARK-50806: HashAggregate codegen should be interrupted on task cancellation *** FAILED *** (1...", "classification": "task", "qna": {"question": "What is the issue SPARK-53811 about?", "answer": "HashAggregateCodegenInterruptionSuite can be flaky"}}}
{"issue_id": "SPARK-53810", "project": "SPARK", "title": "Split large TWS python tests into multiple small tests to speedup the CI", "status": "Closed", "priority": "Major", "reporter": "Huanli Wang", "assignee": "Huanli Wang", "created": "2025-10-06T21:46:40.000+0000", "updated": "2025-10-08T17:40:03.000+0000", "labels": ["pull-request-available"], "description": "title", "comments": ["PR merged here: https://github.com/apache/spark/pull/52531"], "tasks": {"summarization": "Split large TWS python tests into multiple small tests to speedup the CI - title...", "classification": "task", "qna": {"question": "What is the issue SPARK-53810 about?", "answer": "Split large TWS python tests into multiple small tests to speedup the CI"}}}
{"issue_id": "SPARK-53809", "project": "SPARK", "title": "Add canonicalization for dsv2 scan", "status": "Open", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": null, "created": "2025-10-06T18:02:55.000+0000", "updated": "2025-10-10T23:44:26.000+0000", "labels": ["pull-request-available"], "description": "Query optimization rules such as MergeScalarSubqueries check if two plans are identical by\u00a0[comparing their canonicalized form|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala#L219]. For DSv2, for physical plan, the canonicalization goes down in the child hierarchy to the BatchScanExec, which\u00a0[has a doCanonicalize function|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150]; for logical plan, the canonicalization goes down to the DataSourceV2ScanRelation, which, however, does not have a doCanonicalize function. As a result, two logical plans who are semantically identical are not identified.\r \r This PR proposes to add doCanonicalize function for DataSourceV2ScanRelation. The implementation is similar to\u00a0[the one implemented in BatchScanExec|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L150], because they are both the leafNodes of DSv2 logicalPlan and physicalPlan, respectively.", "comments": [], "tasks": {"summarization": "Add canonicalization for dsv2 scan - Query optimization rules such as MergeScalarSubqueries check if two plans are identical by\u00a0[comparin...", "classification": "task", "qna": {"question": "What is the issue SPARK-53809 about?", "answer": "Add canonicalization for dsv2 scan"}}}
{"issue_id": "SPARK-53808", "project": "SPARK", "title": "Allow to pass optional JVM args to `spark-connect-scala-client`", "status": "Resolved", "priority": "Minor", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "created": "2025-10-06T11:41:32.000+0000", "updated": "2025-10-08T13:08:44.000+0000", "labels": ["pull-request-available"], "description": "Different from other REPL tools like spark-shell, spark-connect-scala-client doesn't support optional JVM args.", "comments": ["Issue resolved by pull request 52526 [https://github.com/apache/spark/pull/52526]"], "tasks": {"summarization": "Allow to pass optional JVM args to `spark-connect-scala-client` - Different from other REPL tools like spark-shell, spark-connect-scala-client doesn't support optiona...", "classification": "task", "qna": {"question": "What is the issue SPARK-53808 about?", "answer": "Allow to pass optional JVM args to `spark-connect-scala-client`"}}}
{"issue_id": "SPARK-53807", "project": "SPARK", "title": "Fix a race condition issue between `unlock` and `releaseAllLocksForTask` in `BlockInfoManager for write locks", "status": "Open", "priority": "Major", "reporter": "Kousuke Saruta", "assignee": null, "created": "2025-10-06T07:44:50.000+0000", "updated": "2025-10-06T07:44:50.000+0000", "labels": [], "description": "When `unlock` and `releaseAllLocksForTask` try to release the same lock for a task, assertion error can happen.\r This issue can be reproduced by adding following test into `BlockInfoManagerSuite`.\r {code}\r     val blockId = TestBlockId(\"block\")\r     assert(blockInfoManager.lockNewBlockForWriting(blockId, newBlockInfo()))\r     blockInfoManager.unlock(blockId)\r \r     // Without the fix the block below almost always fails.\r     (0 to 10).foreach { task =>\r       withTaskId(task) {\r         blockInfoManager.registerTask(task)\r \r         assert(blockInfoManager.lockForWriting(blockId).isDefined)\r \r         val future = Future(blockInfoManager.unlock(blockId, Option(task)))\r         blockInfoManager.releaseAllLocksForTask(task)\r \r         ThreadUtils.awaitReady(future, 100.millis)\r       }\r     }\r   }\r {code}", "comments": [], "tasks": {"summarization": "Fix a race condition issue between `unlock` and `releaseAllLocksForTask` in `BlockInfoManager for write locks - When `unlock` and `releaseAllLocksForTask` try to release the same lock for a task, assertion error ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53807 about?", "answer": "Fix a race condition issue between `unlock` and `releaseAllLocksForTask` in `BlockInfoManager for write locks"}}}
{"issue_id": "SPARK-53806", "project": "SPARK", "title": "Allow empty input on AES decrypt to have error class", "status": "Resolved", "priority": "Major", "reporter": "Richard Chen", "assignee": "Richard Chen", "created": "2025-10-06T06:20:13.000+0000", "updated": "2025-10-08T15:21:16.000+0000", "labels": ["pull-request-available"], "description": "previously, if empty input is given to AES decrypt, it would return the error\r \r \"IllegalArgumentException: Invalid buffer arguments\"\r \r Instead, it should have a spark error message/error class", "comments": ["Issue resolved by pull request 52523 [https://github.com/apache/spark/pull/52523]"], "tasks": {"summarization": "Allow empty input on AES decrypt to have error class - previously, if empty input is given to AES decrypt, it would return the error\r \r \"IllegalArgumentExc...", "classification": "task", "qna": {"question": "What is the issue SPARK-53806 about?", "answer": "Allow empty input on AES decrypt to have error class"}}}
{"issue_id": "SPARK-53805", "project": "SPARK", "title": "Push Variant into DSv2 scan", "status": "Resolved", "priority": "Major", "reporter": "Huaxin Gao", "assignee": "Huaxin Gao", "created": "2025-10-06T03:35:53.000+0000", "updated": "2025-10-24T16:19:21.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Push Variant into DSv2 scan - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53805 about?", "answer": "Push Variant into DSv2 scan"}}}
{"issue_id": "SPARK-53804", "project": "SPARK", "title": "Support time radix sort", "status": "Resolved", "priority": "Major", "reporter": "Bruce Robbins", "assignee": "Bruce Robbins", "created": "2025-10-05T20:52:05.000+0000", "updated": "2025-10-07T16:22:58.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52520 [https://github.com/apache/spark/pull/52520]"], "tasks": {"summarization": "Support time radix sort - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53804 about?", "answer": "Support time radix sort"}}}
{"issue_id": "SPARK-53803", "project": "SPARK", "title": "Add ArimaRegression for time series forecasting in MLlib", "status": "Open", "priority": "Major", "reporter": "Anand", "assignee": null, "created": "2025-10-05T08:45:08.000+0000", "updated": "2025-10-21T02:31:25.000+0000", "labels": ["pull-request-available"], "description": "The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) algorithm for univariate time series forecasting within the Spark ML pipeline API.\r \r This work will include:\r - Implementation of ARIMA estimator with parameters (p, d, q)\r - A fitted model `ArimaRegressionModel` for prediction\r - Parameter support for (p, d, q) accessible from Scala and Python APIs\r - PySpark bindings under `pyspark.ml.regression`\r - Unit tests in Scala and Python for fit/transform, persistence, and predict\r - An example usage added to `examples/ml/ArimaRegressionExample.scala`", "comments": [], "tasks": {"summarization": "Add ArimaRegression for time series forecasting in MLlib - The new components will implement the ARIMA (AutoRegressive Integrated Moving Average) algorithm for...", "classification": "task", "qna": {"question": "What is the issue SPARK-53803 about?", "answer": "Add ArimaRegression for time series forecasting in MLlib"}}}
{"issue_id": "SPARK-53802", "project": "SPARK", "title": "Support string values for user-specified schema in SDP tables", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "created": "2025-10-04T14:50:19.000+0000", "updated": "2025-10-07T23:00:08.000+0000", "labels": ["pull-request-available"], "description": "E.g.\r \r ```\r from pyspark.sql.functions import lit\r \r @dp.materialized_view(schema=\"id LONG, name STRING\")\r def table_with_string_schema():\r \u00a0 \u00a0 return spark.range(5).withColumn(\"name\", lit(\"test\"))\r ```", "comments": ["Issue resolved by pull request 52517 [https://github.com/apache/spark/pull/52517]"], "tasks": {"summarization": "Support string values for user-specified schema in SDP tables - E.g.\r \r ```\r from pyspark.sql.functions import lit\r \r @dp.materialized_view(schema=\"id LONG, name ST...", "classification": "task", "qna": {"question": "What is the issue SPARK-53802 about?", "answer": "Support string values for user-specified schema in SDP tables"}}}
{"issue_id": "SPARK-53801", "project": "SPARK", "title": "Enable aggregation pushdown in Data Source V2 streaming", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-03T23:15:54.000+0000", "updated": "2025-10-03T23:15:54.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Enable aggregation pushdown in Data Source V2 streaming - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53801 about?", "answer": "Enable aggregation pushdown in Data Source V2 streaming"}}}
{"issue_id": "SPARK-53800", "project": "SPARK", "title": "Enable predicate pushdown in Data Source V2 streaming", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Gengliang Wang", "created": "2025-10-03T23:15:35.000+0000", "updated": "2025-10-03T23:15:35.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Enable predicate pushdown in Data Source V2 streaming - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53800 about?", "answer": "Enable predicate pushdown in Data Source V2 streaming"}}}
{"issue_id": "SPARK-53799", "project": "SPARK", "title": "Enable column pruning in Data Source V2 streaming", "status": "Open", "priority": "Major", "reporter": "Gengliang Wang", "assignee": "Gengliang Wang", "created": "2025-10-03T23:15:13.000+0000", "updated": "2025-10-10T23:03:14.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Enable column pruning in Data Source V2 streaming - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53799 about?", "answer": "Enable column pruning in Data Source V2 streaming"}}}
{"issue_id": "SPARK-53798", "project": "SPARK", "title": "Enable operator pushdown in Data Source V2 streaming", "status": "Resolved", "priority": "Major", "reporter": "Gengliang Wang", "assignee": null, "created": "2025-10-03T23:02:08.000+0000", "updated": "2025-10-10T23:05:15.000+0000", "labels": [], "description": "This umbrella tracks enabling operator pushdowns in DataSource V2 streaming, bringing it closer to batch parity. Supported pushdowns (filters, projections, aggregates, etc) will be applied directly in DSv2 readers during stream analysis to reduce scanned data and compute cost.", "comments": ["Because Spark\u2019s micro-batch streaming must materialize the MicroBatchStream during the analysis phase, and DSv2 constructs it through the sequence ScanBuilder \u2192 Scan \u2192 Scan.toMicroBatchStream, the optimizer rule V2ScanRelationPushDown needs to be applied early\u2014specifically on the analyzed plan of MicroBatchExecution.\r \r This makes the flow somewhat tricky. Moreover, since V2ScanRelationPushDown\u00a0{*}expects all predicates to be fully combined and pushed down{*}, applying it too early may cause the pushdown to fail. We also need to handle streaming deduplication properly.\r \r Until we find a cleaner solution, I\u2019ll make this Jira as on hold to unblock Spark 4.1 release"], "tasks": {"summarization": "Enable operator pushdown in Data Source V2 streaming - This umbrella tracks enabling operator pushdowns in DataSource V2 streaming, bringing it closer to b...", "classification": "task", "qna": {"question": "What is the issue SPARK-53798 about?", "answer": "Enable operator pushdown in Data Source V2 streaming"}}}
{"issue_id": "SPARK-53797", "project": "SPARK", "title": "File stream source maxBytesPerTrigger is incredibly slow", "status": "Open", "priority": "Major", "reporter": "Adam Binford", "assignee": null, "created": "2025-10-03T18:30:17.000+0000", "updated": "2025-10-03T18:37:10.000+0000", "labels": ["pull-request-available"], "description": "Using the new maxBytesPerTrigger setting for file stream sources is incredibly slow in building the batch for sources with a large number of files", "comments": [], "tasks": {"summarization": "File stream source maxBytesPerTrigger is incredibly slow - Using the new maxBytesPerTrigger setting for file stream sources is incredibly slow in building the ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53797 about?", "answer": "File stream source maxBytesPerTrigger is incredibly slow"}}}
{"issue_id": "SPARK-53796", "project": "SPARK", "title": "Add extension field to pipeline protos to support forward compatibility", "status": "Resolved", "priority": "Major", "reporter": "Yuheng Chang", "assignee": "Yuheng Chang", "created": "2025-10-03T18:23:30.000+0000", "updated": "2025-10-11T15:05:48.000+0000", "labels": ["pull-request-available"], "description": "Adding\u00a0{{google.protobuf.Any extension = 999;}}\u00a0field to\u00a0{{{}PipelineCommand{}}},\u00a0{{{}DefineDataset{}}}, and\u00a0{{DefineFlow}}\u00a0Protos to support forward-compatibility by carrying additional pipeline command types / dataset or flow's fields that are not yet defined in this version of the proto.\r \r During the planning stage, the Spark Server will resolve and dispatch command / message to the correct handler.", "comments": ["Issue resolved by pull request 52514 [https://github.com/apache/spark/pull/52514]"], "tasks": {"summarization": "Add extension field to pipeline protos to support forward compatibility - Adding\u00a0{{google.protobuf.Any extension = 999;}}\u00a0field to\u00a0{{{}PipelineCommand{}}},\u00a0{{{}DefineDataset{...", "classification": "task", "qna": {"question": "What is the issue SPARK-53796 about?", "answer": "Add extension field to pipeline protos to support forward compatibility"}}}
{"issue_id": "SPARK-53795", "project": "SPARK", "title": "Remove unused parameters in LiteralValueProtoConverter", "status": "Resolved", "priority": "Minor", "reporter": "Yihong He", "assignee": "Yihong He", "created": "2025-10-03T11:47:16.000+0000", "updated": "2025-10-08T02:04:42.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52512 [https://github.com/apache/spark/pull/52512]"], "tasks": {"summarization": "Remove unused parameters in LiteralValueProtoConverter - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53795 about?", "answer": "Remove unused parameters in LiteralValueProtoConverter"}}}
{"issue_id": "SPARK-53794", "project": "SPARK", "title": "Add support for maxVersionsToDelete while deleting old rocksdb versions", "status": "Closed", "priority": "Major", "reporter": "Anish Shrigondekar", "assignee": "Anish Shrigondekar", "created": "2025-10-02T23:08:53.000+0000", "updated": "2025-10-07T04:14:43.000+0000", "labels": ["pull-request-available"], "description": "Add support for maxVersionsToDelete while deleting old rocksdb", "comments": ["Merged here: https://github.com/apache/spark/pull/52511"], "tasks": {"summarization": "Add support for maxVersionsToDelete while deleting old rocksdb versions - Add support for maxVersionsToDelete while deleting old rocksdb...", "classification": "task", "qna": {"question": "What is the issue SPARK-53794 about?", "answer": "Add support for maxVersionsToDelete while deleting old rocksdb versions"}}}
{"issue_id": "SPARK-53793", "project": "SPARK", "title": "Use DSv2 predicate to evaluate InternalRow", "status": "Open", "priority": "Major", "reporter": "Yuchuan Huang", "assignee": null, "created": "2025-10-02T19:58:20.000+0000", "updated": "2025-10-10T17:32:54.000+0000", "labels": ["pull-request-available"], "description": "This ticket proposes to add a utility class to enable the evaluation of an InternalRow using a DSv2 predicate. This would be helpful for partition pruning, where the [runtime filters are DSv2 predicates|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsRuntimeV2Filtering.java#L66]\u00a0and the partitionValue are internalRows (for [partitionFiles in Spark|https://github.com/apache/spark/blob/65ff85a31fe8a8ea4a2ba713ba2c624709ce815a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala#L58]). In this way, partitionFiles can be pruned directly with DSv2 predicates at the scan level.\u00a0\r \r \u00a0\r \r To enable this, a DSv2 predicate will be converted to a catalyst expression, and then create an evaluator.", "comments": [], "tasks": {"summarization": "Use DSv2 predicate to evaluate InternalRow - This ticket proposes to add a utility class to enable the evaluation of an InternalRow using a DSv2 ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53793 about?", "answer": "Use DSv2 predicate to evaluate InternalRow"}}}
{"issue_id": "SPARK-53792", "project": "SPARK", "title": "Fix rocksdbPinnedBlocksMemoryUsage when bounded memory usage is enabled", "status": "Closed", "priority": "Minor", "reporter": "Zifei Feng", "assignee": null, "created": "2025-10-02T18:06:18.000+0000", "updated": "2025-10-08T17:51:40.000+0000", "labels": ["pull-request-available"], "description": "We forgot to fix this to show the correct metric when bounded memory usage is enabled. Currently, it is collecting data for each RocksDB without accounting for when all the RocksDB instances on the executor are sharing the same cache, leading to double counting.\r \r This is where we collect the cache memory metric: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L2045]\r \r We are currently using getDBProperty(\"rocksdb.block-cache-pinned-usage\") to read the pinned blocks for each DB. When the block cache is shared, this is wrong because:\r  * {*}Instance-specific vs. global stats{*}: Database properties like \"rocksdb.block-cache-pinned-usage\" report on the memory size of entries requested specifically by that DB instance.\r  * {*}Double-counting potential{*}: If you query the DB property on both instances and add them together, you could potentially double-count because a single block in the shared cache could be used by both DB instances.\r \r *Solution:* Do lrucache.getPinnedUsage ([https://github.com/facebook/rocksdb/blob/v9.8.4/java/src/main/java/org/rocksdb/Cache.java#L33C15-L33C29] in OSS RocksDB) instead, to get the actual memory size of pinned blocks in the shared cache. We are querying the cache here instead of the DB.\r \r To estimate the pinnedBlocks used by a single DB instance, we can divide lrucache.getPinnedUsage() by num_rocksdb_instances_sharing_the_cache. We already do a similar thing for memoryUsage. See: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBMemoryManager.scala#L124]\r \r *Original fix:* OSS Spark PR:[ https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2|https://github.com/apache/spark/commit/35c299a1e3e373e20ae45d7604df51c83ff1dbe2]", "comments": ["PR merged here: https://github.com/apache/spark/pull/52527"], "tasks": {"summarization": "Fix rocksdbPinnedBlocksMemoryUsage when bounded memory usage is enabled - We forgot to fix this to show the correct metric when bounded memory usage is enabled. Currently, it...", "classification": "task", "qna": {"question": "What is the issue SPARK-53792 about?", "answer": "Fix rocksdbPinnedBlocksMemoryUsage when bounded memory usage is enabled"}}}
{"issue_id": "SPARK-53791", "project": "SPARK", "title": "Make the rename operations multi-threaded.", "status": "Open", "priority": "Major", "reporter": "duanshilong", "assignee": null, "created": "2025-10-02T15:15:16.000+0000", "updated": "2025-10-15T03:05:46.000+0000", "labels": ["pull-request-available"], "description": "For example, during the INSERT OVERWRITE DIRECTORY operation, each rename operation triggers an RPC request. Therefore, when there are too many files, it can be time-consuming.\r Converting the serial rename operations to multi-threaded operations can save job execution time.", "comments": [], "tasks": {"summarization": "Make the rename operations multi-threaded. - For example, during the INSERT OVERWRITE DIRECTORY operation, each rename operation triggers an RPC ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53791 about?", "answer": "Make the rename operations multi-threaded."}}}
{"issue_id": "SPARK-53790", "project": "SPARK", "title": "arrow encoder is unable to handle Beans with instantiated generics", "status": "Open", "priority": "Major", "reporter": "Steven Aerts", "assignee": null, "created": "2025-10-02T11:55:46.000+0000", "updated": "2025-10-03T14:32:43.000+0000", "labels": ["pull-request-available"], "description": "We found a bug in the arrow encoder/decoder for spark connect which is unable to handle beans with instantiated generics.\u00a0 Like:\r ```\r class JavaBeanWithGenerics[T] {\r \u00a0 @BeanProperty var value: T = _\r }\r class JavaBeanWithGenericsWrapper {\r \u00a0 @BeanProperty var value: JavaBeanWithGenerics[String] = _\r }\r ...\r val encoder = JavaTypeInference.encoderFor(classOf[JavaBeanWithGenericsWrapper])\r ```\r \r Which results in the following error when the above encoder is used in connect:\r ```\r java.lang.NoSuchMethodException: no such method: JavaBeanWithGenerics.getValue()String/invokeVirtual\r at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:915)\r at java.base/java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:994)\r at java.base/java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:3750)\r at java.base/java.lang.invoke.MethodHandles$Lookup.findVirtual(MethodHandles.java:2767)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19(ArrowSerializer.scala:495)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$serializerFor$19$adapted(ArrowSerializer.scala:491)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:549)\r at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.$anonfun$structSerializerFor$1(ArrowSerializer.scala:548)\r at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)\r at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.structSerializerFor(ArrowSerializer.scala:547)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:491)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serializerFor(ArrowSerializer.scala:237)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer.<init>(ArrowSerializer.scala:62)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$$anon$1.<init>(ArrowSerializer.scala:161)\r at org.apache.spark.sql.connect.client.arrow.ArrowSerializer$.serialize(ArrowSerializer.scala:160)\r ```\r \r We have have a possible patch for this issue.", "comments": [], "tasks": {"summarization": "arrow encoder is unable to handle Beans with instantiated generics - We found a bug in the arrow encoder/decoder for spark connect which is unable to handle beans with i...", "classification": "task", "qna": {"question": "What is the issue SPARK-53790 about?", "answer": "arrow encoder is unable to handle Beans with instantiated generics"}}}
{"issue_id": "SPARK-53789", "project": "SPARK", "title": "Canonicalize error condition CANNOT_MODIFY_STATIC_CONFIG", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-02T06:51:41.000+0000", "updated": "2025-10-15T04:34:57.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52506 [https://github.com/apache/spark/pull/52506]"], "tasks": {"summarization": "Canonicalize error condition CANNOT_MODIFY_STATIC_CONFIG - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53789 about?", "answer": "Canonicalize error condition CANNOT_MODIFY_STATIC_CONFIG"}}}
{"issue_id": "SPARK-53788", "project": "SPARK", "title": "Move VersionUtils to common utils", "status": "Resolved", "priority": "Minor", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-10-02T06:29:37.000+0000", "updated": "2025-10-07T19:04:06.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52505 [https://github.com/apache/spark/pull/52505]"], "tasks": {"summarization": "Move VersionUtils to common utils - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53788 about?", "answer": "Move VersionUtils to common utils"}}}
{"issue_id": "SPARK-53787", "project": "SPARK", "title": "Upgrade Spark to 4.1.0-preview2", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-02T06:26:42.000+0000", "updated": "2025-10-02T06:38:35.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 364 [https://github.com/apache/spark-kubernetes-operator/pull/364]"], "tasks": {"summarization": "Upgrade Spark to 4.1.0-preview2 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53787 about?", "answer": "Upgrade Spark to 4.1.0-preview2"}}}
{"issue_id": "SPARK-53786", "project": "SPARK", "title": "Default value should not conflict with special column name", "status": "Resolved", "priority": "Major", "reporter": "Szehon Ho", "assignee": "Szehon Ho", "created": "2025-10-02T06:06:46.000+0000", "updated": "2025-10-10T11:03:40.000+0000", "labels": ["pull-request-available"], "description": "The following query:\r \r CREATE TABLE t (current_timestamp DEFAULT current_timestamp)\r \r \u00a0\r \r fails after SPARK-51786.\u00a0 This is because for making default value DSV2 expression, it uses the main analyzer to analyze the default value.\u00a0 However, default value should not know about other columns when trying to analyze.", "comments": ["Issue resolved by pull request 52504 [https://github.com/apache/spark/pull/52504]"], "tasks": {"summarization": "Default value should not conflict with special column name - The following query:\r \r CREATE TABLE t (current_timestamp DEFAULT current_timestamp)\r \r \u00a0\r \r fails a...", "classification": "task", "qna": {"question": "What is the issue SPARK-53786 about?", "answer": "Default value should not conflict with special column name"}}}
{"issue_id": "SPARK-53785", "project": "SPARK", "title": "Memory Source for RTM", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-10-02T05:59:55.000+0000", "updated": "2025-10-24T05:11:23.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52502 [https://github.com/apache/spark/pull/52502]"], "tasks": {"summarization": "Memory Source for RTM - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53785 about?", "answer": "Memory Source for RTM"}}}
{"issue_id": "SPARK-53784", "project": "SPARK", "title": "Additional Source APIs needed to support RTM execution", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-10-02T04:57:51.000+0000", "updated": "2025-10-24T05:11:31.000+0000", "labels": ["pull-request-available"], "description": "Currently in Structured Streaming, start and end offsets are determined at the driver prior to running the micro-batch. \u00a0In real-time mode, end offsets are not known apriori. They are communicated to the driver later by the executors at the end of microbatch that runs for a fixed amount of time.\u00a0 Thus, we need to add additional APIs in the source to support this kind of behavior.\r \r The lifecycle of the new API is the following.\r \r Driver side:\r  # prepareForRealTimeMode\r  ** Called during logical planning to inform the source if it's in real time mode\r  # planInputPartitions\r  ** The driver plans partitions via planPartitions but only a starting offset is provided (Compared to existing execution modes that require planPartitions to provide both a starting and end offset)\r  # mergeOffsets\r  ** Merge partitioned offsets coming from partitions/tasks to a single global offset.\r \r \u00a0\r \r Task side:\r  # nextWithTimeout\r  ** Alternative function to be called than next(), that proceed to the next record. The different from next() is that, if there is no more records, the call needs to keep waiting until the timeout\r  # \r getOffset\r  ** Get the offset of the next record, or the start offset if no records have been read. The execution engine will call this method along with get() to keep track of the current offset. When a task ends, the offset in each partition will be passed back to the driver. They will be used as the start offsets of the next batch.", "comments": ["Issue resolved by pull request 52501 [https://github.com/apache/spark/pull/52501]"], "tasks": {"summarization": "Additional Source APIs needed to support RTM execution - Currently in Structured Streaming, start and end offsets are determined at the driver prior to runni...", "classification": "task", "qna": {"question": "What is the issue SPARK-53784 about?", "answer": "Additional Source APIs needed to support RTM execution"}}}
{"issue_id": "SPARK-53783", "project": "SPARK", "title": "Use `log4j2` instead of `log4j`", "status": "Resolved", "priority": "Critical", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-02T04:11:15.000+0000", "updated": "2025-10-02T04:47:41.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 363 [https://github.com/apache/spark-kubernetes-operator/pull/363]"], "tasks": {"summarization": "Use `log4j2` instead of `log4j` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53783 about?", "answer": "Use `log4j2` instead of `log4j`"}}}
{"issue_id": "SPARK-53782", "project": "SPARK", "title": "Use `slf4j-api` as a direct dependency", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-02T03:26:10.000+0000", "updated": "2025-10-02T03:42:09.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 362 [https://github.com/apache/spark-kubernetes-operator/pull/362]"], "tasks": {"summarization": "Use `slf4j-api` as a direct dependency - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53782 about?", "answer": "Use `slf4j-api` as a direct dependency"}}}
{"issue_id": "SPARK-53781", "project": "SPARK", "title": "Exclude Spark's transitive dependencies consistently across modules", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-02T00:17:20.000+0000", "updated": "2025-10-02T06:23:54.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 361 [https://github.com/apache/spark-kubernetes-operator/pull/361]"], "tasks": {"summarization": "Exclude Spark's transitive dependencies consistently across modules - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53781 about?", "answer": "Exclude Spark's transitive dependencies consistently across modules"}}}
{"issue_id": "SPARK-53780", "project": "SPARK", "title": "`spark-submission-worker` should depend on `io.fabric8` directly", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-01T22:39:09.000+0000", "updated": "2025-10-02T00:14:26.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 360 [https://github.com/apache/spark-kubernetes-operator/pull/360]"], "tasks": {"summarization": "`spark-submission-worker` should depend on `io.fabric8` directly - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53780 about?", "answer": "`spark-submission-worker` should depend on `io.fabric8` directly"}}}
{"issue_id": "SPARK-53779", "project": "SPARK", "title": "Implement transform in column API", "status": "Resolved", "priority": "Major", "reporter": "Yicong Huang", "assignee": "Yicong Huang", "created": "2025-10-01T21:42:17.000+0000", "updated": "2025-10-10T10:58:56.000+0000", "labels": ["pull-request-available"], "description": "Proposal to introduce a transform API for Column in Spark, inspired by Scala\u2019s pipe operator and SQL pipeline syntax. This would allow chaining transformations in a pipeline style, improving readability compared to nested function calls.\r \r \u00a0\r \r *Motivation*\r  * Scala\u2019s pipe API and SQL pipeline syntax provide a cleaner, pipeline-oriented style.\r \r  * Current nested function invocations (e.g., f2(f1(col))) are less readable than a chained style (col.transform(f1).transform(f2)).\r \r  * AI code generators also tend to produce pipeline style code more cleanly.\r \r  * This aligns with the existing DataFrame API pipeline style (df.transform(f) \u2192 DataFrame).", "comments": ["I will work on this.", "PR opened at https://github.com/apache/spark/pull/52537", "Issue resolved by pull request 52537 [https://github.com/apache/spark/pull/52537]"], "tasks": {"summarization": "Implement transform in column API - Proposal to introduce a transform API for Column in Spark, inspired by Scala\u2019s pipe operator and SQL...", "classification": "task", "qna": {"question": "What is the issue SPARK-53779 about?", "answer": "Implement transform in column API"}}}
{"issue_id": "SPARK-53778", "project": "SPARK", "title": "Use JDK25 Gradle Image as builder images", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-01T20:37:40.000+0000", "updated": "2025-10-01T20:54:59.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 359 [https://github.com/apache/spark-kubernetes-operator/pull/359]"], "tasks": {"summarization": "Use JDK25 Gradle Image as builder images - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53778 about?", "answer": "Use JDK25 Gradle Image as builder images"}}}
{"issue_id": "SPARK-53777", "project": "SPARK", "title": "Update `Spark Connect`-generated `Swift` source code with 4.1.0-preview2", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-01T19:09:16.000+0000", "updated": "2025-10-01T20:07:15.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 250 [https://github.com/apache/spark-connect-swift/pull/250]"], "tasks": {"summarization": "Update `Spark Connect`-generated `Swift` source code with 4.1.0-preview2 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53777 about?", "answer": "Update `Spark Connect`-generated `Swift` source code with 4.1.0-preview2"}}}
{"issue_id": "SPARK-53776", "project": "SPARK", "title": "Support Spark Context Manager Support in Connect Mode", "status": "Open", "priority": "Major", "reporter": "Shujing Yang", "assignee": null, "created": "2025-10-01T17:33:42.000+0000", "updated": "2025-10-08T01:06:41.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Support Spark Context Manager Support in Connect Mode - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53776 about?", "answer": "Support Spark Context Manager Support in Connect Mode"}}}
{"issue_id": "SPARK-53775", "project": "SPARK", "title": "Spark 4 on k3s NoSuchMethodError in io.netty.util.internal.PlatformDependent", "status": "Open", "priority": "Major", "reporter": "rami", "assignee": null, "created": "2025-10-01T09:32:46.000+0000", "updated": "2025-10-01T09:32:46.000+0000", "labels": [], "description": "Hello,\r \r I see the exception below on k3s but *not* on other Kubernetes flavors.\r \r Looks like k3s sets up pods in such a way that triggers a dependency bug that is otherwise not visible.\r \r I can provide more details if required.\r {noformat}\r Exception in thread \"main\" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred.\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:109)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:102)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:44)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1155)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:98)\r \u00a0 \u00a0 at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:154)\r \u00a0 \u00a0 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6(KubernetesClientApplication.scala:258)\r \u00a0 \u00a0 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6$adapted(KubernetesClientApplication.scala:252)\r \u00a0 \u00a0 at org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:48)\r \u00a0 \u00a0 at org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\r \u00a0 \u00a0 at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:99)\r \u00a0 \u00a0 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:252)\r \u00a0 \u00a0 at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:225)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r \u00a0 \u00a0 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r Caused by: java.io.IOException: Failed to create SSL connection\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:504)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:524)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:340)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:754)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:98)\r \u00a0 \u00a0 at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)\r \u00a0 \u00a0 ... 17 more\r Caused by: javax.net.ssl.SSLHandshakeException: Failed to create SSL connection\r ...\u00a0\r Caused by: javax.net.ssl.SSLException: failure when writing TLS control frames\r \u00a0 \u00a0 at io.netty.handler.ssl.SslHandler.setHandshakeFailureTransportFailure(SslHandler.java:2050)\r \u00a0 \u00a0 ... 30 more\r Caused by: java.lang.NoSuchMethodError: 'java.util.Queue io.netty.util.internal.PlatformDependent.newFixedMpscUnpaddedQueue(int)'\r \u00a0 \u00a0 at io.netty.buffer.PoolThreadCache$MemoryRegionCache.<init>(PoolThreadCache.java:337)\r \u00a0 \u00a0 at io.netty.buffer.PoolThreadCache$SubPageMemoryRegionCache.<init>(PoolThreadCache.java:302)\r \u00a0 \u00a0 at io.netty.buffer.PoolThreadCache.createSubPageCaches(PoolThreadCache.java:113)\r \u00a0 \u00a0 at io.netty.buffer.PoolThreadCache.<init>(PoolThreadCache.java:77)\r \u00a0 \u00a0 at io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache.initialValue(PooledByteBufAllocator.java:541)\r \u00a0 \u00a0 at io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache.initialValue(PooledByteBufAllocator.java:518)\r \u00a0 \u00a0 at io.netty.util.concurrent.FastThreadLocal.initialize(FastThreadLocal.java:177)\r \u00a0 \u00a0 at io.netty.util.concurrent.FastThreadLocal.get(FastThreadLocal.java:142)\r \u00a0 \u00a0 at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:398)\r \u00a0 \u00a0 at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\r \u00a0 \u00a0 at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:179)\r \u00a0 \u00a0 at io.vertx.core.buffer.impl.PartialPooledByteBufAllocator.directBuffer(PartialPooledByteBufAllocator.java:84)\r \u00a0 \u00a0 at io.netty.channel.nio.AbstractNioChannel.newDirectBuffer(AbstractNioChannel.java:447)\r \u00a0 \u00a0 at io.netty.channel.nio.AbstractNioByteChannel.filterOutboundMessage(AbstractNioByteChannel.java:278)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:866)\r \u00a0 \u00a0 at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1314)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:889)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:875)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:984)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:868)\r \u00a0 \u00a0 at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:863)\r \u00a0 \u00a0 ... 21 more{noformat}", "comments": [], "tasks": {"summarization": "Spark 4 on k3s NoSuchMethodError in io.netty.util.internal.PlatformDependent - Hello,\r \r I see the exception below on k3s but *not* on other Kubernetes flavors.\r \r Looks like k3s ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53775 about?", "answer": "Spark 4 on k3s NoSuchMethodError in io.netty.util.internal.PlatformDependent"}}}
{"issue_id": "SPARK-53774", "project": "SPARK", "title": "Update Spark Connect-generated Swift code with protoc-gen-grpc-swift-2 v2.1.1", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-01T05:14:35.000+0000", "updated": "2025-10-01T14:59:43.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 249 [https://github.com/apache/spark-connect-swift/pull/249]"], "tasks": {"summarization": "Update Spark Connect-generated Swift code with protoc-gen-grpc-swift-2 v2.1.1 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53774 about?", "answer": "Update Spark Connect-generated Swift code with protoc-gen-grpc-swift-2 v2.1.1"}}}
{"issue_id": "SPARK-53773", "project": "SPARK", "title": "Recover alphabetic ordering of rules in `RuleIdCollection`", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-10-01T03:27:20.000+0000", "updated": "2025-10-01T19:19:38.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52495 [https://github.com/apache/spark/pull/52495]"], "tasks": {"summarization": "Recover alphabetic ordering of rules in `RuleIdCollection` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53773 about?", "answer": "Recover alphabetic ordering of rules in `RuleIdCollection`"}}}
{"issue_id": "SPARK-53772", "project": "SPARK", "title": "Refactor PythonPipelineSuite on top of APITest", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": null, "created": "2025-10-01T02:37:05.000+0000", "updated": "2025-10-01T02:37:05.000+0000", "labels": [], "description": "The tests in PythonPipelineSuite are essentially API tests, meaning that they could be written to extend the APITest class. That would allow them to rely on the APITest infrastructure and be tested in a higher fidelity way.\r \r \u00a0\r \r Doing this will Spark Connect protocol additions to allow the client to access metadata about the dataflow graph.", "comments": [], "tasks": {"summarization": "Refactor PythonPipelineSuite on top of APITest - The tests in PythonPipelineSuite are essentially API tests, meaning that they could be written to ex...", "classification": "task", "qna": {"question": "What is the issue SPARK-53772 about?", "answer": "Refactor PythonPipelineSuite on top of APITest"}}}
{"issue_id": "SPARK-53771", "project": "SPARK", "title": "Add config to use LargeListType in Arrow Schema", "status": "Open", "priority": "Major", "reporter": "Yicong Huang", "assignee": null, "created": "2025-10-01T00:53:38.000+0000", "updated": "2025-10-14T02:01:48.000+0000", "labels": ["pull-request-available"], "description": "We want to provide a configuration option that allows users to use `LargeListType` instead of `ListType` for all fields in an Arrow schema.", "comments": ["I will work on this.", "PR opened https://github.com/apache/spark/pull/52498"], "tasks": {"summarization": "Add config to use LargeListType in Arrow Schema - We want to provide a configuration option that allows users to use `LargeListType` instead of `ListT...", "classification": "task", "qna": {"question": "What is the issue SPARK-53771 about?", "answer": "Add config to use LargeListType in Arrow Schema"}}}
{"issue_id": "SPARK-53770", "project": "SPARK", "title": "Upgrade `mockito` to 5.20.0", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T22:00:38.000+0000", "updated": "2025-10-01T01:52:33.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 358 [https://github.com/apache/spark-kubernetes-operator/pull/358]"], "tasks": {"summarization": "Upgrade `mockito` to 5.20.0 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53770 about?", "answer": "Upgrade `mockito` to 5.20.0"}}}
{"issue_id": "SPARK-53769", "project": "SPARK", "title": "Enable `Annotation Processing` during Java compilation", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T21:43:33.000+0000", "updated": "2025-10-01T01:51:57.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 357 [https://github.com/apache/spark-kubernetes-operator/pull/357]"], "tasks": {"summarization": "Enable `Annotation Processing` during Java compilation - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53769 about?", "answer": "Enable `Annotation Processing` during Java compilation"}}}
{"issue_id": "SPARK-53768", "project": "SPARK", "title": "Upgrade `lombok` to 1.18.42", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T21:30:14.000+0000", "updated": "2025-10-01T01:51:21.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 356 [https://github.com/apache/spark-kubernetes-operator/pull/356]"], "tasks": {"summarization": "Upgrade `lombok` to 1.18.42 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53768 about?", "answer": "Upgrade `lombok` to 1.18.42"}}}
{"issue_id": "SPARK-53767", "project": "SPARK", "title": "Upgrade `Dropwizard` metrics to 4.2.33", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T21:14:06.000+0000", "updated": "2025-10-01T01:50:39.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 355 [https://github.com/apache/spark-kubernetes-operator/pull/355]"], "tasks": {"summarization": "Upgrade `Dropwizard` metrics to 4.2.33 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53767 about?", "answer": "Upgrade `Dropwizard` metrics to 4.2.33"}}}
{"issue_id": "SPARK-53766", "project": "SPARK", "title": "Document improvements to EXECUTE IMMEDIATE", "status": "Resolved", "priority": "Major", "reporter": "Serge Rielau", "assignee": "Serge Rielau", "created": "2025-09-30T18:53:06.000+0000", "updated": "2025-09-30T20:35:27.000+0000", "labels": ["pull-request-available"], "description": "Execute immediate has been improved to support expressions for USINg and SQL statement. Therefore the docs need to be updated (and generally be improved).", "comments": ["Issue resolved by pull request 52494 [https://github.com/apache/spark/pull/52494]"], "tasks": {"summarization": "Document improvements to EXECUTE IMMEDIATE - Execute immediate has been improved to support expressions for USINg and SQL statement. Therefore th...", "classification": "task", "qna": {"question": "What is the issue SPARK-53766 about?", "answer": "Document improvements to EXECUTE IMMEDIATE"}}}
{"issue_id": "SPARK-53765", "project": "SPARK", "title": "Remove Daily `integration-test-linux` GitHub Action job", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T18:30:59.000+0000", "updated": "2025-09-30T19:12:35.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 248 [https://github.com/apache/spark-connect-swift/pull/248]"], "tasks": {"summarization": "Remove Daily `integration-test-linux` GitHub Action job - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53765 about?", "answer": "Remove Daily `integration-test-linux` GitHub Action job"}}}
{"issue_id": "SPARK-53764", "project": "SPARK", "title": "Collapse nested get_json_object", "status": "Open", "priority": "Major", "reporter": "Wechar", "assignee": null, "created": "2025-09-30T17:59:17.000+0000", "updated": "2025-10-09T15:24:50.000+0000", "labels": ["pull-request-available"], "description": "User may use intermediate table and get nested {{get_json_object}} expressions, which could be collapsed to one {{get_json_object}} expression. After the collapse, Spark need traverse the path only once, which can improve the performance.", "comments": [], "tasks": {"summarization": "Collapse nested get_json_object - User may use intermediate table and get nested {{get_json_object}} expressions, which could be colla...", "classification": "task", "qna": {"question": "What is the issue SPARK-53764 about?", "answer": "Collapse nested get_json_object"}}}
{"issue_id": "SPARK-53763", "project": "SPARK", "title": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview2`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-30T17:56:03.000+0000", "updated": "2025-09-30T19:11:51.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 247 [https://github.com/apache/spark-connect-swift/pull/247]"], "tasks": {"summarization": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview2` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53763 about?", "answer": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview2`"}}}
{"issue_id": "SPARK-53762", "project": "SPARK", "title": "Add date and time conversions simplifier rule to optimizer", "status": "Resolved", "priority": "Major", "reporter": "Peter Toth", "assignee": "Peter Toth", "created": "2025-09-30T17:02:04.000+0000", "updated": "2025-10-15T07:12:16.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52493 [https://github.com/apache/spark/pull/52493]"], "tasks": {"summarization": "Add date and time conversions simplifier rule to optimizer - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53762 about?", "answer": "Add date and time conversions simplifier rule to optimizer"}}}
{"issue_id": "SPARK-53761", "project": "SPARK", "title": "TransportResponseHandler.channelInactive() should call super method.", "status": "Open", "priority": "Major", "reporter": "Tsz-wo Sze", "assignee": null, "created": "2025-09-30T16:26:10.000+0000", "updated": "2025-09-30T16:26:32.000+0000", "labels": [], "description": "According to Google AI, [TransportResponseHandler.channelInactive(..)|https://github.com/apache/spark/blob/46ac78ea367cfa9a7acc04482770aaca33f5a575/common/network-common/src/main/java/org/apache/spark/network/client/TransportResponseHandler.java#L144] should call super.channelInactive(ctx):\r {code}\r             // Important: Call super.channelInactive(ctx) to propagate the event\r             // to the next handler in the ChannelPipeline.\r             super.channelInactive(ctx);\r {code}", "comments": ["Here is the screenshot:\r  !screenshot-1.png!"], "tasks": {"summarization": "TransportResponseHandler.channelInactive() should call super method. - According to Google AI, [TransportResponseHandler.channelInactive(..)|https://github.com/apache/spar...", "classification": "task", "qna": {"question": "What is the issue SPARK-53761 about?", "answer": "TransportResponseHandler.channelInactive() should call super method."}}}
{"issue_id": "SPARK-53760", "project": "SPARK", "title": "Introduce Geography and Geometry logical types", "status": "Resolved", "priority": "Major", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "created": "2025-09-30T15:17:34.000+0000", "updated": "2025-10-18T03:38:30.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["WIP: [https://github.com/apache/spark/pull/52491|https://github.com/apache/spark/pull/52491.]. cc [~mkaravel] [~cloud_fan]", "Issue resolved by pull request 52491 [https://github.com/apache/spark/pull/52491]"], "tasks": {"summarization": "Introduce Geography and Geometry logical types - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53760 about?", "answer": "Introduce Geography and Geometry logical types"}}}
{"issue_id": "SPARK-53759", "project": "SPARK", "title": "PySpark crashes with Python 3.12+ on Windows", "status": "Open", "priority": "Critical", "reporter": "Max Payson", "assignee": null, "created": "2025-09-30T11:28:34.000+0000", "updated": "2025-10-08T12:56:38.000+0000", "labels": [], "description": "Python 3.12+ crashes locally on Windows when using the `createDataFrame` API. All dataframe creation methods in the [Quickstart: Dataframe|https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#DataFrame-Creation] seem to crash but other operations work as expected.\r \r Reproduction:\r {code:java}\r import os\r import sys\r from pyspark.sql import SparkSession\r \r os.environ[\"PYSPARK_PYTHON\"] = sys.executable\r spark = SparkSession.builder.getOrCreate()\r \r df = spark.createDataFrame([(1,), (2,)], [\"myint\"])\r df.show() {code}\r \u00a0\r \r Stack trace. This is with \"spark.python.worker.faulthandler.enabled\" enabled, but the stack trace is the same with it disabled:\r {code:java}\r Traceback (most recent call last):\r File \"<your_script>.py\", line 10, in <module>\r   df.show()\r File \"<pyspark>/sql/classic/dataframe.py\", line 285, in show\r   print(self._show_string(n, truncate, vertical))\r File \"<pyspark>/sql/classic/dataframe.py\", line 303, in _show_string\r   return self._jdf.showString(n, 20, vertical)\r File \"<py4j>/java_gateway.py\", line 1362, in __call__\r   return_value = get_return_value(\r     answer, self.gateway_client, self.target_id, self.name)\r File \"<pyspark>/errors/exceptions/captured.py\", line 282, in deco\r   return f(*a, **kw)\r File \"<py4j>/protocol.py\", line 327, in get_return_value\r   raise Py4JJavaError(\r     \"An error occurred while calling {0}{1}{2}.\\n\".\r     format(target_id, \".\", name), value)\r py4j.protocol.Py4JJavaError: An error occurred while calling o48.showString.:\r org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:945)\r at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r at org.apache.spark.scheduler.Task.run(Task.scala:147)\r at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r at java.base/java.lang.Thread.run(Thread.java:840)Caused by: java.io.EOFException\r at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)\r at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r ... 26 more{code}\r \r Note, I originally posted in the Python 3.13 issue but it seemed better to create a new issue since that one was closed. Please let us know if our team can help debug this further, it seems relatively low level. Thank you!", "comments": ["I don't know if this is related to Windows 11, somehow? I have the exact same issue on pyspark versions 3.5.2 and 4.0.0, but I know that it used to work for 3.5.2 on my previous machine running Windows 10."], "tasks": {"summarization": "PySpark crashes with Python 3.12+ on Windows - Python 3.12+ crashes locally on Windows when using the `createDataFrame` API. All dataframe creation...", "classification": "task", "qna": {"question": "What is the issue SPARK-53759 about?", "answer": "PySpark crashes with Python 3.12+ on Windows"}}}
{"issue_id": "SPARK-53758", "project": "SPARK", "title": "EXIT handler not exiting properly if triggered from another EXIT handler", "status": "Open", "priority": "Major", "reporter": "Teodor", "assignee": null, "created": "2025-09-30T11:05:37.000+0000", "updated": "2025-09-30T14:30:33.000+0000", "labels": [], "description": "An unexpected behavior happens when an EXIT handler inside of an outer scope gets triggered by an exception inside an EXIT handler thats inside an inner nested scope: instead of leaving the outer scope after finishing all the exception handling, execution only leaves the inner scope.\r \r Example:\r {code:java}\r BEGIN\r   DECLARE VARIABLE flag INT = -1;\r   l1: BEGIN\r     DECLARE EXIT HANDLER FOR UNRESOLVED_COLUMN.WITHOUT_SUGGESTION\r     BEGIN\r       SELECT flag;\r       SET flag = 2;\r     END;\r     l2: BEGIN\r       DECLARE EXIT HANDLER FOR DIVIDE_BY_ZERO\r       BEGIN\r         SELECT flag;\r         SET flag = 1;\r         select X; -- select non existing variable\r         SELECT 2;\r       END;\r       SELECT 5;\r       SELECT 1/0; -- divide by zero\r       SELECT 6;\r     END l2;\r     SELECT 3, flag;\r   END l1;\r END {code}\r \u00a0\r \r Returns 3, 2 at the end, but it shouldn't reach that part of the code.", "comments": ["The issue is originating from `SqlScriptingExecution#getNextStatement`, more specifically from this piece of code:\r {code:java}\r \u00a0 \u00a0 \u00a0 // If the last frame is a handler, set leave statement to be the next one in the\r \u00a0 \u00a0 \u00a0 // innermost scope that should be exited.\r \u00a0 \u00a0 \u00a0 if (lastFrame.frameType == SqlScriptingFrameType.HANDLER && context.frames.nonEmpty) {\r \u00a0 \u00a0 \u00a0 \u00a0 // Remove the scope if handler is executed.\r \u00a0 \u00a0 \u00a0 \u00a0 if (context.firstHandlerScopeLabel.isDefined\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 && lastFrame.scopeLabel.get == context.firstHandlerScopeLabel.get) {\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 context.firstHandlerScopeLabel = None\r \u00a0 \u00a0 \u00a0 \u00a0 }\r \u00a0 \u00a0 \u00a0 \u00a0 // Inject leave statement into the execution plan of the last frame.\r \u00a0 \u00a0 \u00a0 \u00a0 injectLeaveStatement(context.frames.last.executionPlan, lastFrame.scopeLabel.get)\r \u00a0 \u00a0 \u00a0 }{code}\r \u00a0\r \r The handlers are properly found in `findHandler` and properly added to the call stack. In this case, we will have call stack looking like SQL_SCRIPT -> HANDLER -> HANDLER, which is fine. However, when the last handler is exited, we are injecting LEAVE statements in SQL_SCRIPT and the other HANDLER. Now, this would be fine, except with the current implementation, when the other HANDLER is exited, the LEAVE statement would get overridden with different label."], "tasks": {"summarization": "EXIT handler not exiting properly if triggered from another EXIT handler - An unexpected behavior happens when an EXIT handler inside of an outer scope gets triggered by an ex...", "classification": "task", "qna": {"question": "What is the issue SPARK-53758 about?", "answer": "EXIT handler not exiting properly if triggered from another EXIT handler"}}}
{"issue_id": "SPARK-53757", "project": "SPARK", "title": "Upgrade Jetty to 12.0.12", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-09-30T09:16:32.000+0000", "updated": "2025-10-21T13:14:59.000+0000", "labels": ["pull-request-available"], "description": "Jetty-http 11.0.25 contains CVE-2024-6763", "comments": [], "tasks": {"summarization": "Upgrade Jetty to 12.0.12 - Jetty-http 11.0.25 contains CVE-2024-6763...", "classification": "task", "qna": {"question": "What is the issue SPARK-53757 about?", "answer": "Upgrade Jetty to 12.0.12"}}}
{"issue_id": "SPARK-53756", "project": "SPARK", "title": "Flaky test: FileStreamSinkSuite.\"cleanup complete but invalid output for aborted job\"", "status": "Open", "priority": "Major", "reporter": "Zhen Wang", "assignee": null, "created": "2025-09-30T05:18:39.000+0000", "updated": "2025-09-30T05:37:43.000+0000", "labels": ["pull-request-available"], "description": "I found an the flaky test when running spark sql test cases with datafusion-comet. https://github.com/apache/datafusion-comet/actions/runs/18110382622/job/51535109725\r \r error details:\r {code:java}\r [info] - cleanup complete but invalid output for aborted job *** FAILED *** (430 milliseconds)\r [info]   java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /__w/datafusion-comet/datafusion-comet/apache-spark/target/tmp/spark-2cf998bb-fd3c-4621-88d8-05e9decad882/output @#output/.part-00009-cb134e84-5d4a-42d2-a342-b5edc52776ce-c000.snappy.parquet.crc\r [info]   at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:87)\r [info]   at java.base/java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:103)\r [info]   at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1855)\r [info]   at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:292)\r [info]   at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206)\r [info]   at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169)\r [info]   at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:298)\r [info]   at java.base/java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681)\r [info]   at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\r [info]   at scala.collection.Iterator$$anon$6.hasNext(Iterator.scala:477)\r [info]   at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r [info]   at scala.collection.mutable.Growable.addAll(Growable.scala:61)\r [info]   at scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r [info]   at scala.collection.immutable.SetBuilderImpl.addAll(Set.scala:405)\r [info]   at scala.collection.immutable.Set$.from(Set.scala:362)\r [info]   at scala.collection.IterableOnceOps.toSet(IterableOnce.scala:1469)\r [info]   at scala.collection.IterableOnceOps.toSet$(IterableOnce.scala:1469)\r [info]   at scala.collection.AbstractIterator.toSet(Iterator.scala:1306)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$52(FileStreamSinkSuite.scala:538)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$52$adapted(FileStreamSinkSuite.scala:505)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1(SQLTestUtils.scala:83)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1$adapted(SQLTestUtils.scala:82)\r [info]   at org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:245)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.org$apache$spark$sql$test$SQLTestUtils$$super$withTempDir(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir(SQLTestUtils.scala:82)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir$(SQLTestUtils.scala:81)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.withTempDir(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$51(FileStreamSinkSuite.scala:505)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:56)\r [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:301)\r [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:297)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.withSQLConf(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$50(FileStreamSinkSuite.scala:505)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.apache.spark.sql.catalyst.util.package$.quietly(package.scala:43)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$testQuietly$1(SQLTestUtils.scala:119)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r [info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r [info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r [info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r [info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r [info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r [info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r [info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r [info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)\r [info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r [info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r [info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r [info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r [info]   at scala.collection.immutable.List.foreach(List.scala:334)\r [info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r [info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r [info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r [info]   at org.scalatest.Suite.run(Suite.scala:1114)\r [info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r [info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r [info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r [info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r [info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r [info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r [info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r [info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r [info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r [info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r [info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r [info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r [info]   at java.base/java.lang.Thread.run(Thread.java:840)\r [info]   Cause: java.nio.file.NoSuchFileException: /__w/datafusion-comet/datafusion-comet/apache-spark/target/tmp/spark-2cf998bb-fd3c-4621-88d8-05e9decad882/output @#output/.part-00009-cb134e84-5d4a-42d2-a342-b5edc52776ce-c000.snappy.parquet.crc\r [info]   at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r [info]   at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)\r [info]   at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\r [info]   at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)\r [info]   at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:148)\r [info]   at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)\r [info]   at java.base/java.nio.file.Files.readAttributes(Files.java:1851)\r [info]   at java.base/java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:220)\r [info]   at java.base/java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:277)\r [info]   at java.base/java.nio.file.FileTreeWalker.next(FileTreeWalker.java:374)\r [info]   at java.base/java.nio.file.FileTreeIterator.fetchNextIfNeeded(FileTreeIterator.java:83)\r [info]   at java.base/java.nio.file.FileTreeIterator.hasNext(FileTreeIterator.java:103)\r [info]   at java.base/java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1855)\r [info]   at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:292)\r [info]   at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206)\r [info]   at java.base/java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:169)\r [info]   at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:298)\r [info]   at java.base/java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681)\r [info]   at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)\r [info]   at scala.collection.Iterator$$anon$6.hasNext(Iterator.scala:477)\r [info]   at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r [info]   at scala.collection.mutable.Growable.addAll(Growable.scala:61)\r [info]   at scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r [info]   at scala.collection.immutable.SetBuilderImpl.addAll(Set.scala:405)\r [info]   at scala.collection.immutable.Set$.from(Set.scala:362)\r [info]   at scala.collection.IterableOnceOps.toSet(IterableOnce.scala:1469)\r [info]   at scala.collection.IterableOnceOps.toSet$(IterableOnce.scala:1469)\r [info]   at scala.collection.AbstractIterator.toSet(Iterator.scala:1306)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$52(FileStreamSinkSuite.scala:538)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$52$adapted(FileStreamSinkSuite.scala:505)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1(SQLTestUtils.scala:83)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$withTempDir$1$adapted(SQLTestUtils.scala:82)\r [info]   at org.apache.spark.SparkFunSuite.withTempDir(SparkFunSuite.scala:245)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.org$apache$spark$sql$test$SQLTestUtils$$super$withTempDir(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir(SQLTestUtils.scala:82)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.withTempDir$(SQLTestUtils.scala:81)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.withTempDir(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$51(FileStreamSinkSuite.scala:505)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:56)\r [info]   at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:301)\r [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:297)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.withSQLConf(FileStreamSinkSuite.scala:50)\r [info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite.$anonfun$new$50(FileStreamSinkSuite.scala:505)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.apache.spark.sql.catalyst.util.package$.quietly(package.scala:43)\r [info]   at org.apache.spark.sql.test.SQLTestUtils.$anonfun$testQuietly$1(SQLTestUtils.scala:119)\r [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r [info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r [info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r [info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r [info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r [info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)\r [info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)\r [info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r [info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r [info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)\r [info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r [info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r [info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r [info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r [info]   at scala.collection.immutable.List.foreach(List.scala:334)\r [info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r [info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r [info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r [info]   at org.scalatest.Suite.run(Suite.scala:1114)\r [info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r [info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)\r [info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r [info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r [info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r [info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)\r [info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r [info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r [info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r [info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r [info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r [info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r [info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": [], "tasks": {"summarization": "Flaky test: FileStreamSinkSuite.\"cleanup complete but invalid output for aborted job\" - I found an the flaky test when running spark sql test cases with datafusion-comet. https://github.co...", "classification": "task", "qna": {"question": "What is the issue SPARK-53756 about?", "answer": "Flaky test: FileStreamSinkSuite.\"cleanup complete but invalid output for aborted job\""}}}
{"issue_id": "SPARK-53755", "project": "SPARK", "title": "Logging support by blockmanager", "status": "Resolved", "priority": "Major", "reporter": "Tengfei Huang", "assignee": "Tengfei Huang", "created": "2025-09-30T03:06:11.000+0000", "updated": "2025-10-21T00:54:40.000+0000", "labels": ["pull-request-available"], "description": "Add logging support by blockmanager for collecting/storing python worker logs.", "comments": ["Issue resolved by pull request 52643 [https://github.com/apache/spark/pull/52643]"], "tasks": {"summarization": "Logging support by blockmanager - Add logging support by blockmanager for collecting/storing python worker logs....", "classification": "task", "qna": {"question": "What is the issue SPARK-53755 about?", "answer": "Logging support by blockmanager"}}}
{"issue_id": "SPARK-53754", "project": "SPARK", "title": "Python worker logging infrastructure", "status": "Open", "priority": "Major", "reporter": "Takuya Ueshin", "assignee": null, "created": "2025-09-30T01:46:51.000+0000", "updated": "2025-09-30T02:14:17.000+0000", "labels": [], "description": "Add Python worker logging infrastructure for users to collect their logs written to stdout/stderr or logging API.", "comments": [], "tasks": {"summarization": "Python worker logging infrastructure - Add Python worker logging infrastructure for users to collect their logs written to stdout/stderr or...", "classification": "task", "qna": {"question": "What is the issue SPARK-53754 about?", "answer": "Python worker logging infrastructure"}}}
{"issue_id": "SPARK-53753", "project": "SPARK", "title": "Shade antlr4-runtime.jar", "status": "Open", "priority": "Major", "reporter": "Vlad Rozov", "assignee": null, "created": "2025-09-29T23:19:52.000+0000", "updated": "2025-10-16T03:02:10.000+0000", "labels": ["pull-request-available"], "description": "antlr4-runtime library 4.10+ is not semver compatible with 4.9: \"{_}Mixing ANTLR 4.9.3 and 4.10 can lead to errors that point to a version mismatch. A very common Java error looks like this:{_}\u00a0{{{}Caused by: java.io.InvalidClassException: org.antlr.v4.runtime.atn.ATN; Could not deserialize ATN with version 3 (expected 4).{}}}\" By shading Spark dependency the conflict will be avoided.", "comments": [], "tasks": {"summarization": "Shade antlr4-runtime.jar - antlr4-runtime library 4.10+ is not semver compatible with 4.9: \"{_}Mixing ANTLR 4.9.3 and 4.10 can ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53753 about?", "answer": "Shade antlr4-runtime.jar"}}}
{"issue_id": "SPARK-53752", "project": "SPARK", "title": "Publish Apache Spark 4.1.0-preview2 to docker registry", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-29T20:36:28.000+0000", "updated": "2025-10-24T15:17:31.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Publish Apache Spark 4.1.0-preview2 to docker registry - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53752 about?", "answer": "Publish Apache Spark 4.1.0-preview2 to docker registry"}}}
{"issue_id": "SPARK-53751", "project": "SPARK", "title": "Explicit Checkpoint Location", "status": "Resolved", "priority": "Major", "reporter": "Jacky Wang", "assignee": "Jacky Wang", "created": "2025-09-29T20:01:03.000+0000", "updated": "2025-10-08T19:28:28.000+0000", "labels": ["pull-request-available"], "description": "Add support for explicit checkpoint location with the `storage` field in pipeline spec.\r Add support for versioned and multi-flow checkpoint dir.", "comments": ["Issue resolved by pull request 52487 [https://github.com/apache/spark/pull/52487]"], "tasks": {"summarization": "Explicit Checkpoint Location - Add support for explicit checkpoint location with the `storage` field in pipeline spec.\r Add support...", "classification": "task", "qna": {"question": "What is the issue SPARK-53751 about?", "answer": "Explicit Checkpoint Location"}}}
{"issue_id": "SPARK-53750", "project": "SPARK", "title": "Use `4.1.0-preview2` instead of `RC1`", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-29T19:43:50.000+0000", "updated": "2025-09-29T20:26:10.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 246 [https://github.com/apache/spark-connect-swift/pull/246]"], "tasks": {"summarization": "Use `4.1.0-preview2` instead of `RC1` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53750 about?", "answer": "Use `4.1.0-preview2` instead of `RC1`"}}}
{"issue_id": "SPARK-53749", "project": "SPARK", "title": "Release Spark Connect Swift Client 0.5.0", "status": "Open", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": null, "created": "2025-09-29T19:41:33.000+0000", "updated": "2025-10-01T19:07:06.000+0000", "labels": [], "description": "", "comments": [], "tasks": {"summarization": "Release Spark Connect Swift Client 0.5.0 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53749 about?", "answer": "Release Spark Connect Swift Client 0.5.0"}}}
{"issue_id": "SPARK-53748", "project": "SPARK", "title": "Support CREATE OR REFRESH syntax in pipelines SQL files", "status": "Open", "priority": "Major", "reporter": "Sanford Ryza", "assignee": null, "created": "2025-09-29T15:03:23.000+0000", "updated": "2025-09-29T15:03:23.000+0000", "labels": [], "description": "Currently, we only support syntax like the following in pipelines SQL files:\r \r CREATE MATERIALIZED VIEW ...\r \r CREATE STREAMING TABLE ...\r \r \u00a0\r \r We should also support\r \r CREATE OR REFRESH MATERIALIZED VIEW ...\r \r CREATE OR REFRESH STREAMING TABLE ...", "comments": [], "tasks": {"summarization": "Support CREATE OR REFRESH syntax in pipelines SQL files - Currently, we only support syntax like the following in pipelines SQL files:\r \r CREATE MATERIALIZED ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53748 about?", "answer": "Support CREATE OR REFRESH syntax in pipelines SQL files"}}}
{"issue_id": "SPARK-53747", "project": "SPARK", "title": "spark.app.name should be escaped", "status": "Open", "priority": "Major", "reporter": "PJ Fanning", "assignee": null, "created": "2025-09-29T11:32:23.000+0000", "updated": "2025-09-29T13:24:38.000+0000", "labels": ["pull-request-available"], "description": "Discussed on ASF mailing lists:\r * https://lists.apache.org/thread/5d0rr96ng2scypnqbw7wn9lk8hzhvptc\r * https://lists.apache.org/thread/8szh3plh2lz6co1qd2opmzssr649jvf1", "comments": [], "tasks": {"summarization": "spark.app.name should be escaped - Discussed on ASF mailing lists:\r * https://lists.apache.org/thread/5d0rr96ng2scypnqbw7wn9lk8hzhvptc\r...", "classification": "task", "qna": {"question": "What is the issue SPARK-53747 about?", "answer": "spark.app.name should be escaped"}}}
{"issue_id": "SPARK-53746", "project": "SPARK", "title": "Update Python pyyaml to 5.4", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-09-29T08:52:39.000+0000", "updated": "2025-09-29T08:53:16.000+0000", "labels": ["pull-request-available"], "description": "pyyaml 3.11 contains the following vulnerabilities:\u00a0\r  * CVE-2017-18342 : 9.8 Severity\r  * CVE-2020-14343 : 9.8 Severity", "comments": [], "tasks": {"summarization": "Update Python pyyaml to 5.4 - pyyaml 3.11 contains the following vulnerabilities:\u00a0\r  * CVE-2017-18342 : 9.8 Severity\r  * CVE-2020-...", "classification": "task", "qna": {"question": "What is the issue SPARK-53746 about?", "answer": "Update Python pyyaml to 5.4"}}}
{"issue_id": "SPARK-53745", "project": "SPARK", "title": "Update Python mlflow to 3.1.0", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-09-29T08:49:43.000+0000", "updated": "2025-09-29T08:49:43.000+0000", "labels": [], "description": "Mlflow 2.3.1 contains the following vulnerabilities:\r  * CVE-2023-6909 : 8.7 Severity\r  * CVE-2023-6831 : 10.0 Severity\r  * CVE-2023-6568 : 6.5 Severity\r  * CVE-2023-4033 : 8.8 Severity\r  * CVE-2023-6709 : 8.8 Severity\r  * CVE-2023-3765 : 10.0 Severity\r  * CVE-2023-6753 : 8.8 Severity\r  * CVE-2024-27134 : 7.3 Severity\r  * CVE-2024-0520 : 10.0 Severity\r  * CVE-2024-27132 : 9.6 Severity\r  * CVE-2024-27133 : 9.6 Severity\r  * CVE-2024-2928 : 7.5 Severity\r  * CVE-2024-3573 : 9.3 Severity\r  * CVE-2024-3848 : 7.5 Severity\r  * CVE-2025-1474 : 5.5 Severity\r  * CVE-2025-52967 : 5.8 Severity\r  * CVE-2023-6014 : 9.1 Severity\r  * CVE-2024-8859 : 7.5 Severity\r  * CVE-2023-6974 : 9.8 Severity\r  * CVE-2023-6018 : 10.0 Severity\r  * CVE-2024-3099 : 5.4 Severity\r  * CVE-2023-6015 : 10.0 Severity\r  * CVE-2024-1483 : 7.5 Severity\r  * CVE-2023-6975 : 9.8 Severity\r  * CVE-2023-6940 : 8.8 Severity\r  * CVE-2024-1558 : 7.5 Severity\r  * CVE-2024-4263 : 5.4 Severity\r  * CVE-2023-6977 : 7.5 Severity\r  * CVE-2023-43472 : 7.5 Severity\r  * CVE-2023-6976 : 8.8 Severity", "comments": [], "tasks": {"summarization": "Update Python mlflow to 3.1.0 - Mlflow 2.3.1 contains the following vulnerabilities:\r  * CVE-2023-6909 : 8.7 Severity\r  * CVE-2023-6...", "classification": "task", "qna": {"question": "What is the issue SPARK-53745 about?", "answer": "Update Python mlflow to 3.1.0"}}}
{"issue_id": "SPARK-53744", "project": "SPARK", "title": "Update Python black to 24.3.0", "status": "Open", "priority": "Major", "reporter": "Cameron", "assignee": null, "created": "2025-09-29T08:46:51.000+0000", "updated": "2025-09-29T08:51:12.000+0000", "labels": ["pull-request-available"], "description": "Python black 23.12.1 contains the vulnerability CVE-2024-21503", "comments": [], "tasks": {"summarization": "Update Python black to 24.3.0 - Python black 23.12.1 contains the vulnerability CVE-2024-21503...", "classification": "task", "qna": {"question": "What is the issue SPARK-53744 about?", "answer": "Update Python black to 24.3.0"}}}
{"issue_id": "SPARK-53743", "project": "SPARK", "title": "ListState fetchWithArrow option incurs runtime error on some types with values", "status": "Resolved", "priority": "Major", "reporter": "Jungtaek Lim", "assignee": "Jungtaek Lim", "created": "2025-09-28T22:56:10.000+0000", "updated": "2025-09-30T01:04:43.000+0000", "labels": ["pull-request-available"], "description": "We got a report that TWS PySpark with Row type API failed on requesting ListState.put(), weirdly ran fine and eventually failed.\r \r From stack trace of the report, we figured out it took the code path of fetchWithArrow (which is only triggered when the list size is exactly 100 - which was a bug) and the conversion somehow failed on below stack trace:\r {code:java}\r   File \"/databricks/spark/python/pyspark/sql/streaming/stateful_processor.py\", line 147, in put\r \u00a0 \u00a0 self._listStateClient.put(self._stateName, newState)\r \u00a0 File \"/databricks/spark/python/pyspark/sql/streaming/list_state_client.py\", line 195, in put\r \u00a0 \u00a0 self._stateful_processor_api_client._send_arrow_state(self.schema, values)\r \u00a0 File \"/spark/python/pyspark/sql/streaming/stateful_processor_api_client.py\", line 604, in _send_arrow_state\r \u00a0 \u00a0 pandas_df = convert_pandas_using_numpy_type(\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/spark/python/pyspark/sql/pandas/types.py\", line 1599, in convert_pandas_using_numpy_type\r \u00a0 \u00a0 df[field.name] = df[field.name].astype(np_type)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/generic.py\", line 6643, in astype\r \u00a0 \u00a0 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/internals/managers.py\", line 430, in astype\r \u00a0 \u00a0 return self.apply(\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/internals/managers.py\", line 363, in apply\r \u00a0 \u00a0 applied = getattr(b, f)(**kwargs)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/internals/blocks.py\", line 758, in astype\r \u00a0 \u00a0 new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 237, in astype_array_safe\r \u00a0 \u00a0 new_values = astype_array(values, dtype, copy=copy)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 182, in astype_array\r \u00a0 \u00a0 values = _astype_nansafe(values, dtype, copy=copy)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r \u00a0 File \"/python/lib/python3.12/site-packages/pandas/core/dtypes/astype.py\", line 133, in _astype_nansafe\r \u00a0 \u00a0 return arr.astype(dtype, copy=True)\r \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r TypeError: int() argument must be a string, a bytes-like object or a real number, not 'NoneType' {code}\r The report stated that they don't have an issue when they workaround to do clear() and put element separately (not triggering the path of fetchWithArrow).", "comments": ["Issue resolved via https://github.com/apache/spark/pull/52479"], "tasks": {"summarization": "ListState fetchWithArrow option incurs runtime error on some types with values - We got a report that TWS PySpark with Row type API failed on requesting ListState.put(), weirdly ran...", "classification": "task", "qna": {"question": "What is the issue SPARK-53743 about?", "answer": "ListState fetchWithArrow option incurs runtime error on some types with values"}}}
{"issue_id": "SPARK-53742", "project": "SPARK", "title": "Push down the filter used in the count_if function", "status": "Open", "priority": "Minor", "reporter": "Ji Jun Tang", "assignee": null, "created": "2025-09-28T05:09:58.000+0000", "updated": "2025-10-09T10:19:28.000+0000", "labels": ["pull-request-available"], "description": "By pushing down the filter condition in the count_if function, we can reduce the volume of data that needs to be processed.\r \r \u00a0\r {code:java}\r // code placeholder\r spark.sql(\"create table t1(a int, b int, c int) using parquet\")\r spark.sql(\"select count_if(a <>1) from t1\").explain(\"cost\") {code}\r Current:\r {code:java}\r == Optimized Logical Plan ==\r Aggregate [count(if (NOT _common_expr_0#6) null else _common_expr_0#6) AS count_if((NOT (a = 1)))#4L], Statistics(sizeInBytes=16.0 B, rowCount=1)\r +- Project [NOT (a#0 = 1) AS _common_expr_0#6], Statistics(sizeInBytes=1.0 B)\r \u00a0 \u00a0+- Relation spark_catalog.default.t1[a#0,b#1,c#2] parquet, Statistics(sizeInBytes=0.0 B) {code}\r Excepted:\r {code:java}\r == Optimized Logical Plan ==\r Aggregate [count(if (NOT _common_expr_2#22) null else _common_expr_2#22) AS count_if((NOT (a = 1)))#21L], Statistics(sizeInBytes=16.0 B, rowCount=1)\r +- Project [NOT (a#3 = 1) AS _common_expr_2#22], Statistics(sizeInBytes=1.0 B)\r \u00a0 \u00a0+- Filter (isnotnull(a#3) AND NOT (a#3 = 1)), Statistics(sizeInBytes=1.0 B)\r \u00a0 \u00a0 \u00a0 +- Relation spark_catalog.default.t1[a#3,b#4,c#5] parquet, Statistics(sizeInBytes=0.0 B) {code}", "comments": [], "tasks": {"summarization": "Push down the filter used in the count_if function - By pushing down the filter condition in the count_if function, we can reduce the volume of data that...", "classification": "task", "qna": {"question": "What is the issue SPARK-53742 about?", "answer": "Push down the filter used in the count_if function"}}}
{"issue_id": "SPARK-53741", "project": "SPARK", "title": "Upgrade ORC to 2.2.1", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-27T22:50:53.000+0000", "updated": "2025-10-01T17:49:48.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52478 [https://github.com/apache/spark/pull/52478]"], "tasks": {"summarization": "Upgrade ORC to 2.2.1 - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53741 about?", "answer": "Upgrade ORC to 2.2.1"}}}
{"issue_id": "SPARK-53740", "project": "SPARK", "title": "Update `(pi|spark-history-server)-preview.yaml` to use `4.1.0-preview2`", "status": "Resolved", "priority": "Minor", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-27T22:38:33.000+0000", "updated": "2025-09-29T19:37:31.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 354 [https://github.com/apache/spark-kubernetes-operator/pull/354]"], "tasks": {"summarization": "Update `(pi|spark-history-server)-preview.yaml` to use `4.1.0-preview2` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53740 about?", "answer": "Update `(pi|spark-history-server)-preview.yaml` to use `4.1.0-preview2`"}}}
{"issue_id": "SPARK-53739", "project": "SPARK", "title": "Add `pi-preview-with-eventlog.yaml` example", "status": "Resolved", "priority": "Major", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "created": "2025-09-27T22:29:08.000+0000", "updated": "2025-09-29T19:36:51.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 353 [https://github.com/apache/spark-kubernetes-operator/pull/353]"], "tasks": {"summarization": "Add `pi-preview-with-eventlog.yaml` example - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53739 about?", "answer": "Add `pi-preview-with-eventlog.yaml` example"}}}
{"issue_id": "SPARK-53738", "project": "SPARK", "title": "PlannedWrite should preserve custom sort order when query output contains literal", "status": "Resolved", "priority": "Major", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "created": "2025-09-27T01:43:54.000+0000", "updated": "2025-10-22T17:36:53.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52474 [https://github.com/apache/spark/pull/52474]", "[https://github.com/apache/spark/pull/52474] was reverted from `master` in [https://github.com/apache/spark/commit/ce3437af0179db986d57b464e69b845337f469ac] due to an alternative fix proposed. Please find details here: [https://github.com/apache/spark/pull/52474#issuecomment-3384559926].", "Issue reresolved by pull request [https://github.com/apache/spark/pull/52584]"], "tasks": {"summarization": "PlannedWrite should preserve custom sort order when query output contains literal - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53738 about?", "answer": "PlannedWrite should preserve custom sort order when query output contains literal"}}}
{"issue_id": "SPARK-53737", "project": "SPARK", "title": "Add Real-time Mode trigger", "status": "Resolved", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": "Boyang Jerry Peng", "created": "2025-09-27T01:03:12.000+0000", "updated": "2025-10-24T05:11:52.000+0000", "labels": ["pull-request-available"], "description": "Introduce a new trigger type for Real-time Mode (RTM) in Structured Streaming.\u00a0 This new trigger will be how users enable their Structured Streaming query to run in Real-time Mode.", "comments": ["Issue resolved by pull request 52473 [https://github.com/apache/spark/pull/52473]"], "tasks": {"summarization": "Add Real-time Mode trigger - Introduce a new trigger type for Real-time Mode (RTM) in Structured Streaming.\u00a0 This new trigger wil...", "classification": "task", "qna": {"question": "What is the issue SPARK-53737 about?", "answer": "Add Real-time Mode trigger"}}}
{"issue_id": "SPARK-53736", "project": "SPARK", "title": "Real-time Mode in Structured Streaming", "status": "Open", "priority": "Major", "reporter": "Boyang Jerry Peng", "assignee": null, "created": "2025-09-27T00:48:28.000+0000", "updated": "2025-10-07T18:56:48.000+0000", "labels": ["releasenotes"], "description": "Real-time mode for Apache Spark Structured Streaming is a new execution model designed to significantly lower end-to-end data processing latency to the order of 100 milliseconds.\r \r \u00a0\r \r More details can be found in the SPIP\r \r [https://docs.google.com/document/d/1CvJvtlTGP6TwQIT4kW6GFT1JbdziAYOBvt60ybb7Dw8/edit?usp=sharing]\r \r \u00a0\r \r SPIP approved by the community:\r \r [https://lists.apache.org/thread/k93gj0ko54kcslzkjwp95nqvjnkwcb63]", "comments": ["I added `releasenotes` label not to forget to mention this in the release note."], "tasks": {"summarization": "Real-time Mode in Structured Streaming - Real-time mode for Apache Spark Structured Streaming is a new execution model designed to significan...", "classification": "task", "qna": {"question": "What is the issue SPARK-53736 about?", "answer": "Real-time Mode in Structured Streaming"}}}
{"issue_id": "SPARK-53735", "project": "SPARK", "title": "Hide server-side JVM stack traces by default in spark-pipelines output", "status": "Resolved", "priority": "Major", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "created": "2025-09-26T17:10:02.000+0000", "updated": "2025-09-29T14:39:14.000+0000", "labels": ["pull-request-available"], "description": "Error output for failing pipeline runs can be very verbose and show a bunch of info that is not relevant to the user. We should hide the server-side stack traces by default.\r \r \u00a0\r 2025-09-26 17:07:50: Failed to resolve flow: 'spark_catalog.default.rental_bike_trips'.\r Error: [TABLE_OR_VIEW_NOT_FOUND] The table or view `spark_catalog`.`default`.`rental_bike_trips_raws` cannot be found. Verify the spelling and correctness of the schema and catalog.\r If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\r To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\r 'UnresolvedRelation [spark_catalog, default, rental_bike_trips_raws], [], true\r \u00a0\r Traceback (most recent call last):\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/pipelines/cli.py\", line 360, in <module>\r \u00a0 \u00a0 run(\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/pipelines/cli.py\", line 287, in run\r \u00a0 \u00a0 handle_pipeline_events(result_iter)\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/pipelines/spark_connect_pipeline.py\", line 53, in handle_pipeline_events\r \u00a0 \u00a0 for result in iter:\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/sql/connect/client/core.py\", line 1169, in execute_command_as_iterator\r \u00a0 \u00a0 for response in self._execute_and_fetch_as_iterator(req, observations or {}):\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/sql/connect/client/core.py\", line 1559, in _execute_and_fetch_as_iterator\r \u00a0 \u00a0 self._handle_error(error)\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/sql/connect/client/core.py\", line 1833, in _handle_error\r \u00a0 \u00a0 self._handle_rpc_error(error)\r \u00a0 File \"/Users/sandy.ryza/oss/python/pyspark/sql/connect/client/core.py\", line 1904, in _handle_rpc_error\r \u00a0 \u00a0 raise convert_exception(\r pyspark.errors.exceptions.connect.AnalysisException:\r Failed to resolve flows in the pipeline.\r \u00a0\r A flow can fail to resolve because the flow itself contains errors or because it reads\r from an upstream flow which failed to resolve.\r \u00a0\r Flows with errors: spark_catalog.default.rental_bike_trips\r Flows that failed due to upstream errors:\r \u00a0\r To view the exceptions that were raised while resolving these flows, look for flow\r failures that precede this log.\r \u00a0\r JVM stacktrace:\r org.apache.spark.sql.pipelines.graph.UnresolvedPipelineException\r at org.apache.spark.sql.pipelines.graph.GraphValidations.validateSuccessfulFlowAnalysis(GraphValidations.scala:284)\r at org.apache.spark.sql.pipelines.graph.GraphValidations.validateSuccessfulFlowAnalysis$(GraphValidations.scala:247)\r at org.apache.spark.sql.pipelines.graph.DataflowGraph.validateSuccessfulFlowAnalysis(DataflowGraph.scala:33)\r at org.apache.spark.sql.pipelines.graph.DataflowGraph.$anonfun$validationFailure$1(DataflowGraph.scala:186)\r at scala.util.Try$.apply(Try.scala:217)\r at org.apache.spark.sql.pipelines.graph.DataflowGraph.validationFailure$lzycompute(DataflowGraph.scala:185)\r at org.apache.spark.sql.pipelines.graph.DataflowGraph.validationFailure(DataflowGraph.scala:185)\r at org.apache.spark.sql.pipelines.graph.DataflowGraph.validate(DataflowGraph.scala:173)\r at org.apache.spark.sql.pipelines.graph.PipelineExecution.resolveGraph(PipelineExecution.scala:109)\r at org.apache.spark.sql.pipelines.graph.PipelineExecution.startPipeline(PipelineExecution.scala:48)\r at org.apache.spark.sql.pipelines.graph.PipelineExecution.runPipeline(PipelineExecution.scala:63)\r at org.apache.spark.sql.connect.pipelines.PipelinesHandler$.startRun(PipelinesHandler.scala:294)\r at org.apache.spark.sql.connect.pipelines.PipelinesHandler$.handlePipelinesCommand(PipelinesHandler.scala:93)\r at org.apache.spark.sql.connect.planner.SparkConnectPlanner.handlePipelineCommand(SparkConnectPlanner.scala:2727)\r at org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2697)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:322)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:224)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)\r at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:349)\r at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:349)\r at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:187)\r at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)\r at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r at org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:348)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)\r at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)\r 25/09/26 10:07:50 INFO ShutdownHookManager: Shutdown hook called", "comments": ["User 'sryza' has created a pull request for this issue: https://github.com/apache/spark/pull/52470", "Issue resolved by pull request 52470 [https://github.com/apache/spark/pull/52470]"], "tasks": {"summarization": "Hide server-side JVM stack traces by default in spark-pipelines output - Error output for failing pipeline runs can be very verbose and show a bunch of info that is not rele...", "classification": "task", "qna": {"question": "What is the issue SPARK-53735 about?", "answer": "Hide server-side JVM stack traces by default in spark-pipelines output"}}}
{"issue_id": "SPARK-53734", "project": "SPARK", "title": "Prefer table column over LCA in `UnresolvedExtractValue` extractor", "status": "Resolved", "priority": "Major", "reporter": "Mihailo Timotic", "assignee": "Mihailo Timotic", "created": "2025-09-26T16:49:40.000+0000", "updated": "2025-09-30T17:17:36.000+0000", "labels": ["pull-request-available"], "description": "Prefer table column over LCA in `UnresolvedExtractValue` extractor. For example:\r \r `SELECT 1 AS col1, col2[col1] FROM VALUES(0, ARRAY(1,2))`\r \r should return (1,1) and not (1,2)", "comments": ["Issue resolved by pull request 52472 [https://github.com/apache/spark/pull/52472]"], "tasks": {"summarization": "Prefer table column over LCA in `UnresolvedExtractValue` extractor - Prefer table column over LCA in `UnresolvedExtractValue` extractor. For example:\r \r `SELECT 1 AS col...", "classification": "task", "qna": {"question": "What is the issue SPARK-53734 about?", "answer": "Prefer table column over LCA in `UnresolvedExtractValue` extractor"}}}
{"issue_id": "SPARK-53733", "project": "SPARK", "title": "Delay `resolveColsLastResort` until previous `UnresolvedAlias`es are resolved", "status": "Open", "priority": "Major", "reporter": "Mihailo Timotic", "assignee": null, "created": "2025-09-26T11:58:04.000+0000", "updated": "2025-09-30T14:06:40.000+0000", "labels": ["pull-request-available"], "description": "Delay `resolveColsLastResort` until previous `UnresolvedAlias`es are resolved. This is needed in order to prevent incorrectly resolving a column to a session/local variable or outer reference instead of lateral column alias reference.", "comments": [], "tasks": {"summarization": "Delay `resolveColsLastResort` until previous `UnresolvedAlias`es are resolved - Delay `resolveColsLastResort` until previous `UnresolvedAlias`es are resolved. This is needed in ord...", "classification": "task", "qna": {"question": "What is the issue SPARK-53733 about?", "answer": "Delay `resolveColsLastResort` until previous `UnresolvedAlias`es are resolved"}}}
{"issue_id": "SPARK-53732", "project": "SPARK", "title": "Add TimeTravelSpec to DataSourceV2Relation", "status": "Open", "priority": "Major", "reporter": "Anton Okolnychyi", "assignee": null, "created": "2025-09-26T08:08:11.000+0000", "updated": "2025-10-24T17:31:34.000+0000", "labels": ["pull-request-available"], "description": "", "comments": [], "tasks": {"summarization": "Add TimeTravelSpec to DataSourceV2Relation - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53732 about?", "answer": "Add TimeTravelSpec to DataSourceV2Relation"}}}
{"issue_id": "SPARK-53731", "project": "SPARK", "title": "Update the type hints of iterator API in `python/pyspark/sql/pandas/_typing/__init__.pyi`", "status": "Open", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": null, "created": "2025-09-26T07:32:48.000+0000", "updated": "2025-09-26T07:34:48.000+0000", "labels": [], "description": "iterator APIs should use {{Iterator}} instead of {{Iterable}}", "comments": [], "tasks": {"summarization": "Update the type hints of iterator API in `python/pyspark/sql/pandas/_typing/__init__.pyi` - iterator APIs should use {{Iterator}} instead of {{Iterable}}...", "classification": "task", "qna": {"question": "What is the issue SPARK-53731 about?", "answer": "Update the type hints of iterator API in `python/pyspark/sql/pandas/_typing/__init__.pyi`"}}}
{"issue_id": "SPARK-53730", "project": "SPARK", "title": "Switch Spark 3.5 Java 11 docker registry images to 11-jammy", "status": "Resolved", "priority": "Major", "reporter": "Peter Toth", "assignee": "Peter Toth", "created": "2025-09-26T07:13:57.000+0000", "updated": "2025-10-07T16:36:02.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 95 [https://github.com/apache/spark-docker/pull/95]"], "tasks": {"summarization": "Switch Spark 3.5 Java 11 docker registry images to 11-jammy - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53730 about?", "answer": "Switch Spark 3.5 Java 11 docker registry images to 11-jammy"}}}
{"issue_id": "SPARK-53729", "project": "SPARK", "title": "Fix serialization of `pyspark.sql.connect.window.WindowSpec`", "status": "Resolved", "priority": "Major", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "created": "2025-09-26T03:26:55.000+0000", "updated": "2025-10-24T05:13:24.000+0000", "labels": ["pull-request-available"], "description": "", "comments": ["Issue resolved by pull request 52464 [https://github.com/apache/spark/pull/52464]"], "tasks": {"summarization": "Fix serialization of `pyspark.sql.connect.window.WindowSpec` - ...", "classification": "task", "qna": {"question": "What is the issue SPARK-53729 about?", "answer": "Fix serialization of `pyspark.sql.connect.window.WindowSpec`"}}}
